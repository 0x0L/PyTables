<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:bib="http://bibtexml.org/STSCHEMA" xmlns:adr="http://tbookdtd.sourceforge.net/addressbook" xmlns:ref="http://tbookdtd.sourceforge.net/references" xmlns:index="http://tbookdtd.sourceforge.net/index" xmlns:dimensions="http://tbookdtd.sourceforge.net/dimension-file" xmlns:depths="http://tbookdtd.sourceforge.net/depths-file" xmlns:loc="local" xmlns:CSS="http://www.w3.org/1998/Style/CSS2" xmlns:pref="http://www.w3.org/2002/Math/preference" id="optimizationTips"><a name="optimizationTips"></a><head>
      <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <title>PyTables User's Guide</title><meta http-equiv="Content-Type" content="text/html"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="generator" content="The tbook system at http://tbookdtd.sourceforge.net"><meta name="robots" content="index"><meta name="DC.Title" content="PyTables User's Guide"><meta name="DC.Description" content="PyTables User's Guide Hierarchical datasets in Python Release 0.8"><meta name="DC.Creator" content="Francesc Alted"><meta name="Author" content="Francesc Alted"><meta name="DC.Creator" content="Scott Prater"><meta name="Author" content="Scott Prater"><meta name="DC.Date" content="2004-02-29T17:36:13+01:00"><meta name="Date" content="2004-02-29T17:36:13+01:00"><meta name="DC.Rights" content="(c) 2002, 2003, 2004 Francesc AltedCopyright Notice and Statement for PyTables Software Library and Utilities Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this lis"><meta name="Copyright" content="(c) 2002, 2003, 2004 Francesc AltedCopyright Notice and Statement for PyTables Software Library and Utilities Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this lis"><meta name="DC.Type" content="Text"><meta name="DC.Format" content="text/html"><meta name="DC.Language" scheme="rfc3066" content="en"><meta name="Language" content="en"><meta http-equiv="Content-Style-Type" content="text/css"><style type="text/css">
    body {
      font-family: serif; text-align: justify;
      margin: 0pt; line-height: 1.3; background-color: white; color: black }
    body.letter { background-image: none }
    h1, h2, h3, h4, h5, h6, div.partheadline, div.title,
      div.title-article {
      font-family: sans-serif; font-weight: bold; font-stretch: condensed;
      text-align: left; }
    h1, h2, h3, h4, h5, h6, div.partheadline { padding-left: 18pt }
    h1 { clear: both }
    h1.toc { clear: none }
    a:link, a:visited, a:active { text-decoration: none }
    a:hover { text-decoration: underline; color: #0066FF; }
    div.document, div.letter { margin-left: 80px; margin-right: 150px;
      padding-top: 3ex; padding-bottom: 5ex }
    div.authorlist { font-family: sans-serif;
      font-stretch: condensed; text-align: left; font-size: x-large;
      margin-bottom: 1ex }
    div.authorlist-article { font-size: x-large; text-align: center;
      margin-bottom: 2ex }
    div.title { font-size: xx-large; color: gray; margin-bottom: 5ex; }
    div.title-article { font-size: xx-large; text-align: center;
      margin-bottom: 1ex }
    div.date-article { font-size: large; text-align: center;
      margin-bottom: 5ex }
    div.subtitle { text-align: left; margin-bottom: 3ex }
    div.legal { text-align: right }
    div.legalnotice { margin-top: 3ex; text-align: left }
    div.partheadline { font-size: xx-large; margin-top: 1ex; margin-bottom: 1ex }
    div.footnote { }
    hr.footnoterule { text-align: left; width: 20%; color: black }
    div.mathdisplay { text-align: center;
    margin-top: 2ex; margin-bottom: 2ex }
    div.figure, div.table, div.mathml-remark { margin-bottom: 3ex;
            margin-top: 2ex}
    div.mathml-remark { width: 50%; text-align: left; text-indent: 0pt }
    div.caption { margin-top: 1ex; width: 20em}
    div.caption-text { font-size: small;
      padding-left: 1em; text-indent: -1em; text-align: left}
    caption { max-width: 100%; min-width: 200px; margin-bottom: 1ex;
      text-align: left }
    div.verse { white-space: pre; margin-left: 2em;
      margin-top: 2ex; margin-bottom: 2ex }
    div.toc { margin-bottom: 6ex }
    div.biblio { }
    div.index { width: 100% }
    div.aphorism { margin-bottom: 2ex; font-style: italic; text-align: right;
      margin-left: 50%}
    div.aphorism-origin { margin-top: 1ex }
    div.letter { }
    div.subject { margin-top: 3ex; margin-bottom: 3ex }
    div.opening { margin-top: 3ex; margin-bottom: 3ex }
    div.closing { margin-top: 3ex; text-align: right }
    div.to { margin-top: 3ex; margin-bottom: 3ex }
    span.subject { font-weight: bold }
    div.speedbar-top, div.speedbar-bottom { margin-left: 1em; margin-right: 1em }
    div.speedbar-top { margin-top: 2ex }
    div.speedbar-bottom { margin-bottom: 3ex }
    table.speedbar { width: 100% }
    hr.speedbar { clear: both }
    div.theorem-plain, div.theorem-definition, div.theorem-remark { margin-top: 2ex;
      margin-bottom: 2ex }
    div.theorem-plain { font-style: italic }
    div.proof { margin-top: 2ex; margin-bottom: 2ex }
    span.theorem-head-plain, span.theorem-head-definition, span.theorem-head-remark
      { margin-right: 0.66em }
    span.theorem-head-plain, span.theorem-head-definition
      { font-style: normal; font-weight: bold }
    span.theorem-head-remark { font-style: italic }
    span.proof-head { font-style: italic; margin-right: 0.66em }
    acronym { font-size: 95%; letter-spacing: 0.1em; text-transform: uppercase }
    td.header { text-align: center; font-weight: bold }
    table.toc { }
    table.tabular { font-size: smaller }
    td.thickline { height: 0pt; border-bottom: medium solid; padding: 2px }
    td.thinline { height: 0pt; border-bottom: thin solid;   padding: 2px }
     /* FixMe: Following definition Still bad */
    div.part-toc { margin-top: 2ex; font-weight: bold; font-size: larger }
     /* FixMe: Following definition: How cleaner? */
    div.toplevel-toc { margin-top: 1ex; font-weight: bold }
    td.number1 { width: 1.5em }
    td.number2 { width: 2em }
    td.number3 { width: 3em }
    td.author { text-align: center; width: 50% }
    div { }  /* Must remain empty! */
    span.captionlabel { font-family: sans-serif; font-weight: bold;
      margin-right: 0.66em }
    span.captionlabel:after { content: " " }
    b.captionlabel {  }
    b.captionlabel:after { content: " " }
    span.headlinenumber { margin-right: 0.66em }
    tt.verb {font-family: monospace }
    span.bibtag { font-weight: bold; margin-right: 1em }
    span.bib-author { }
    span.bib-lastname { font-variant: small-caps }
    span.bib-title { font-style: italic }
    span.bib-booktitle { }
    span.bib-journal { }
    span.bib-journal-volume { font-weight: bold }
    span.bib-publisher { }
    span.bib-school { }
    span.bib-year { }
    span.bib-note { }
    span { } /* Muss leer bleiben! */
    li.bibitem { margin-left: 1em; text-indent: -1em }
    img.graphics { border: 0pt }
    p,div.p { margin: 0pt; text-indent: 18pt }
    ul, dl, ol { text-indent: 0pt; }
    p.first, div.p-first { margin: 0pt; text-indent: 0pt }
    p.bibitem { margin-top: 1.5ex; margin-left: 3em; text-indent: -3em }
    ul.biblio { list-style: none }
    mi.ch { font-style: normal }
    math[display="block"], div.equation {
      margin-top: 1ex; margin-bottom: 1ex }
    div.i-lettergroup { font-size: larger; font-weight: bold; padding-left: 18pt;
      margin-top: 3ex; margin-bottom: 1.5ex }
    div.i-item { }
    div.i-main { }
    dt { display: compact; font-weight: bold }
    dd { text-indent: 0pt }
    pre { text-indent: 0pt }
    span.i-see { font-style: italic; }

h1 { color: red }
h2 { color: olive }
h3 { color: navy }
h4 { color: maroon }
body { background-image: url("tile.jpg");
 font: 14px/1.5 Verdana, Arial, Helvetica, sans-serif;
 background:#fff;
 padding:5%;
 padding-top:2%;
 margin:0px;
}
/*     body { background-image: url("tile.jpg");
         background-color: white;
         color: black;
	 font-size: 13pt; } */
hr.footnoterule { color: white }
div.title { font-size: x-large; color: maroon }
div.subtitle { font-size: x-large; color: olive }
div.title-article { font-size: x-large }
div.partheadline { font-size: x-large }
</style></head><body><div class="speedbar-top"><table class="speedbar"><tbody><tr><td style="text-align: left; width: 15%"><a href="usersguide4.html">previous</a></td><td style="text-align: center"><a href="usersguide.html#tb:table-of-contents">Table of Contents</a></td><td style="text-align: right; width: 15%"><a href="usersguide6.html">next</a></td></tr><tr><td colspan="3">&nbsp;</td></tr></tbody></table><hr class="speedbar"></div><div class="document"><div id="optimizationTips"><a name="optimizationTips"></a>
      <h1 id="chapter5"><a name="chapter5"></a>Chapter&nbsp;5: Optimization tips</h1>






      <p class="first">On this chapter, you will get deeper knowledge of
	<tt class="verb">PyTables</tt> internals. <tt class="verb">PyTables</tt> has
	several places where the user can improve the performance of
	his application. If you are planning to deal with really large
	data, you should read carefully this section in order to learn
	how to get an important boost for your code. But if your
	dataset is small or medium size (say, up to 1 MB), you should
	not worry about that as the default parameters in
	<tt class="verb">PyTables</tt> are already tuned to handle that
	perfectly.
      </p>

      <div>
	<h2 id="section5.1"><span class="headlinenumber"><a name="section5.1"></a>5.1 </span>Taking advantage of Psyco</h2>

	<p class="first">Psyco (see <a href="#psycoRef"></a>)is a kind of
	  specialized compiler for Python that typically accelerates
	  Python applications with no change in source code. You can
	  think of Psyco as a kind of just-in-time (JIT) compiler, a
	  little bit like Java's, that emit machine code on the fly
	  instead of interpreting your Python program step by
	  step. The result is that your unmodified Python programs run
	  faster.
	</p>

	<p>Psyco is very easy to install and use, so in most scenarios
	  it is worth to have it a try. However, it only runs on Intel
	  386 architectures, so if you are using other architectures,
	  you are out of luck (at least until Psyco will support
	  yours).
	</p>

	<p>As an example, imagine that you have a small script that
	  reads and selects data over a series of datasets, like this:
	</p>

	<pre>
def readFile(filename):
    "Select data from all the tables in filename"

    fileh = openFile(filename, mode = "r")
    result = []
    for table in fileh("/", 'Table'):
        result = [ p['var3'] for p in table if p['var2'] &lt;= 20 ]

    fileh.close()
    return e

if __name__=="__main__":
    print readFile("myfile.h5")
	</pre>

	<p>In order to accelerate this piece of code, you can rewrite
	  your main program to look like:
	</p>

	<pre>
if __name__=="__main__":
    import pysco
    psyco.bind(readFile)
    print readFile("myfile.h5")
	</pre>

	<p>That's all!. From now on, each time that you execute your
	  python script, Psyco will deploy its sophisticated
	  algorithms so as to accelerate your calculations.
	</p>

	<p>You can see in the graphs <a href="#psycoWriteComparison">5.1</a> and <a href="#psycoReadComparison">5.2</a> how much I/O speed
	  improvement you can get by using Psyco. By looking at this
	  figures you can get an idea if these improvements are of
	  your interest or not. In general, if you are not going to
	  use compression you will take advantage of Psyco if your
	  tables are medium sized (1e+3 &lt; nrows &lt; 1e+6), and
	  this advantage will disappear progressively when the number
	  of rows grows well over one million. However if you use
	  compression, you will probably see improvements even beyond
	  this limit (see <a href="#compressionIssues">section&nbsp;5.2</a>). As always, there
	  is no substitute for experimentation with your own dataset.
	</p>

	<div class="figure" id="psycoWriteComparison"><a name="psycoWriteComparison"></a>
	  <img class="graphics" alt="Writing tables with/without Psyco.&#xA;	  " src="write-medium-psyco-nopsyco-comparison-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.1:</span> Writing tables with/without Psyco.
	  </div></div>
	</div>

	<div class="figure" id="psycoReadComparison"><a name="psycoReadComparison"></a>
	  <img class="graphics" alt="Reading tables with/without Psyco.&#xA;	  " src="read-medium-psyco-nopsyco-comparison-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.2:</span> Reading tables with/without Psyco.
	  </div></div>
	</div>

      </div> 

      <div id="compressionIssues"><a name="compressionIssues"></a>
	<h2 id="section5.2"><span class="headlinenumber"><a name="section5.2"></a>5.2 </span>Compression issues</h2>

	<p class="first">One of the beauties of <tt class="verb">PyTables</tt> is that it
	  supports compression on tables (but not on arrays!, that may
	  come later), although it is disabled by default. Compression
	  of big amounts of data might be a bit controversial feature,
	  because compression has a legend of being a very big CPU
	  time resources consumer. However, if you are willing to
	  check if compression can help not only reducing your dataset
	  file size but <b>also</b> improving
	  your I/O efficiency, keep reading.
	</p>

	<p>There is an usual scenario where users need to save
	  duplicated data in some record fields, while the others
	  have varying values. In a relational database approach
	  such a redundant data can normally be moved to other
	  tables and a relationship between the rows on the separate
	  tables can be created. But that takes analysis and
	  implementation time, and made the underlying libraries
	  more complex and slower.
	</p>

	<p><tt class="verb">PyTables</tt> transparent compression allows the
	  user to not worry about finding which is their optimum data
	  tables strategy, but rather use less, not directly related,
	  tables with a larger number of columns while still not
	  cluttering the database too much with duplicated data
	  (compression is responsible to avoid that). As a side
	  effect, data selections can be made more easily because you
	  have more fields available in a single table, and they can
	  be referred in the same loop. This process may normally end
	  in a simpler, yet powerful manner to process your data
	  (although you should still be careful about what kind of
	  scenarios compression use is convenient or not).
	</p>

	<p>The compression library used by default is the <b>Zlib</b> (see <a href="#zlibRef"></a>), and as HDF5 <em>requires</em>
	  it, you can safely use it and expect that your HDF5 files
	  can be read on any other platform that has HDF5 libraries
	  installed. Zlib provides good compression ratio, although
	  somewhat slow, and reasonably fast decompression. Because
	  of that, it is a good candidate to be used for compress
	  you data.
	</p>

	<p>However, in many situations (i.e. write <em>once</em>, read
	  <em>multiple</em>), it is critical to have <em>very
	  good</em> decompression speed (at expense of whether less
	  compression or more CPU wasted on compression, as we will
	  see soon). This is why support for two additional
	  compressors has been added to PyTables: LZO and UCL (see
	  <a href="#lzouclRef"></a>). Following his author (and
	  checked by the author of this manual), LZO offers pretty
	  fast compression (although small compression ratio) and
	  extremely fast decompression while UCL achieve an excellent
	  compression ratio (at the price of spending much more CPU
	  time) while allowing very fast decompression (and <em>very
	  close</em> to the LZO one). In fact, LZO and UCL are so fast
	  when decompressing that, in general (that depends on your
	  data, of course), writing and reading a compressed table is
	  actually faster (and sometimes <b>much
	  faster</b>) than if it is uncompressed. This fact is
	  very important, specially if you have to deal with very
	  large amounts of data.
	</p>

	<p>Be aware that the LZO and UCL support in PyTables is not
	  standard on HDF5, so if you are going to use your PyTables
	  files in other contexts different from PyTables you will not
	  be able to read them. Still, see the <a href="usersguide8.html#ptrepackDescr">appendix&nbsp;C.2</a> where the
	  <tt class="verb">ptrepack</tt> utility is described to find a way to
	  free your files from LZO or UCL dependencies, so that you
	  can use these compressors locally with the guaranty that you
	  can replace them by ZLIB (or even remove compression
	  completely) if you want to export the files to other HDF5
	  tools afterwards.
	</p>

	<p>In order to give you a raw idea of what ratios would be
	  achieved, and what resources would be consumed, look at the
	  <a href="#comprTblComparison">table&nbsp;5.1</a>. This table has
	  been obtained from synthetic data and with a somewhat
	  outdated PyTables version (0.5), so take this just as a
	  guide because your mileage will probably vary. Have also a
	  look at the graphs <a href="#lzozlibuclWriteComparison">5.3</a> and <a href="#lzozlibuclReadComparison">5.4</a> (these graphs has
	  been obtained with tables with different row sizes and
	  PyTables version than the previous example, so, do not try
	  to directly compare the figures). They show how evolves the
	  speed of writing/reading rows as the size (the row number)
	  of tables grows. Even though in these graphs the size of one
	  single row is 56 bytes, you can most probably extrapolate
	  this figures to other row sizes. If you are curious how well
	  can perform compression together with Psyco, look at the
	  graphs <a href="#psycolzozlibuclWriteComparison">5.5</a>
	  and <a href="#psycolzozlibuclReadComparison">5.6</a>. As
	  you can see, the results are pretty interesting.
	</p>

	<div class="table" id="comprTblComparison"><a name="comprTblComparison"></a>
	  
	  <table class="tabular" cellspacing="0" cellpadding="5" frame="hsides" rules="groups"><caption><span class="captionlabel">Table&nbsp;5.1:</span> Comparison between different compression
	    libraries. The tests has been conducted on a Pentium 4 at 2
	    GHz and a hard disk at 4200 RPM.</caption><col align="left"><col align="center"><col align="center"><col align="center"><col align="center"><col align="center">
	    <thead><tr><td colspan="6" class="thickline"></td></tr>
	      <tr><th align="left">Compr. Lib</th><th align="center">File size (MB)</th><th align="center">Time writing (s)</th><th align="center">Time reading (s)</th><th align="center">Speed writing (Krow/s)</th><th align="center">Speed reading (Krow/s)</th></tr>
	    <tr><td colspan="6" class="thinline"></td></tr></thead>
	    <tbody>
	      <tr><td align="left">NO COMPR</td><td align="center">244.0</td><td align="center">24.4</td><td align="center">16.0</td><td align="center">18.0</td><td align="center">27.8</td></tr>
	      <tr><td align="left">Zlib (lvl 1)</td><td align="center">8.5</td><td align="center">17.0</td><td align="center">3.11</td><td align="center">26.5</td><td align="center">144.4</td></tr>
	      <tr><td align="left">Zlib (lvl 6)</td><td align="center">7.1</td><td align="center">20.1</td><td align="center">3.10</td><td align="center">22.4</td><td align="center">144.9</td></tr>
	      <tr><td align="left">Zlib (lvl 9)</td><td align="center">7.2</td><td align="center">42.5</td><td align="center">3.10</td><td align="center">10.6</td><td align="center">145.1</td></tr>
	      <tr><td align="left">LZO (lvl 1)</td><td align="center">9.7</td><td align="center">14.6</td><td align="center">1.95</td><td align="center">30.6</td><td align="center">230.5</td></tr>
	      <tr><td align="left">UCL (lvl 1)</td><td align="center">6.9</td><td align="center">38.3</td><td align="center">2.58</td><td align="center">11.7</td><td align="center">185.4</td></tr>
	    <tr><td colspan="6" class="thickline"></td></tr></tbody>
	  </table>
	  
	</div>

	<div class="figure" id="lzozlibuclWriteComparison"><a name="lzozlibuclWriteComparison"></a>
	  <img class="graphics" alt="Writing tables with several compressors.&#xA;	  " src="write-medium-lzo-zlib-ucl-comparison-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.3:</span> Writing tables with several compressors.
	  </div></div>
	</div>

	<div class="figure" id="lzozlibuclReadComparison"><a name="lzozlibuclReadComparison"></a>
	  <img class="graphics" alt="Reading tables with several compressors.&#xA;	  " src="read-medium-lzo-zlib-ucl-comparison-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.4:</span> Reading tables with several compressors.
	  </div></div>
	</div>

	<div class="figure" id="psycolzozlibuclWriteComparison"><a name="psycolzozlibuclWriteComparison"></a>
	  <img class="graphics" alt="Writing tables with several compressors and Psyco.&#xA;	  " src="write-medium-psyco-lzo-zlib-ucl-comparison-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.5:</span> Writing tables with several compressors and Psyco.
	  </div></div>
	</div>

	<div class="figure" id="psycolzozlibuclReadComparison"><a name="psycolzozlibuclReadComparison"></a>
	  <img class="graphics" alt="Reading tables with several compressors and Psyco.&#xA;	  " src="read-medium-psyco-lzo-zlib-ucl-comparison-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.6:</span> Reading tables with several compressors and Psyco.
	  </div></div>
	</div>

	<p>
	  By looking at graphs, you can expect that, generally
	  speaking, LZO would be the fastest both compressing and
	  uncompressing, but the one that achieves the worse
	  compression ratio (although that may be just ok for many
	  situations). UCL is the slowest when compressing, but is
	  faster than Zlib when decompressing, and, besides, it
	  achieves very good compression ratios (generally better than
	  Zlib). Zlib represents a balance between them: it's somewhat
	  slow compressing, the slowest during decompressing, but it
	  normally achieves fairly good compression ratios.
	</p>

	<p>So, if your ultimate goal is reading as fast as possible,
	  choose LZO. If you want to reduce as much as possible your
	  data, while retaining good read speed, choose UCL. If you
	  don't mind too much about the above parameters and/or
	  portability is important for you, Zlib is your best bet.
	</p>

	<p>The compression level that I recommend to use for all
	  compression libraries is 1. This is the lowest level of
	  compression, but if you take the approach suggested above,
	  normally the redundant data is to be found in the same
	  row, so the redundant data locality is very high and such
	  a small level of compression should be enough to achieve a
	  good compression ratio on your data tables, saving CPU
	  cycles for doing other things. Nonetheless, in some
	  situations you may want to check how compression level
	  affects your application.
	</p>

	<p> You can select the compression library and level by
	  setting the <tt class="verb">complib</tt> and <tt class="verb">compress</tt>
	  keywords in the <tt class="verb">Filters</tt> class (see <a href="usersguide4.html#FiltersClassDescr">4.12.1</a>). A compression level of 0
	  will completely disable compression (the default), 1 is the
	  less CPU time demanding level, while 9 is the maximum level
	  and most CPU intensive. Finally, have in mind that LZO is
	  not accepting a compression level right now, so, when using
	  LZO, 0 means that compression is not active, and any other
	  value means that LZO is active.
	</p>

      </div>

      <div>
	<h2 id="section5.3"><span class="headlinenumber"><a name="section5.3"></a>5.3 </span>Shuffling (or how to make the compression process
	  more effective)</h2>

	<p class="first">The <tt class="verb">HDF5</tt> library provides an interesting
	  filter that can leverage the results of your favorite
	  compressor. Its name is <em>shuffle</em>, and because it can
	  greatly benefit compression and don't take many CPU
	  resources, it is active by <em>default</em> in
	  <tt class="verb">PyTables</tt> whenever the compression is activated
	  (independently of the compressor choosed). It is of course
	  deactivated when compression is off (which is the default,
	  as you already should know).
	</p>

	<p>From the HDF5 reference manual:</p>

	<blockquote>The <em>shuffle</em> filter de-interlaces a block of data
	  by reordering the bytes. All the bytes from one consistent
	  byte position of each data element are placed together in
	  one block; all bytes from a second consistent byte position
	  of each data element are placed together a second block;
	  etc. For example, given three data elements of a 4-byte
	  datatype stored as 012301230123, shuffling will re-order
	  data as 000111222333. This can be a valuable step in an
	  effective compression algorithm because the bytes in each
	  byte position are often closely related to each other and
	  putting them together can increase the compression
	  ratio.
	</blockquote>

	<p>In <a href="#comprShuffleComparison">table&nbsp;5.2</a> you can
	  see a benchmark that shows how the <em>shuffle</em> filter
	  can help to the different libraries to compress data in
	  three table datasets. Generally speaking, <em>shuffle</em>
	  makes the writing process (shuffling+compressing) faster
	  (between 7% and 22%), which is an interesting result in
	  itself. However, the reading process
	  (unshuffling+decompressing) is slower, but by a lesser
	  extent (between 3% and 18%).
	</p>
	<p>But the most remarkable fact is the level of compression
	  that compressor filters can achieve after <em>shuffle</em>
	  has passed over the data: the total file size can be up to
	  40 times smaller than the uncompressed file, and up to 5
	  times smaller than the already compressed files (!). Of
	  course, the data for doing this test is synthetic, and
	  <em>shuffle</em> seems to do a great work with it, so in
	  general, the results will vary in your case. However, due to
	  the small drawbacks (read are slowed down by a small extent)
	  and its potential gains (faster writing, but specially much
	  better compression level), I do believe that it is a good
	  thing to have such a filter enabled by default in the battle
	  for discovering redundancy in your data.
	</p>

	<div class="table" id="comprShuffleComparison"><a name="comprShuffleComparison"></a>
	  
	  <table class="tabular" cellspacing="0" cellpadding="5" frame="hsides" rules="groups"><caption><span class="captionlabel">Table&nbsp;5.2:</span> Comparison between different compression
	    libraries. The tests has been conducted on a Pentium 4 at 2
	    GHz and a hard disk at 4200 RPM.</caption><col align="left"><col align="right"><col align="center"><col align="center"><col align="center"><col align="center">
	    <thead><tr><td colspan="6" class="thickline"></td></tr>
	      <tr><th align="left">Compr. Lib</th><th align="right">File size (MB)</th><th align="center">Time writing (s)</th><th align="center">Time reading (s)</th><th align="center">Speed writing (MB/s)</th><th align="center">Speed reading (MB/s)</th></tr>
	    <tr><td colspan="6" class="thinline"></td></tr></thead>
	    <tbody>
	      <tr><td align="left">NO COMPR</td><td align="right">165.4</td><td align="center">24.5</td><td align="center">17.13</td><td align="center">6.6</td><td align="center">9.6</td></tr>
	      <tr><td align="left">Zlib (lvl 1)</td><td align="right">26.4</td><td align="center">22.2</td><td align="center">5.77</td><td align="center">7.3</td><td align="center">28.4</td></tr>
	      <tr><td align="left">Zlib+shuffle</td><td align="right">4.0</td><td align="center">19.0</td><td align="center">5.94</td><td align="center">8.6</td><td align="center">27.6</td></tr>
	      <tr><td align="left">LZO (lvl 1)</td><td align="right">44.9</td><td align="center">17.8</td><td align="center">4.13</td><td align="center">9.2</td><td align="center">39.7</td></tr>
	      <tr><td align="left">LZO+shuffle</td><td align="right">4.3</td><td align="center">16.4</td><td align="center">5.03</td><td align="center">9.9</td><td align="center">32.6</td></tr>
	      <tr><td align="left">UCL (lvl 1)</td><td align="right">27.4</td><td align="center">48.8</td><td align="center">5.02</td><td align="center">3.3</td><td align="center">32.7</td></tr>
	      <tr><td align="left">UCL+shuffle</td><td align="right">3.5</td><td align="center">38.1</td><td align="center">5.31</td><td align="center">4.3</td><td align="center">30.9</td></tr>
	    <tr><td colspan="6" class="thickline"></td></tr></tbody>
	  </table>
	  
	</div>



      </div> 

      <div id="expectedRowsOptim"><a name="expectedRowsOptim"></a>
	<h2 id="section5.4"><span class="headlinenumber"><a name="section5.4"></a>5.4 </span>Informing <tt>PyTables</tt>
	  about expected number of rows in tables</h2>

	<p class="first">The underlying HDF5 library that is used by
	  <tt class="verb">PyTables</tt> takes the data in bunches of a
	  certain length, so-called <em>chunks</em>, to write them
	  on disk as a whole, i.e. the HDF5 library treats chunks as
	  atomic objects and disk I/O is always made in terms of
	  complete chunks. This allows data filters to be defined by
	  the application to perform tasks such as compression,
	  encryption, checksumming, etc. on entire chunks.
	</p>

	<p>An in-memory B-tree is used to map chunk structures on
	  disk. The more chunks that are allocated for a dataset the
	  larger the B-tree. Large B-trees take memory and causes
	  file storage overhead as well as more disk I/O and higher
	  contention for the metadata cache. Consequently, it's
	  important to balance between memory and I/O overhead
	  (small B-trees) and time to access to data (big B-trees).
	</p>

	<p><tt class="verb">PyTables</tt> can determine an optimum chunk size
	  to make B-trees adequate to your dataset size if you help
	  it by providing an estimation of the number of rows for a
	  table. This must be made in table creation time by passing
	  this value in the <tt class="verb">expectedrows</tt> keyword of
	  <tt class="verb">createTable</tt> method (see <a href="#createTableDescr"><strong>??</strong></a>).
	</p>

	<p>When your table size is bigger than 1 MB (take this figure
	  only as a reference, not strictly), by providing this guess
	  of the number of rows you will be optimizing the access to
	  your data. When the table size is larger than, say 100MB,
	  you are <b>strongly</b> suggested to
	  provide such a guess; failing to do that may cause your
	  application doing very slow I/O operations and demanding
	  <b>huge</b> amounts of memory. You
	  have been warned!.
	</p>

      </div>

      <div>
	<h2 id="section5.5"><span class="headlinenumber"><a name="section5.5"></a>5.5 </span>Selecting an User Entry Point (UEP) in your
	  tree</h2>

	<p class="first">If you have a <b>huge</b> tree in
	  your data file with many nodes on it, creating the object
	  tree would take long time. Many times, however, you are
	  interested only in access to a part of the complete tree, so
	  you won't strictly need PyTables to build the entire object
	  tree in-memory, but only the <em>interesting</em> part.
	</p>

	<p>This is where the <tt class="verb">rootUEP</tt> parameter of
	  <tt class="verb">openFile</tt> function (see <a href="#openFileDescr"><strong>??</strong></a>) can be helpful. Imagine that
	  you have a file called <tt class="verb">"test.h5"</tt> with the
	  associated tree that you can see in figure <a href="#rootUEPfig1">5.7</a>, and you are interested only in
	  the section marked in red.  You can avoid the build of all
	  the object tree by saying to <tt class="verb">openFile</tt> that your
	  root will be the <tt class="verb">/Group2/Group3</tt> group. That is:
	</p>
	<pre>
	  fileh = openFile("test.h5", rootUEP="/Group2/Group3")
	</pre>

	<p>As a result, the actual object tree built will be like the
	  one that can be seen in <a href="#rootUEPfig2">figure&nbsp;5.8</a>.
	</p>

	<p>Of course this has been a simple example and the use
	  of the <tt class="verb">rootUEP</tt> parameter was not very
	  necessary. But when you have <em>thousands</em> of nodes on
	  a tree, you will certainly appreciate the
	  <tt class="verb">rootUEP</tt> parameter.
	</p>

	<div class="figure" id="rootUEPfig1"><a name="rootUEPfig1"></a>
	  <img class="graphics" alt="Complete tree in file test.h5, and subtree of interest for&#xA;	    the us..." src="rootUEP1-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.7:</span> Complete tree in file <tt>test.h5</tt>, and subtree of interest for
	    the user.
	  </div></div>
	</div>

	<div class="figure" id="rootUEPfig2"><a name="rootUEPfig2"></a>
	  <img class="graphics" alt="Resulting object tree derived from the use of the&#xA;	    rootUEP paramet..." src="rootUEP2-web.png">
	  <div class="caption" style="width: 200px"><div class="caption-text"><span class="captionlabel">Figure&nbsp;5.8:</span> Resulting object tree derived from the use of the
	    <tt>rootUEP</tt> parameter.
	  </div></div>
	</div>
	
      </div>

      <div>
	<h2 id="section5.6"><span class="headlinenumber"><a name="section5.6"></a>5.6 </span>Compacting your <tt>PyTables</tt>
	  files
	</h2>

	<p class="first">Let's suppose that you have a file on which you have made a
	  lot of row deletions on one or more tables, or deleted many
	  leaves or even entire subtrees. These operations migth leave
	  <em>holes</em> (i.e. space that is not used anymore) in your
	  files, that may potentially affect not only the size of the
	  files but, more importantly, the performance of I/O. This is
	  because when you delete a lot of rows on a table, the space
	  is not automatically recovered on-the-flight. In addition,
	  if you add many more rows to a table than specified in the
	  <tt class="verb">expectedrows</tt> keyword in creation time this may
	  affect performace as well as explained in <a href="#expectedRowsOptim">section&nbsp;5.4</a>.
	</p>

	<p>In order to cope with these issues, you should be aware
	  that a handy <tt class="verb">PyTables</tt> utility called
	  <tt class="verb">ptrepack</tt> can be very useful, not only to
	  compact your already existing <em>leaky</em> files, but also
	  to adjust some internal parameters (both in memory and in
	  file) in order to create adequate buffer sizes and chunk
	  sizes for optimum I/O speed. Please, check the <a href="usersguide8.html#ptrepackDescr">appendix&nbsp;C.2</a> for a brief tutorial on
	  its use.
	</p>

	<p>Another thing that you might want to use
	  <tt class="verb">ptrepack</tt> for is changing the compression
	  filters or compression levels on your existing data for
	  different goals, like checking how this can affect both
	  final size and I/O performance, or getting ride of the
	  optional compressors like <tt class="verb">LZO</tt> or
	  <tt class="verb">UCL</tt> in your existing files in case you want to
	  use them with generic HDF5 tools that does not have support
	  for these filters.
	</p>

      </div>

    </div></div><div class="speedbar-bottom"><hr class="speedbar"><table class="speedbar"><tbody><tr><td style="text-align: left; width: 15%"><a href="usersguide4.html">previous</a></td><td style="text-align: center"><a href="usersguide.html#tb:table-of-contents">Table of Contents</a></td><td style="text-align: right; width: 15%"><a href="usersguide6.html">next</a></td></tr></tbody></table></div></body></html>