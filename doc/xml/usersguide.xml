<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">
<book lang="en">
  <title><literal>PyTables</literal> User's Guide</title>

  <subtitle>Hierarchical datasets in Python - Release 2.0 beta</subtitle>

  <bookinfo>
    <authorgroup>
      <author>
        <firstname>Francesc</firstname>

        <surname>Altet</surname>
      </author>

      <author>
        <firstname>Ivan</firstname>

        <surname>Vilata</surname>
      </author>

      <author>
        <firstname>Scott</firstname>

        <surname>Prater</surname>
      </author>

      <author>
        <firstname>Vicent</firstname>

        <surname>Mas</surname>
      </author>

      <author>
        <firstname>Tom</firstname>

        <surname>Hedley</surname>
      </author>

      <author>
        <firstname>Antonio</firstname>

        <surname>Valentino</surname>
      </author>

      <author>
        <firstname>Jeffrey</firstname>

        <surname>Whitaker</surname>
      </author>
    </authorgroup>

    <pubdate>$LastChangedDate: 2007-03-13 10:51:14 +0100 (dt, 13 mar 2007)
    $</pubdate>

    <mediaobject>
      <imageobject>
        <imagedata align="center" fileref="logo3-ombra.png" format="PNG"
                   scale="100" />
      </imageobject>
    </mediaobject>

    <copyright>
      <year>2002, 2003, 2004</year>

      <holder>Francesc Altet</holder>
    </copyright>

    <copyright>
      <year>2005, 2006, 2007</year>

      <holder>Cárabos Coop. V.</holder>
    </copyright>

    <legalnotice>
      <para><emphasis role="bold">Copyright Notice and Statement for
      <literal>PyTables</literal> Software Library and Utilities.
      </emphasis></para>

      <para>Redistribution and use in source and binary forms, with or without
      modification, are permitted provided that the following conditions are
      met:</para>

      <para>1. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.</para>

      <para>2. Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following disclaimer
      in the documentation and/or other materials provided with the
      distribution.</para>

      <para>THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
      OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
      WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
      DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
      INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
      (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
      SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
      HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
      STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
      ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
      POSSIBILITY OF SUCH DAMAGE.</para>
    </legalnotice>

    <!-- NCSA license -->

    <legalnotice>
      <para><emphasis role="bold">Copyright Notice and Statement for NCSA
      Hierarchical Data Format (HDF) Software Library and
      Utilities</emphasis></para>

      <para>NCSA HDF5 (Hierarchical Data Format 5) Software Library and
      Utilities Copyright 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005 by
      the Board of Trustees of the University of Illinois. All rights
      reserved.</para>

      <para>See more information about the terms of this license at: <ulink
      url="http://hdf.ncsa.uiuc.edu/HDF5/doc/Copyright.html"><literal>http://hdf.ncsa.uiuc.edu/HDF5/doc/Copyright.html</literal></ulink></para>
    </legalnotice>

    <!-- lrucache license -->

    <legalnotice>
      <para><emphasis role="bold">Copyright Notice and Statement for the
      lrucache.py module</emphasis></para>

      <para>Copyright 2004 Evan Prodromou. Licensed under the Academic Free
      License 2.1.</para>

      <para>See more information about the terms of this license at: <ulink
      url="http://opensource.org/licenses/afl-2.1.php"><literal>http://opensource.org/licenses/afl-2.1.php</literal></ulink></para>
    </legalnotice>

    <!-- Numexpr license -->

    <legalnotice>
      <para><emphasis role="bold">Copyright Notice and Statement for the
      Numexpr package</emphasis></para>

      <para>Copyright © 2006, David M. Cooke. All rights reserved.</para>

      <para>Redistribution and use in source and binary forms, with or without
      modification, are permitted provided that the following conditions are
      met:</para>

      <orderedlist numeration="loweralpha">
        <listitem>
          <para>Redistributions of source code must retain the above copyright
          notice, this list of conditions and the following disclaimer.</para>
        </listitem>

        <listitem>
          <para>Redistributions in binary form must reproduce the above
          copyright notice, this list of conditions and the following
          disclaimer in the documentation and/or other materials provided with
          the distribution.</para>
        </listitem>

        <listitem>
          <para>Neither the name of the owner nor the names of its
          contributors may be used to endorse or promote products derived from
          this software without specific prior written permission.</para>
        </listitem>
      </orderedlist>

      <para>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
      CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
      BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
      FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
      REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
      SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
      TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
      PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
      LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
      NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
      SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</para>
    </legalnotice>
  </bookinfo>

  <part label="I">
    <title>The PyTables Core Library</title>

    <chapter>
      <title>Introduction</title>

      <epigraph>
        <attribution>Gabriel García Márquez, <citetitle>A wise Catalan in
        "Cien años de soledad"</citetitle></attribution>

        <literallayout>
      La sabiduría no vale la pena si no es posible servirse de ella para
      inventar una nueva manera de preparar los garbanzos.

      [Wisdom isn't worth anything if you can't use it to come up with a new
      way to cook garbanzos.]
      </literallayout>
      </epigraph>

      <para></para>

      <para>The goal of PyTables is to enable the end user to manipulate
      easily data <emphasis>tables</emphasis> and <emphasis>array</emphasis>
      objects in a hierarchical structure. The foundation of the underlying
      hierarchical data organization is the excellent <literal>HDF5</literal>
      library (see <biblioref linkend="HDFWhatIs" />).</para>

      <para>It should be noted that this package is not intended to serve as a
      complete wrapper for the entire HDF5 API, but only to provide a
      flexible, <emphasis>very pythonic</emphasis> tool to deal with
      (arbitrarily) large amounts of data (typically bigger than available
      memory) in tables and arrays organized in a hierarchical and persistent
      disk storage structure.</para>

      <para>A table is defined as a collection of records whose values are
      stored in <emphasis>fixed-length</emphasis> fields. All records have the
      same structure and all values in each field have the same <emphasis>data
      type</emphasis>. The terms <emphasis>fixed-length</emphasis> and strict
      <emphasis>data types</emphasis> may seem to be a strange requirement for
      an interpreted language like Python, but they serve a useful function if
      the goal is to save very large quantities of data (such as is generated
      by many data acquisition systems, Internet services or scientific
      applications, for example) in an efficient manner that reduces demand on
      CPU time and I/O.</para>

      <para>In order to emulate in Python records mapped to HDF5 C structs
      PyTables implements a special class so as to easily define all its
      fields and other properties. PyTables also provides a powerful interface
      to mine data in tables. Records in tables are also known in the HDF5
      naming scheme as <emphasis>compound</emphasis> data types.</para>

      <para>For example, you can define arbitrary tables in Python simply by
      declaring a class with named fields and type information, such as in the
      following example:</para>

      <programlisting width="72">class Particle(IsDescription):
    name      = StringCol(16)   # 16-character String
    idnumber  = Int64Col()      # signed 64-bit integer
    ADCcount  = UInt16Col()     # unsigned short integer
    TDCcount  = UInt8Col()      # unsigned byte
    grid_i    = Int32Col()      # integer
    grid_j    = Int32Col()      # integer
    class Properties(IsDescription):  # A sub-structure (nested data-type)
        pressure = Float32Col(shape=(2,3)) # 2-D float array (single-precision)
        energy   = Float64Col(shape=(2,3,4)) # 3-D float array (double-precision)</programlisting>

      <para>You then pass this class to the table constructor, fill its rows
      with your values, and save (arbitrarily large) collections of them to a
      file for persistent storage. After that, the data can be retrieved and
      post-processed quite easily with PyTables or even with another HDF5
      application (in C, Fortran, Java or whatever language that provides a
      library to interface with HDF5).</para>

      <para>Other important entities in PyTables are
      <emphasis>array</emphasis> objects, which are analogous to tables with
      the difference that all of their components are homogeneous. They come
      in different flavors, like <emphasis>generic</emphasis> (they provide a
      quick and fast way to deal with for numerical arrays),
      <emphasis>enlargeable</emphasis> (arrays can be extended along a single
      dimension) and <emphasis>variable length</emphasis> (each row in the
      array can have a different number of elements).</para>

      <para>The next section describes the most interesting capabilities of
      PyTables.</para>

      <section>
        <title>Main Features</title>

        <para>PyTables takes advantage of the object orientation and
        introspection capabilities offered by Python, the powerful data
        management features of HDF5, and NumPy's flexibility and
        high-performance manipulation of large sets of objects organized in a
        grid-like fashion to provide these features:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis>Support for table entities:</emphasis> You can
            tailor your data adding or deleting records in your tables. Large
            numbers of rows (up to 2**62, much more than will fit into memory)
            are supported as well.</para>
          </listitem>

          <listitem>
            <para><emphasis>Multidimensional and nested table
            cells:</emphasis> You can declare a column to consist of values
            having any number of dimensions besides scalars, which is the only
            dimensionality allowed by the majority of relational databases.
            You can even declare columns that are made of other columns (of
            different types).</para>
          </listitem>

          <listitem>
            <para><emphasis>Indexing support for columns of tables:</emphasis>
            Very useful if you have large tables and you want to quickly look
            up for values in columns satisfying some criteria.</para>
            <note>
              <!-- XXXPRO -->
              <para>Column indexing is only available in PyTables Pro.</para>
            </note>
          </listitem>

          <listitem>
            <para><emphasis>Support for numerical arrays:</emphasis>
            <literal>NumPy</literal> (see <biblioref linkend="NumPy" />),
            <literal>Numeric</literal> (see <biblioref linkend="Numeric" />)
            and <literal>numarray</literal> (see <biblioref
            linkend="Numarray" />) arrays can be used as a useful complement
            of tables to store homogeneous data.</para>
          </listitem>

          <listitem>
            <para><emphasis>Enlargeable arrays:</emphasis> You can add new
            elements to existing arrays on disk in any dimension you want (but
            only one). Besides, you are able to access just a slice of your
            datasets by using the powerful extended slicing mechanism, without
            need to load all your complete dataset in memory.</para>
          </listitem>

          <listitem>
            <para><emphasis>Variable length arrays:</emphasis> The number of
            elements in these arrays can vary from row to row. This provides a
            lot of flexibility when dealing with complex data.</para>
          </listitem>

          <listitem>
            <para><emphasis>Supports a hierarchical data model:</emphasis>
            Allows the user to clearly structure all data. PyTables builds up
            an <emphasis>object tree</emphasis> in memory that replicates the
            underlying file data structure. Access to objects in the file is
            achieved by walking through and manipulating this object
            tree.</para>
          </listitem>

          <listitem>
            <para><emphasis>User defined metadata:</emphasis> Besides
            supporting system metadata (like the number of rows of a table,
            shape, flavor, etc.) the user may specify arbitrary metadata (as
            for example, room temperature, or protocol for IP traffic that was
            collected) that complement the meaning of actual data.</para>
          </listitem>

          <listitem>
            <para><emphasis>Ability to read/modify generic HDF5
            files:</emphasis> PyTables can access a wide range of objects in
            generic HDF5 files, like compound type datasets (that can be
            mapped to <literal>Table</literal> objects), homogeneous datasets
            (that can be mapped to <literal>Array</literal> objects) or
            variable length record datasets (that can be mapped to
            <literal>VLArray</literal> objects). Besides, if a dataset is not
            supported, it will be mapped to a special
            <literal>UnImplemented</literal> class (see <xref
            linkend="UnImplementedClassDescr" xrefstyle="select: label" />),
            that will let the user see that the data is there, although it
            will be unreachable (still, you will be able to access the
            attributes and some metadata in the dataset). With that, PyTables
            probably can access and <emphasis>modify</emphasis> most of the
            HDF5 files out there.</para>
          </listitem>

          <listitem>
            <para><emphasis>Data compression:</emphasis> Supports data
            compression (using the <emphasis>Zlib</emphasis>,
            <emphasis>LZO</emphasis> and <emphasis>bzip2</emphasis>
            compression libraries) out of the box. This is important when you
            have repetitive data patterns and don't want to spend time
            searching for an optimized way to store them (saving you time
            spent analyzing your data organization).</para>
          </listitem>

          <listitem>
            <para><emphasis>High performance I/O:</emphasis> On modern systems
            storing large amounts of data, tables and array objects can be
            read and written at a speed only limited by the performance of the
            underlying I/O subsystem. Moreover, if your data is compressible,
            even that limit is surmountable!</para>
          </listitem>

          <listitem>
            <para><emphasis>Support of files bigger than 2 GB:</emphasis>
            PyTables automatically inherits this capability from the
            underlying HDF5 library (assuming your platform supports the C
            long long integer, or, on Windows, __int64).</para>
          </listitem>

          <listitem>
            <para><emphasis>Architecture-independent:</emphasis> PyTables has
            been carefully coded (as HDF5 itself) with
            little-endian/big-endian byte ordering issues in mind. So, you can
            write a file on a big-endian machine (like a Sparc or MIPS) and
            read it on other little-endian machine (like an Intel or Alpha)
            without problems. In addition, it has been tested successfully
            with 64 bit platforms (Intel-64, AMD-64, PowerPC-G5, MIPS,
            UltraSparc) using code generated with 64 bit aware
            compilers.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section id="ObjectTreeSection">
        <title>The Object Tree</title>

        <para>The hierarchical model of the underlying HDF5 library allows
        PyTables to manage tables and arrays in a tree-like structure. In
        order to achieve this, an <emphasis>object tree</emphasis> entity is
        <emphasis>dynamically</emphasis> created imitating the HDF5 structure
        on disk. The HDF5 objects are read by walking through this object
        tree. You can get a good picture of what kind of data is kept in the
        object by examining the <emphasis>metadata</emphasis> nodes.</para>

        <para>The different nodes in the object tree are instances of PyTables
        classes. There are several types of classes, but the most important
        ones are the <literal>Node</literal>, <literal>Group</literal> and
        <literal>Leaf</literal> classes. All nodes in a PyTables tree are
        instances of the <literal>Node</literal> class. The
        <literal>Group</literal> and <literal>Leaf</literal> classes are
        descendants of <literal>Node</literal>. <literal>Group</literal>
        instances (referred to as <emphasis>groups</emphasis> from now on) are
        a grouping structure containing instances of zero or more groups or
        leaves, together with supplementary metadata. <literal>Leaf</literal>
        instances (referred to as <emphasis>leaves</emphasis>) are containers
        for actual data and can not contain further groups or leaves. The
        <literal>Table</literal>, <literal>Array</literal>,
        <literal>CArray</literal>, <literal>EArray</literal>,
        <literal>VLArray</literal> and <literal>UnImplemented</literal>
        classes are descendants of <literal>Leaf</literal>, and inherit all
        its properties.</para>

        <para>Working with groups and leaves is similar in many ways to
        working with directories and files on a Unix filesystem, i.e. a node
        (file or directory) is always a <emphasis>child</emphasis> of one and
        only one group (directory), its <emphasis>parent group</emphasis>
        <footnote>
            <para>PyTables does not support hard links – for the
            moment.</para>
          </footnote>. Inside of that group, the node is accessed by its
        <emphasis>name</emphasis>. As is the case with Unix directories and
        files, objects in the object tree are often referenced by giving their
        full (absolute) path names. In PyTables this full path can be
        specified either as string (such as
        <literal>'/subgroup2/table3'</literal>, using <literal>/</literal> as
        a parent/child separator) or as a complete object path written in a
        format known as the <emphasis>natural name</emphasis> schema (such as
        <literal>file.root.subgroup2.table3</literal>).</para>

        <para>Support for <emphasis>natural naming</emphasis> is a key aspect
        of PyTables. It means that the names of instance variables of the node
        objects are the same as the names of its children<footnote>
            <para>I got this simple but powerful idea from the excellent
            <literal>Objectify</literal> module by David Mertz (see <biblioref
            linkend="Objectify" />)</para>
          </footnote>. This is very <emphasis>Pythonic</emphasis> and
        intuitive in many cases. Check the tutorial <xref
        linkend="readingAndSelectingUsage" xrefstyle="select: label" /> for
        usage examples.</para>

        <para>You should also be aware that not all the data present in a file
        is loaded into the object tree. Only the <emphasis>metadata</emphasis>
        (i.e. special data that describes the structure of the actual data) is
        loaded. The actual data is not read until you request it (by calling a
        method on a particular node). Using the object tree (the metadata) you
        can retrieve information about the objects on disk such as table
        names, titles, column names, data types in columns, numbers of rows,
        or, in the case of arrays, their shapes, typecodes, etc. You can also
        search through the tree for specific kinds of data then read it and
        process it. In a certain sense, you can think of PyTables as a tool
        that applies the same introspection capabilities of Python objects to
        large amounts of data in persistent storage.</para>

        <para>It is worth noting that PyTables sports a <emphasis>node cache
        system</emphasis> that loads nodes on demand, and unloads nodes that
        have not been used for some time (following a <emphasis>Least Recently
        Used</emphasis> schema). This feature allows opening files with large
        hierarchies very quickly and with a low memory consumption, while
        retaining all the powerful browsing capabilities of the previous
        implementation of the object tree. See <biblioref
        linkend="NewObjectTreeCacheRef" /> for more facts about the advantages
        introduced by this new node cache system.</para>

        <para>To better understand the dynamic nature of this object tree
        entity, let's start with a sample PyTables script (which you can find
        in <literal>examples/objecttree.py</literal>) to create an HDF5
        file:</para>

        <screen>from tables import *

class Particle(IsDescription):
    identity = StringCol(itemsize=22, dflt=" ", pos=0)  # character String
    idnumber = Int16Col(dflt=1, pos = 1)  # short integer
    speed    = Float32Col(dflt=1, pos = 1)  # single-precision

# Open a file in "w"rite mode
fileh = openFile("objecttree.h5", mode = "w")
# Get the HDF5 root group
root = fileh.root

# Create the groups:
group1 = fileh.createGroup(root, "group1")
group2 = fileh.createGroup(root, "group2")

# Now, create an array in root group
array1 = fileh.createArray(root, "array1", ["string", "array"], "String array")
# Create 2 new tables in group1
table1 = fileh.createTable(group1, "table1", Particle)
table2 = fileh.createTable("/group2", "table2", Particle)
# Create the last table in group2
array2 = fileh.createArray("/group1", "array2", [1,2,3,4])

# Now, fill the tables:
for table in (table1, table2):
    # Get the record object associated with the table:
    row = table.row
    # Fill the table with 10 records
    for i in xrange(10):
        # First, assign the values to the Particle record
        row['identity']  = 'This is particle: %2d' % (i)
        row['idnumber'] = i
        row['speed']  = i * 2.
        # This injects the Record values
        row.append()

    # Flush the table buffers
    table.flush()

# Finally, close the file (this also will flush all the remaining buffers!)
fileh.close()</screen>

        <para>This small program creates a simple HDF5 file called
        <literal>objecttree.h5</literal> with the structure that appears in
        <xref linkend="objecttree-h5" xrefstyle="select: label" /><footnote>
            <para>We have used ViTables (see <biblioref
            linkend="ViTablesRef" />) in order to create this snapshot.</para>
          </footnote>. When the file is created, the metadata in the object
        tree is updated in memory while the actual data is saved to disk. When
        you close the file the object tree is no longer available. However,
        when you reopen this file the object tree will be reconstructed in
        memory from the metadata on disk, allowing you to work with it in
        exactly the same way as when you originally created it.</para>

        <figure id="objecttree-h5">
          <title>An HDF5 example with 2 subgroups, 2 tables and 1
          array.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="objecttree-h5.png"
                         format="PNG" scale="55" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="objecttree-h5.png"
                         format="PNG" scale="100" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>In <xref linkend="objecttree" xrefstyle="select: label" /> you
        can see an example of the object tree created when the above
        <literal>objecttree.h5</literal> file is read (in fact, such an object
        tree is always created when reading any supported generic HDF5 file).
        It is worthwhile to take your time to understand it<footnote>
            <para>Bear in mind, however, that this diagram is
            <emphasis>not</emphasis> a standard UML class diagram; it is
            rather meant to show the connections between the PyTables objects
            and some of its most important attributes and methods.</para>
          </footnote>. It will help you understand the relationships of
        in-memory PyTables objects.</para>

        <figure id="objecttree">
          <title>A PyTables object tree example.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="objecttree.svg" format="SVG"
                         scale="40" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="objecttree.png" format="PNG"
                         scale="100" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </chapter>

    <chapter>
      <title>Installation</title>

      <epigraph>
        <attribution>Albert Einstein</attribution>

        <literallayout>
      Make things as simple as possible, but not any simpler.
      </literallayout>
      </epigraph>

      <para></para>

      <para>The Python <literal>Distutils</literal> are used to build and
      install PyTables, so it is fairly simple to get the application up and
      running. If you want to install the package from sources you can go on
      reading to the next section.</para>

      <para>However, if you are running Windows and want to install
      precompiled binaries, you can jump straight to <xref
      linkend="binaryInstallationDescr" xrefstyle="select: label" />. In
      addition, packages are available for many different Linux distributions,
      for instance <ulink url="http://www.t2-project.org">T2 Project</ulink>,
      <ulink url="http://www.debian.org/"><literal>Debian</literal></ulink>,
      or <ulink
      url="http://www.ubuntu.com/"><literal>Ubuntu</literal></ulink>, among
      others. There are also packages for other Unices like <ulink
      url="http://www.freshports.org/"><literal>FreeBSD</literal></ulink> or
      <ulink
      url="http://www.opendarwin.org/"><literal>MacOSX</literal></ulink></para>

      <section id="sourceInstallationDescr">
        <title>Installation from source</title>

        <para>These instructions are for both Unix/MacOS X and Windows
        systems. If you are using Windows, it is assumed that you have a
        recent version of <literal>MS Visual C++</literal> compiler installed.
        A <literal>GCC</literal> compiler is assumed for Unix, but other
        compilers should work as well.</para>

        <para>Extensions in PyTables have been developed in Pyrex (see
        <biblioref linkend="Pyrex" />) and the C language. You can rebuild
        everything from scratch if you have Pyrex installed, but this is not
        necessary, as the Pyrex compiled source is included in the source
        distribution.</para>

        <para>To compile PyTables you will need a recent version of Python,
        the HDF5 (C flavor) library, and the <literal>NumPy</literal> (see
        <biblioref linkend="NumPy" />) package. Although you won't need
        <literal>numarray</literal> (see <biblioref linkend="Numarray" />) or
        <literal>Numeric</literal> (see <biblioref linkend="Numeric" />) in
        order to compile PyTables, they are supported; you only need a
        reasonably recent version of them (&gt;= 1.5.2 for numarray and &gt;=
        24.2 for Numeric) if you plan on using them in your applications. If
        you already have <literal>numarray</literal> and/or
        <literal>Numeric</literal> installed, the test driver module will
        detect them and will run the tests for <literal>numarray</literal>
        and/or <literal>Numeric</literal> automatically.</para>

        <section id="PrerequisitesSourceDescr">
          <title>Prerequisites</title>

          <para>First, make sure that you have at least Python 2.4, HDF5 1.6.5
          and NumPy 1.0.1 or higher installed (I'm using HDF5 1.6.5 and NumPy
          1.0.1 currently). If you don't, fetch and install them before
          proceeding.</para>

          <para>Compile and install these packages (but see <xref
          linkend="prerequisitesBinInst" xrefstyle="select: label" /> for
          instructions on how to install precompiled binaries if you are not
          willing to compile the prerequisites on Windows systems).</para>

          <para>For compression (and possibly improved performance), you will
          need to install the <literal>Zlib</literal> (see <biblioref
          linkend="zlibRef" />), which is also required by HDF5 as well. You
          may also optionally install the excellent <literal>LZO</literal>
          compression library (see <biblioref linkend="lzoRef" /> and <xref
          linkend="compressionIssues" xrefstyle="select: label" />). The
          high-performance bzip2 compression library can also be used with
          PyTables (see <biblioref linkend="bzip2Ref" />).</para>

          <variablelist>
            <varlistentry>
              <term><emphasis role="bold">Unix</emphasis></term>

              <listitem>
                <para><literal>setup.py</literal> will detect
                <literal>HDF5</literal>, <literal>LZO</literal>, or
                <literal>bzip2</literal> libraries and include files under
                <literal>/usr</literal> or <literal>/usr/local</literal>; this
                will cover most manual installations as well as installations
                from packages. If <literal>setup.py</literal> can not find
                <literal>libhdf5</literal> (or <literal>liblzo</literal>, or
                <literal>libbz2</literal> that you may wish to use) or if you
                have several versions of a library installed and want to use a
                particular one, then you can set the path to the resource in
                the environment, by setting the values of the
                <literal>HDF5_DIR</literal>, <literal>LZO_DIR</literal>, or
                <literal>BZIP2_DIR</literal> environment variables to the path
                to the particular resource. You may also specify the locations
                of the resource root directories on the
                <literal>setup.py</literal> command line. For example:</para>

                <screen>--hdf5=/stuff/hdf5-1.6.5
--lzo=/stuff/lzo-1.08
--bzip2=/stuff/bzip2-1.0.3</screen>

                <para>If your HDF5 library was built as a shared library not
                in the runtime load path, then you can specify the additional
                linker flags needed to find the shared library on the command
                line as well. For example:</para>

                <screen>--lflags="-Xlinker -rpath -Xlinker /stuff/hdf5-1.6.5/lib"</screen>

                <para>You may also want to try setting the LD_LIBRARY_PATH
                environment variable to point to the directory where the
                shared libraries can be found. Check your compiler and linker
                documentation as well as the Python
                <literal>Distutils</literal> documentation for the correct
                syntax or environment variable names.</para>

                <para>It is also possible to link with specific libraries by
                setting the <literal>LIBS</literal> environment
                variable:</para>

                <screen>LIBS="hdf5-1.6.5 nsl"</screen>

                <para>Finally, you can give additional flags to your compiler
                by passing them to the <literal>--cflags</literal>
                flag:</para>

                <screen>--cflags="-w -O3"</screen>

                <para>In the above case, a <literal>gcc</literal> compiler is
                used and you instructed it to suppress all the warnings and
                set the level 3 of optimization.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term><emphasis role="bold">Windows</emphasis></term>

              <listitem>
                <para>Once you have installed the prerequisites,
                <literal>setup.py</literal> needs to know where the necessary
                library <emphasis>stub</emphasis> (<literal>.lib</literal>)
                and <emphasis>header</emphasis> (<literal>.h</literal>) files
                are installed. You can set the path to the
                <literal>include</literal> and <literal>dll</literal>
                directories for the HDF5 (mandatory) and LZO or BZIP2
                (optional) libraries in the environment, by setting the values
                of the <literal>HDF5_DIR</literal>,
                <literal>LZO_DIR</literal>, or <literal>BZIP2_DIR</literal>
                environment variables to the path to the particular resource.
                For example:</para>

                <screen>set HDF5_DIR=c:\stuff\5-165-win
set LZO_DIR=c:\stuff\lzo-1-08
set BZIP2_DIR=c:\stuff\bzip2-1-0-3</screen>

                <para>You may also specify the locations of the resource root
                directories on the <literal>setup.py</literal> command line.
                For example:</para>

                <screen>--hdf5=c:\stuff\5-165-win
--lzo=c:\stuff\lzo-1-08
--bzip2=c:\stuff\bzip2-1-0-3</screen>

                <para>You can get ready-to-use Windows binaries and other
                development files for most of those libraries from the
                GnuWin32 project (see <biblioref
                linkend="GnuWin32" />).</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>

        <section id="PyTablesSourceInstallationDescr">
          <title>PyTables package installation</title>

          <para>Once you have installed the HDF5 library and the NumPy
          package, you can proceed with the PyTables package itself:</para>

          <orderedlist>
            <listitem>
              <para>Run this command from the main PyTables distribution
              directory, including any extra command line arguments as
              discussed above:</para>

              <screen>python setup.py build_ext --inplace</screen>

              <para>Depending on the compiler flags used when compiling your
              Python executable, there may appear many warnings. Don't worry,
              almost all of them are caused by variables declared but never
              used. That's normal in Pyrex extensions.</para>
            </listitem>

            <listitem>
              <para>To run the test suite, execute this command:</para>

              <variablelist>
                <varlistentry>
                  <term><emphasis role="bold">Unix</emphasis></term>

                  <listitem>
                    <para>In the <literal>sh</literal> shell and its
                    variants:</para>

                    <screen>PYTHONPATH=.:$PYTHONPATH  python tables/tests/test_all.py</screen>

                    <para>or, if you prefer:</para>

                    <screen>PYTHONPATH=.:$PYTHONPATH  python -c "import tables; tables.test()"</screen>

                    <para>Both commands do the same thing.</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term><emphasis role="bold">Windows</emphasis></term>

                  <listitem>
                    <para>Open the command prompt (<literal>cmd.exe</literal>
                    or <literal>command.com</literal>) and type:</para>

                    <screen>set PYTHONPATH=.;%PYTHONPATH%
python tables\tests\test_all.py</screen>

                    <para>or:</para>

                    <screen>set PYTHONPATH=.;%PYTHONPATH%
python -c "import tables; tables.test()"</screen>
                  </listitem>
                </varlistentry>
              </variablelist>

              <para>If you would like to see verbose output from the tests
              simply add the <literal>-v</literal> flag or the word
              <literal>verbose</literal> to the command line. You can also run
              only the tests in a particular test module. For example, to
              execute just the <literal>test_types</literal> test suite, you
              only have to specify it:</para>

              <screen>python tables/tests/test_types.py -v  # change to backslashes for win</screen>

              <para>You have other options to pass to the
              <literal>test_all.py</literal> driver: <screen>python tables/tests/test_all.py --heavy  # change to backslashes for win</screen>
              The command above runs every test in the test unit. Beware, it
              can take a lot of time, CPU and memory resources to complete.
              <screen>python tables/tests/test_all.py --show-versions  # change to backslashes for win</screen>
              The command above shows the versions for all the packages that
              PyTables relies on. Please be sure to include this when
              reporting bugs. <screen>python tables/tests/test_all.py --show-memory  # only under Linux 2.6.x</screen>
              The command above prints out the evolution of the memory
              consumption after each test module completion. It's useful for
              locating memory leaks in PyTables (or packages behind it). Only
              valid for Linux 2.6.x kernels.</para>

              <para>And last, but not least, in case a test fails, please run
              the failing test module again and enable the verbose output:
              <screen>python tables/tests/test_&lt;module&gt;.py -v verbose</screen>
              and, very important, obtain your PyTables version information by
              using the <literal>--show-versions</literal> flag (see above)
              and send back both outputs to developers so that we may continue
              improving PyTables.</para>

              <para>If you run into problems because Python can not load the
              HDF5 library or other shared libraries:</para>

              <variablelist>
                <varlistentry>
                  <term><emphasis role="bold">Unix</emphasis></term>

                  <listitem>
                    <para>Try setting the LD_LIBRARY_PATH or equivalent
                    environment variable to point to the directory where the
                    missing libraries can be found.</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term><emphasis role="bold">Windows</emphasis></term>

                  <listitem>
                    <para>Put the DLL libraries
                    (<literal>hdf5dll.dll</literal> and, optionally,
                    <literal>lzo1.dll</literal> and
                    <literal>bzip2.dll</literal>) in a directory listed in
                    your <literal>PATH</literal> environment variable or in
                    <literal>python_installation_path\Lib\site-packages\tables</literal>
                    (the last directory may not exist yet, so if you want to
                    install the DLLs there, you should do so
                    <emphasis>after</emphasis> installing the PyTables
                    package). The <literal>setup.py</literal> installation
                    program will print out a warning to that effect if the
                    libraries can not be found.</para>
                  </listitem>
                </varlistentry>
              </variablelist>
            </listitem>

            <listitem>
              <para>To install the entire PyTables Python package, change back
              to the root distribution directory and run the following command
              (make sure you have sufficient permissions to write to the
              directories where the PyTables files will be installed):</para>

              <screen>python setup.py install</screen>

              <para>Of course, you will need super-user privileges if you want
              to install PyTables on a system-protected area. You can select,
              though, a different place to install the package using the
              <literal>--prefix</literal> flag:</para>

              <screen>python setup.py install --prefix="/home/myuser/mystuff"</screen>

              <para>Have in mind, however, that if you use the
              <literal>--prefix</literal> flag to install in a non-standard
              place, you should properly setup your
              <literal>PYTHONPATH</literal> environment variable, so that the
              Python interpreter would be able to find your new PyTables
              installation.</para>

              <para>You have more installation options available in the
              Distutils package. Issue a: <screen>python setup.py install --help</screen>
              for more information on that subject.</para>
            </listitem>
          </orderedlist>

          <para>That's it! Now you can skip to the next chapter to learn how
          to use PyTables.</para>
        </section>
      </section>

      <section id="binaryInstallationDescr">
        <title>Binary installation (Windows)</title>

        <para>This section is intended for installing precompiled binaries on
        Windows platforms. You may also find it useful for instructions on how
        to install <emphasis>binary prerequisites</emphasis> even if you want
        to compile PyTables itself on Windows.</para>

        <section id="prerequisitesBinInst">
          <title>Windows prerequisites</title>

          <para>First, make sure that you have Python 2.4 or higher, HDF5
          1.6.5 or higher and NumPy 1.0.1 or higher installed (PyTables
          binaries have been built using HDF5 1.6.5 and NumPy 1.0.1).</para>

          <para>For the HDF5 library it should be enough to manually copy the
          <literal>hdf5dll.dll</literal>, <literal>zlib1.dll</literal>
          <emphasis>and</emphasis> <literal>szipdll.dll</literal> files to a
          directory in your <literal>PATH</literal> environment variable (for
          example <literal>C:\WINDOWS\SYSTEM32</literal>) or
          <literal>python_installation_path\Lib\site-packages\tables</literal>
          (the last directory may not exist yet, so if you want to install the
          DLLs there, you should do so <emphasis>after</emphasis> installing
          the PyTables package).</para>

          <para><emphasis>Caveat:</emphasis> When downloading the binary
          distribution for HDF5 libraries, select the one compiled with a .NET
          version of MSVC compiler. The file
          <literal>5-165-win-net.zip</literal> was compiled with the MSVC 7.1
          (aka "<literal>.NET 2003</literal>") and you
          <emphasis>must</emphasis> choose it if you want to run PyTables with
          Python 2.4.x or 2.5.x series. You have been warned!</para>

          <para>To enable compression with the optional LZO or bzip2 libraries
          (see the <xref linkend="compressionIssues"
          xrefstyle="select: label" /> for hints about how they may be used to
          improve performance), fetch and install the <literal>LZO</literal>
          (choose v1.x, <literal>LZO</literal> v2.x is not supported in
          precompiled Windows builds) and <literal>bzip2</literal> binaries
          from <biblioref linkend="GnuWin32" />. Normally, you will only need
          to fetch and install the
          <literal>&lt;package&gt;-&lt;version&gt;-bin.zip</literal> file and
          copy the <literal>lzo1.dll</literal> or <literal>bzip2.dll</literal>
          files in a directory in the <literal>PATH</literal> environment
          variable, or in
          <literal>python_installation_path\Lib\site-packages\tables</literal>
          (the last directory may not exist yet, so if you want to install the
          DLLs there, you should do so <emphasis>after</emphasis> installing
          the PyTables package), so that they can be found by the PyTables
          extensions.</para>

          <para>Please note that PyTables has internal machinery for dealing
          with uninstalled optional compression libraries, so, you don't need
          to install any of LZO or bzip2 dynamic libraries if you don't want
          to.</para>
        </section>

        <section id="PyTablesBinInstallDescr">
          <title>PyTables package installation</title>

          <para>Download the
          <literal>tables-&lt;version&gt;.win32-py&lt;version&gt;.exe</literal>
          file and execute it.</para>

          <para>You can (and <emphasis>you should</emphasis>) test your
          installation by running the next commands: <screen>&gt;&gt;&gt; import tables
&gt;&gt;&gt; tables.test()</screen> on your favorite python shell. If all the
          tests pass (possibly with a few warnings, related to the potential
          unavailability of LZO or bzip2 libs) you already have a working,
          well-tested copy of PyTables installed! If any test fails, please
          copy the output of the error messages as well as the output of:
          <screen>&gt;&gt;&gt; tables.print_versions()</screen> and mail them
          to the developers so that the problem can be fixed in future
          releases.</para>

          <para>You can proceed now to the next chapter to see how to use
          PyTables.</para>
        </section>
      </section>
    </chapter>

    <chapter id="usage">
      <title>Tutorials</title>

      <epigraph>
        <attribution>Lyrics: Vicent Andrés i Estellés. Music: Ovidi Montllor,
        <citetitle>M'aclame a tu</citetitle></attribution>

        <literallayout>
      Seràs la clau que obre tots els panys,
      seràs la llum, la llum il.limitada,
      seràs confí on l'aurora comença,
      seràs forment, escala il.luminada!
      </literallayout>
      </epigraph>

      <para></para>

      <para>This chapter consists of a series of simple yet comprehensive
      tutorials that will enable you to understand PyTables' main features. If
      you would like more information about some particular instance variable,
      global function, or method, look at the doc strings or go to the library
      reference in <xref linkend="libraryReference"
      xrefstyle="select: label" />. If you are reading this in PDF or HTML
      formats, follow the corresponding hyperlink near each newly introduced
      entity.</para>

      <para>Please note that throughout this document the terms
      <emphasis>column</emphasis> and <emphasis>field</emphasis> will be used
      interchangeably, as will the terms <emphasis>row</emphasis> and
      <emphasis>record</emphasis>.</para>

      <section>
        <title>Getting started</title>

        <para>In this section, we will see how to define our own records in
        Python and save collections of them (i.e. a
        <emphasis>table</emphasis>) into a file. Then we will select some of
        the data in the table using Python cuts and create NumPy arrays to
        store this selection as separate objects in a tree.</para>

        <para>In <emphasis>examples/tutorial1-1.py</emphasis> you will find
        the working version of all the code in this section. Nonetheless, this
        tutorial series has been written to allow you reproduce it in a Python
        interactive console. I encourage you to do parallel testing and
        inspect the created objects (variables, docs, children objects, etc.)
        during the course of the tutorial!</para>

        <section>
          <title>Importing <literal>tables</literal> objects</title>

          <para>Before starting you need to import the public objects in the
          <literal>tables</literal> package. You normally do that by
          executing:</para>

          <screen>&gt;&gt;&gt; import tables</screen>

          <para>This is the recommended way to import
          <literal>tables</literal> if you don't want to pollute your
          namespace. However, PyTables has a contained set of first-level
          primitives, so you may consider using the alternative:</para>

          <screen>&gt;&gt;&gt; from tables import *</screen>

          <para>If you are going to work with <literal>NumPy</literal> (or
          <literal>numarray</literal> or <literal>Numeric</literal>) arrays
          (and normally, you will) you will also need to import functions from
          them. So most PyTables programs begin with:</para>

          <screen>&gt;&gt;&gt; import tables        # but in this tutorial we use "from tables import *"
&gt;&gt;&gt; import numpy         # or "import numarray" or "import Numeric"</screen>
        </section>

        <section>
          <title>Declaring a Column Descriptor</title>

          <para>Now, imagine that we have a particle detector and we want to
          create a table object in order to save data retrieved from it. You
          need first to define the table, the number of columns it has, what
          kind of object is contained in each column, and so on.</para>

          <para>Our particle detector has a TDC (Time to Digital Converter)
          counter with a dynamic range of 8 bits and an ADC (Analogical to
          Digital Converter) with a range of 16 bits. For these values, we
          will define 2 fields in our record object called
          <literal>TDCcount</literal> and <literal>ADCcount</literal>. We also
          want to save the grid position in which the particle has been
          detected, so we will add two new fields called
          <literal>grid_i</literal> and <literal>grid_j</literal>. Our
          instrumentation also can obtain the pressure and energy of the
          particle. The resolution of the pressure-gauge allows us to use a
          simple-precision float to store <literal>pressure</literal>
          readings, while the <literal>energy</literal> value will need a
          double-precision float. Finally, to track the particle we want to
          assign it a name to identify the kind of the particle it is and a
          unique numeric identifier. So we will add two more fields:
          <literal>name</literal> will be a string of up to 16 characters, and
          <literal>idnumber</literal> will be an integer of 64 bits (to allow
          us to store records for extremely large numbers of
          particles).</para>

          <para>Having determined our columns and their types, we can now
          declare a new <literal>Particle</literal> class that will contain
          all this information:</para>

          <screen>&gt;&gt;&gt; from tables import *
&gt;&gt;&gt; class Particle(IsDescription):
      name      = StringCol(16)   # 16-character String
      idnumber  = Int64Col()      # Signed 64-bit integer
      ADCcount  = UInt16Col()     # Unsigned short integer
      TDCcount  = UInt8Col()      # unsigned byte
      grid_i    = Int32Col()      # 32-bit integer
      grid_j    = Int32Col()      # 32-bit integer
      pressure  = Float32Col()    # float  (single-precision)
      energy    = Float64Col()    # double (double-precision)
 
&gt;&gt;&gt; </screen>

          <para>This definition class is self-explanatory. Basically, you
          declare a class variable for each field you need. As its value you
          assign an instance of the appropriate <literal>Col</literal>
          subclass, according to the kind of column defined (the data type,
          the length, the shape, etc). See the <xref linkend="ColClassDescr"
          xrefstyle="select: label" /> for a complete description of these
          subclasses. See also <xref linkend="datatypesSupported"
          xrefstyle="select: label" /> for a list of data types supported by
          the <literal>Col</literal> constructor.</para>

          <para>From now on, we can use <literal>Particle</literal> instances
          as a descriptor for our detector data table. We will see later on
          how to pass this object to construct the table. But first, we must
          create a file where all the actual data pushed into our table will
          be saved.</para>
        </section>

        <section>
          <title>Creating a PyTables file from scratch</title>

          <para>Use the first-level <literal>openFile</literal> function (see
          <xref linkend="openFileDescr" />) to create a PyTables file:</para>

          <screen>&gt;&gt;&gt; h5file = openFile("tutorial1.h5", mode = "w", title = "Test file")</screen>

          <para><literal>openFile()</literal> (see <xref
          linkend="openFileDescr" />) is one of the objects imported by the
          "<literal>from tables import *</literal>" statement. Here, we are
          saying that we want to create a new file in the current working
          directory called "<literal>tutorial1.h5</literal>" in
          "<literal>w</literal>"rite mode and with an descriptive title string
          ("<literal>Test file</literal>"). This function attempts to open the
          file, and if successful, returns the <literal>File</literal> (see
          <xref linkend="FileClassDescr" xrefstyle="select: label" />) object
          instance <literal>h5file</literal>. The root of the object tree is
          specified in the instance's <literal>root</literal>
          attribute.</para>
        </section>

        <section>
          <title>Creating a new group</title>

          <para>Now, to better organize our data, we will create a group
          called <emphasis>detector</emphasis> that branches from the root
          node. We will save our particle data table in this group.</para>

          <screen>&gt;&gt;&gt; group = h5file.createGroup("/", 'detector', 'Detector information')</screen>

          <para>Here, we have taken the <literal>File</literal> instance
          <literal>h5file</literal> and invoked its
          <literal>createGroup()</literal> method (see <xref
          linkend="createGroupDescr" xrefstyle="select: label" />) to create a
          new group called <emphasis>detector</emphasis> branching from
          "<emphasis>/</emphasis>" (another way to refer to the
          <literal>h5file.root</literal> object we mentioned above). This will
          create a new <literal>Group</literal> (see <xref
          linkend="GroupClassDescr" xrefstyle="select: label" />) object
          instance that will be assigned to the variable
          <literal>group</literal>.</para>
        </section>

        <section>
          <title>Creating a new table</title>

          <para>Let's now create a <literal>Table</literal> (see <xref
          linkend="TableClassDescr" xrefstyle="select: label" />) object as a
          branch off the newly-created group. We do that by calling the
          <literal>createTable</literal> (see <xref linkend="createTableDescr"
          />) method of the <literal>h5file</literal> object:</para>

          <screen>&gt;&gt;&gt; table = h5file.createTable(group, 'readout', Particle, "Readout example")</screen>

          <para>We create the <literal>Table</literal> instance under
          <literal>group</literal>. We assign this table the node name
          "<emphasis>readout</emphasis>". The <literal>Particle</literal>
          class declared before is the <emphasis>description</emphasis>
          parameter (to define the columns of the table) and finally we set
          "<emphasis>Readout example</emphasis>" as the
          <literal>Table</literal> title. With all this information, a new
          <literal>Table</literal> instance is created and assigned to the
          variable <emphasis>table</emphasis>.</para>

          <para>If you are curious about how the object tree looks right now,
          simply <literal>print</literal> the <literal>File</literal> instance
          variable <emphasis>h5file</emphasis>, and examine the output:</para>

          <screen>&gt;&gt;&gt; print h5file
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 11:06:12 2007'
Object Tree:
/ (RootGroup) 'Test file'
/detector (Group) 'Detector information'
/detector/readout (Table(0L,)) 'Readout example'</screen>

          <para>As you can see, a dump of the object tree is displayed. It's
          easy to see the <literal>Group</literal> and
          <literal>Table</literal> objects we have just created. If you want
          more information, just type the variable containing the
          <literal>File</literal> instance:</para>

          <screen>&gt;&gt;&gt; h5file
File(filename='tutorial1.h5', title='Test file', mode='w', trMap={}, rootUEP='/', filters=Filters(complevel=0, shuffle=False, fletcher32=False))
/ (RootGroup) 'Test file'
/detector (Group) 'Detector information'
/detector/readout (Table(0L,)) 'Readout example'
  description := {
  "ADCcount": UInt16Col(shape=(), dflt=0, pos=0),
  "TDCcount": UInt8Col(shape=(), dflt=0, pos=1),
  "energy": Float64Col(shape=(), dflt=0.0, pos=2),
  "grid_i": Int32Col(shape=(), dflt=0, pos=3),
  "grid_j": Int32Col(shape=(), dflt=0, pos=4),
  "idnumber": Int64Col(shape=(), dflt=0, pos=5),
  "name": StringCol(itemsize=16, shape=(), dflt='', pos=6),
  "pressure": Float32Col(shape=(), dflt=0.0, pos=7)}
  byteorder := 'little'</screen>

          <para>More detailed information is displayed about each object in
          the tree. Note how <literal>Particle</literal>, our table descriptor
          class, is printed as part of the <emphasis>readout</emphasis> table
          description information. In general, you can obtain much more
          information about the objects and their children by just printing
          them. That introspection capability is very useful, and I recommend
          that you use it extensively.</para>

          <para>The time has come to fill this table with some values. First
          we will get a pointer to the <literal>Row</literal> (see <xref
          linkend="RowClassDescr" xrefstyle="select: label" />) instance of
          this <literal>table</literal> instance:</para>

          <screen>&gt;&gt;&gt; particle = table.row</screen>

          <para>The <literal>row</literal> attribute of
          <literal>table</literal> points to the <literal>Row</literal>
          instance that will be used to write data rows into the table. We
          write data simply by assigning the <literal>Row</literal> instance
          the values for each row as if it were a dictionary (although it is
          actually an <emphasis>extension class</emphasis>), using the column
          names as keys.</para>

          <para>Below is an example of how to write rows: <screen>&gt;&gt;&gt; for i in xrange(10):
      particle['name']  = 'Particle: %6d' % (i)
      particle['TDCcount'] = i % 256
      particle['ADCcount'] = (i * 256) % (1 &lt;&lt; 16)
      particle['grid_i'] = i
      particle['grid_j'] = 10 - i
      particle['pressure'] = float(i*i)
      particle['energy'] = float(particle['pressure'] ** 4)
      particle['idnumber'] = i * (2 ** 34)
      # Insert a new particle record
      particle.append()

&gt;&gt;&gt; </screen></para>

          <para>This code should be easy to understand. The lines inside the
          loop just assign values to the different columns in the Row instance
          <literal>particle</literal> (see <xref linkend="RowClassDescr"
          xrefstyle="select: label" />). A call to its
          <literal>append()</literal> method writes this information to the
          <literal>table</literal> I/O buffer.</para>

          <para>After we have processed all our data, we should flush the
          table's I/O buffer if we want to write all this data to disk. We
          achieve that by calling the <literal>table.flush()</literal>
          method.</para>

          <screen>&gt;&gt;&gt; table.flush()</screen>

          <para>Remember, flushing a table is a <emphasis>very
          important</emphasis> step as it will not only help to maintain the
          integrity of your file, but also will free valuable memory resources
          (i.e. internal buffers) that your program may need for other
          things.</para>
        </section>

        <section id="readingAndSelectingUsage">
          <title>Reading (and selecting) data in a table</title>

          <para>Ok. We have our data on disk, and now we need to access it and
          select from specific columns the values we are interested in. See
          the example below:</para>

          <screen>&gt;&gt;&gt; table = h5file.root.detector.readout
&gt;&gt;&gt; pressure = [ x['pressure'] for x in table.iterrows()
               if x['TDCcount'] &gt; 3 and 20 &lt;= x['pressure'] &lt; 50 ]
&gt;&gt;&gt; pressure
[25.0, 36.0, 49.0]</screen>

          <para>The first line creates a "shortcut" to the
          <emphasis>readout</emphasis> table deeper on the object tree. As you
          can see, we use the <emphasis>natural naming</emphasis> schema to
          access it. We also could have used the
          <literal>h5file.getNode()</literal> method, as we will do later
          on.</para>

          <para>You will recognize the last two lines as a Python list
          comprehension. It loops over the rows in <emphasis>table</emphasis>
          as they are provided by the <literal>table.iterrows()</literal>
          iterator (see <xref linkend="Table.iterrows" />). The iterator
          returns values until all the data in table is exhausted. These rows
          are filtered using the expression: <screen>x['TDCcount'] &gt; 3 and 20 &lt;= x['pressure'] &lt; 50</screen>So,
          we are selecting the values of the <literal>pressure</literal>
          column from filtered records to create the final list and assign it
          to <literal>pressure</literal> variable.</para>

          <para>We could have used a normal <literal>for</literal> loop to
          accomplish the same purpose, but I find comprehension syntax to be
          more compact and elegant.</para>

          <para>Let's select the <literal>name</literal> column for the same
          set of cuts:</para>

          <screen>&gt;&gt;&gt; names = [ x['name'] for x in table
          if x['TDCcount'] &gt; 3 and 20 &lt;= x['pressure'] &lt; 50 ]
&gt;&gt;&gt; names
['Particle:      5', 'Particle:      6', 'Particle:      7']</screen>

          <para>Note how we have omitted the <literal>iterrows()</literal>
          call in the list comprehension. The <literal>Table</literal> class
          has an implementation of the special method
          <literal>__iter__()</literal> that iterates over all the rows in the
          table. In fact, <literal>iterrows()</literal> internally calls this
          special <literal>__iter__()</literal> method. Accessing all the rows
          in a table using this method is very convenient, especially when
          working with the data interactively.</para>

          <para>PyTables do offer other, more powerful ways of performing
          selections which may be more suitable if you have very large tables
          or if you need very high query speeds. They are called
          <emphasis>in-kernel</emphasis> and <emphasis>indexed</emphasis>
          queries, and you can use them through
          <literal>Table.where()</literal> (see <xref
          linkend="Table.where" />) and other related methods. See <xref
          linkend="conditionSyntax" xrefstyle="select: label" /> and <xref
          linkend="searchOptim" xrefstyle="select: label" /> for more
          information on in-kernel and indexed selections.</para>

          <para>That's enough about selections for now. The next section will
          show you how to save these selected results to a file.</para>
        </section>

        <section>
          <title>Creating new array objects</title>

          <para>In order to separate the selected data from the mass of
          detector data, we will create a new group <literal>columns</literal>
          branching off the root group. Afterwards, under this group, we will
          create two arrays that will contain the selected data. First, we
          create the group:</para>

          <screen>&gt;&gt;&gt; gcolumns = h5file.createGroup(h5file.root, "columns", "Pressure and Name")</screen>

          <para>Note that this time we have specified the first parameter
          using <emphasis>natural naming</emphasis>
          (<literal>h5file.root</literal>) instead of with an absolute path
          string ("/").</para>

          <para>Now, create the first of the two <literal>Array</literal>
          objects we've just mentioned:</para>

          <screen>&gt;&gt;&gt; h5file.createArray(gcolumns, 'pressure', array(pressure),
                    "Pressure column selection")
/columns/pressure (Array(3,)) 'Pressure column selection'
  atom := Float64Atom(shape=(), dflt=0.0)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'</screen>

          <para>We already know the first two parameters of the
          <literal>createArray</literal> (see <xref
          linkend="createArrayDescr" />) methods (these are the same as the
          first two in <literal>createTable</literal>): they are the parent
          group <emphasis>where</emphasis> <literal>Array</literal> will be
          created and the <literal>Array</literal> instance
          <emphasis>name</emphasis>. The third parameter is the
          <emphasis>object</emphasis> we want to save to disk. In this case,
          it is a <literal>NumPy</literal> array that is built from the
          selection list we created before. The fourth parameter is the
          <emphasis>title</emphasis>.</para>

          <para>Now, we will save the second array. It contains the list of
          strings we selected before: we save this object as-is, with no
          further conversion.</para>

          <screen>&gt;&gt;&gt; h5file.createArray(gcolumns, 'name', names, "Name column selection")
/columns/name (Array(3,)) 'Name column selection'
  atom := StringAtom(itemsize=16, shape=(), dflt='')
  maindim := 0
  flavor := 'python'
  byteorder := 'irrelevant'</screen>

          <para>As you can see, <literal>createArray()</literal> accepts
          <emphasis>names</emphasis> (which is a regular Python list) as an
          <emphasis>object</emphasis> parameter. Actually, it accepts a
          variety of different regular objects (see <xref
          linkend="createArrayDescr" />) as parameters. The
          <literal>flavor</literal> attribute (see the output above) saves the
          original kind of object that was saved. Based on this
          <emphasis>flavor</emphasis>, PyTables will be able to retrieve
          exactly the same object from disk later on.</para>

          <para>Note that in these examples, the
          <literal>createArray</literal> method returns an
          <literal>Array</literal> instance that is not assigned to any
          variable. Don't worry, this is intentional to show the kind of
          object we have created by displaying its representation. The
          <literal>Array</literal> objects have been attached to the object
          tree and saved to disk, as you can see if you print the complete
          object tree:</para>

          <screen>&gt;&gt;&gt; print h5file
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 19:40:44 2007'
Object Tree:
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10L,)) 'Readout example'
</screen>
        </section>

        <section>
          <title>Closing the file and looking at its content</title>

          <para>To finish this first tutorial, we use the
          <literal>close</literal> method of the h5file
          <literal>File</literal> object to close the file before exiting
          Python:</para>

          <screen>&gt;&gt;&gt; h5file.close()
&gt;&gt;&gt; ^D
$ </screen>

          <para>You have now created your first PyTables file with a table and
          two arrays. You can examine it with any generic HDF5 tool, such as
          <literal>h5dump</literal> or <literal>h5ls</literal>. Here is what
          the <literal>tutorial1.h5</literal> looks like when read with the
          <literal>h5ls</literal> program:</para>

          <screen>$ h5ls -rd tutorial1.h5
/columns                 Group
/columns/name            Dataset {3}
    Data:
        (0) "Particle:      5", "Particle:      6", "Particle:      7"
/columns/pressure        Dataset {3}
    Data:
        (0) 25, 36, 49
/detector                Group
/detector/readout        Dataset {10/Inf}
    Data:
        (0) {0, 0, 0, 0, 10, 0, "Particle:      0", 0},
        (1) {256, 1, 1, 1, 9, 17179869184, "Particle:      1", 1},
        (2) {512, 2, 256, 2, 8, 34359738368, "Particle:      2", 4},
        (3) {768, 3, 6561, 3, 7, 51539607552, "Particle:      3", 9},
        (4) {1024, 4, 65536, 4, 6, 68719476736, "Particle:      4", 16},
        (5) {1280, 5, 390625, 5, 5, 85899345920, "Particle:      5", 25},
        (6) {1536, 6, 1679616, 6, 4, 103079215104, "Particle:      6", 36},
        (7) {1792, 7, 5764801, 7, 3, 120259084288, "Particle:      7", 49},
        (8) {2048, 8, 16777216, 8, 2, 137438953472, "Particle:      8", 64},
        (9) {2304, 9, 43046721, 9, 1, 154618822656, "Particle:      9", 81}</screen>

          <para>Here's the output as displayed by the "ptdump" PyTables
          utility (located in <literal>utils/</literal> directory):</para>

          <screen>$ ptdump tutorial1.h5
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 19:50:57 2007'
Object Tree:
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10L,)) 'Readout example'</screen>

          <para>You can pass the <literal>-v</literal> or
          <literal>-d</literal> options to <literal>ptdump</literal> if you
          want more verbosity. Try them out!</para>

          <para>Also, in <xref linkend="tutorial1-1-tableview"
          xrefstyle="select: label" />, you can admire how the
          <literal>tutorial1.h5</literal> looks like using the <ulink
          url="http://www.carabos.com/products/vitables.html">ViTables
          </ulink> graphical interface .</para>

          <figure id="tutorial1-1-tableview">
            <title>The initial version of the data file for tutorial 1, with a
            view of the data objects.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial1-1-tableview.png"
                           format="PNG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial1-1-tableview.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <!-- -->

      <section>
        <title>Browsing the <emphasis>object tree</emphasis></title>

        <para>In this section, we will learn how to browse the tree and
        retrieve data and also meta-information about the actual data.</para>

        <para>In <emphasis>examples/tutorial1-2.py</emphasis> you will find
        the working version of all the code in this section. As before, you
        are encouraged to use a python shell and inspect the object tree
        during the course of the tutorial.</para>

        <section>
          <title>Traversing the object tree</title>

          <para>Let's start by opening the file we created in last tutorial
          section.</para>

          <screen>&gt;&gt;&gt; h5file = openFile("tutorial1.h5", "a")</screen>

          <para>This time, we have opened the file in "a"ppend mode. We use
          this mode to add more information to the file.</para>

          <para>PyTables, following the Python tradition, offers powerful
          introspection capabilities, i.e. you can easily ask information
          about any component of the object tree as well as search the
          tree.</para>

          <para>To start with, you can get a preliminary overview of the
          object tree by simply printing the existing <literal>File</literal>
          instance:</para>

          <screen>&gt;&gt;&gt; print h5file
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 19:50:57 2007'
Object Tree:
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10L,)) 'Readout example'</screen>

          <para>It looks like all of our objects are there. Now let's make use
          of the <literal>File</literal> iterator to see how to list all the
          nodes in the object tree:</para>

          <screen>&gt;&gt;&gt; for node in h5file:
      print node

/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/detector (Group) 'Detector information'
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'
/detector/readout (Table(10L,)) 'Readout example'</screen>

          <para>We can use the <literal>walkGroups</literal> method (see <xref
          linkend="walkGroupsDescr" />) of the <literal>File</literal> class
          to list only the <emphasis>groups</emphasis> on tree:</para>

          <screen>&gt;&gt;&gt; for group in h5file.walkGroups():
      print group

/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/detector (Group) 'Detector information'</screen>

          <para>Note that <literal>walkGroups()</literal> actually returns an
          <emphasis>iterator</emphasis>, not a list of objects. Using this
          iterator with the <literal>listNodes()</literal> method is a
          powerful combination. Let's see an example listing of all the arrays
          in the tree:</para>

          <screen>&gt;&gt;&gt; for group in h5file.walkGroups("/"):
      for array in h5file.listNodes(group, classname='Array'):
          print array
 
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen>

          <para><literal>listNodes()</literal> (see <xref
          linkend="File.listNodes" />) returns a list containing all the nodes
          hanging off a specific <literal>Group</literal>. If the
          <emphasis>classname</emphasis> keyword is specified, the method will
          filter out all instances which are not descendants of the class. We
          have asked for only <literal>Array</literal> instances. There exist
          also an iterator counterpart called <literal>iterNodes()</literal>
          (see <xref linkend="File.iterNodes" />) that might be handy is some
          situations, like for example when dealing with groups with a large
          number of nodes behind it.</para>

          <para>We can combine both calls by using the
          <literal>walkNodes(where, classname)</literal> special method of the
          <literal>File</literal> object (see <xref
          linkend="File.walkNodes" />). For example:</para>

          <screen>&gt;&gt;&gt; for array in h5file.walkNodes("/", "Array"):
      print array
 
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen>

          <para>This is a nice shortcut when working interactively.</para>

          <para>Finally, we will list all the <literal>Leaf</literal>, i.e.
          <literal>Table</literal> and <literal>Array</literal> instances (see
          <xref linkend="LeafClassDescr" xrefstyle="select: label" />
          for detailed information on <literal>Leaf</literal> class), in the
          <literal>/detector</literal> group. Note that only one instance of
          the <literal>Table</literal> class (i.e. <literal>readout</literal>)
          will be selected in this group (as should be the case):</para>

          <screen>&gt;&gt;&gt; for leaf in h5file.root.detector._f_walkNodes('Leaf'):
      print leaf

/detector/readout (Table(10L,)) 'Readout example'</screen>

          <para>We have used a call to the
          <literal>Group._f_walkNodes(classname, recursive)</literal> method
          (see <xref linkend="Group._f_walkNodes" />), using the
          <emphasis>natural naming</emphasis> path specification.</para>

          <para>Of course you can do more sophisticated node selections using
          these powerful methods. But first, let's take a look at some
          important PyTables object instance variables.</para>
        </section>

        <section>
          <title>Setting and getting user attributes</title>

          <para>PyTables provides an easy and concise way to complement the
          meaning of your node objects on the tree by using the
          <literal>AttributeSet</literal> class (see <xref
          linkend="AttributeSetClassDescr" xrefstyle="select: label" />). You
          can access this object through the standard attribute
          <literal>attrs</literal> in <literal>Leaf</literal> nodes and
          <literal>_v_attrs</literal> in <literal>Group</literal>
          nodes.</para>

          <para>For example, let's imagine that we want to save the date
          indicating when the data in <literal>/detector/readout</literal>
          table has been acquired, as well as the temperature during the
          gathering process:</para>

          <screen>&gt;&gt;&gt; table = h5file.root.detector.readout
&gt;&gt;&gt; table.attrs.gath_date = "Wed, 06/12/2003 18:33"
&gt;&gt;&gt; table.attrs.temperature = 18.4
&gt;&gt;&gt; table.attrs.temp_scale = "Celsius"</screen>

          <para>Now, let's set a somewhat more complex attribute in the
          <literal>/detector</literal> group:</para>

          <screen>&gt;&gt;&gt; detector = h5file.root.detector
&gt;&gt;&gt; detector._v_attrs.stuff = [5, (2.3, 4.5), "Integer and tuple"]</screen>

          <para>Note how the AttributeSet instance is accessed with the
          <literal>_v_attrs</literal> attribute because detector is a
          <literal>Group</literal> node. In general, you can save any standard
          Python data structure as an attribute node. See <xref
          linkend="AttributeSetClassDescr" xrefstyle="select: label" /> for a
          more detailed explanation of how they are serialized for export to
          disk.</para>

          <para>Retrieving the attributes is equally simple:</para>

          <screen>&gt;&gt;&gt; table.attrs.gath_date
'Wed, 06/12/2003 18:33'
&gt;&gt;&gt; table.attrs.temperature
18.399999999999999
&gt;&gt;&gt; table.attrs.temp_scale
'Celsius'
&gt;&gt;&gt; detector._v_attrs.stuff
[5, (2.2999999999999998, 4.5), 'Integer and tuple']</screen>

          <para>You can probably guess how to delete attributes:</para>

          <screen>&gt;&gt;&gt; del table.attrs.gath_date</screen>

          <para>If you want to examine the current user attribute set of
          <literal>/detector/table</literal>, you can print its representation
          (try hitting the <literal>TAB</literal> key twice if you are on a
          Unix Python console with the <literal>rlcompleter</literal> module
          active):</para>

          <screen>&gt;&gt;&gt; table.attrs
/detector/readout._v_attrs (AttributeSet), 23 attributes:
   [CLASS := 'TABLE',
    FIELD_0_FILL := 0,
    FIELD_0_NAME := 'ADCcount',
    FIELD_1_FILL := 0,
    FIELD_1_NAME := 'TDCcount',
    FIELD_2_FILL := 0.0,
    FIELD_2_NAME := 'energy',
    FIELD_3_FILL := 0,
    FIELD_3_NAME := 'grid_i',
    FIELD_4_FILL := 0,
    FIELD_4_NAME := 'grid_j',
    FIELD_5_FILL := 0,
    FIELD_5_NAME := 'idnumber',
    FIELD_6_FILL := '',
    FIELD_6_NAME := 'name',
    FIELD_7_FILL := 0.0,
    FIELD_7_NAME := 'pressure',
    FLAVOR := 'numpy',
    NROWS := 10,
    TITLE := 'Readout example',
    VERSION := '2.6',
    temp_scale := 'Celsius',
    temperature := 18.399999999999999]</screen>

          <para>We've got all the attributes (including the
          <emphasis>system</emphasis> attributes). You can get a list of
          <emphasis>all</emphasis> attributes or only the
          <emphasis>user</emphasis> or <emphasis>system</emphasis> attributes
          with the <literal>_f_list()</literal> method.</para>

          <screen>&gt;&gt;&gt; print table.attrs._f_list("all")
['CLASS', 'FIELD_0_FILL', 'FIELD_0_NAME', 'FIELD_1_FILL', 'FIELD_1_NAME',
 'FIELD_2_FILL', 'FIELD_2_NAME', 'FIELD_3_FILL', 'FIELD_3_NAME', 'FIELD_4_FILL',
 'FIELD_4_NAME', 'FIELD_5_FILL', 'FIELD_5_NAME', 'FIELD_6_FILL', 'FIELD_6_NAME',
 'FIELD_7_FILL', 'FIELD_7_NAME', 'FLAVOR', 'NROWS', 'TITLE', 'VERSION',
 'temp_scale', 'temperature']
&gt;&gt;&gt; print table.attrs._f_list("user")
['temp_scale', 'temperature']
&gt;&gt;&gt; print table.attrs._f_list("sys")
['CLASS', 'FIELD_0_FILL', 'FIELD_0_NAME', 'FIELD_1_FILL', 'FIELD_1_NAME',
 'FIELD_2_FILL', 'FIELD_2_NAME', 'FIELD_3_FILL', 'FIELD_3_NAME', 'FIELD_4_FILL',
 'FIELD_4_NAME', 'FIELD_5_FILL', 'FIELD_5_NAME', 'FIELD_6_FILL', 'FIELD_6_NAME',
 'FIELD_7_FILL', 'FIELD_7_NAME', 'FLAVOR', 'NROWS', 'TITLE', 'VERSION']</screen>

          <para>You can also rename attributes:</para>

          <screen>&gt;&gt;&gt; table.attrs._f_rename("temp_scale","tempScale")
&gt;&gt;&gt; print table.attrs._f_list()
['tempScale', 'temperature']</screen>

          <para>And, from PyTables 2.0 on, you are allowed also to set, delete
          or rename system attributes:</para>

          <screen>&gt;&gt;&gt; table.attrs._f_rename("VERSION", "version")
&gt;&gt;&gt; table.attrs.VERSION
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "tables/attributeset.py", line 222, in __getattr__
    (name, self._v__nodePath)
AttributeError: Attribute 'VERSION' does not exist in node: '/detector/readout'
&gt;&gt;&gt; table.attrs.version
'2.6'</screen>

          <para><emphasis role="bold">Caveat emptor:</emphasis> you must be
          careful when modifying system attributes because you may end fooling
          PyTables and ultimately getting unwanted behaviour. Use this only if
          you know what are you doing.</para>

          <para>So, given the caveat above, we will proceed to restore the
          original name of VERSION attribute:<screen>&gt;&gt;&gt; table.attrs._f_rename("version", "VERSION")
&gt;&gt;&gt; table.attrs.VERSION
'2.6'</screen></para>

          <para>Ok. that's better. If you would terminate your session now,
          you would be able to use the <literal>h5ls</literal> command to read
          the <literal>/detector/readout</literal> attributes from the file
          written to disk:</para>

          <screen>$ h5ls -vr tutorial1.h5/detector/readout
Opened "tutorial1.h5" with sec2 driver.
/detector/readout        Dataset {10/Inf}
    Attribute: CLASS     scalar
        Type:      6-byte null-terminated ASCII string
        Data:  "TABLE"
    Attribute: VERSION   scalar
        Type:      4-byte null-terminated ASCII string
        Data:  "2.6"
    Attribute: TITLE     scalar
        Type:      16-byte null-terminated ASCII string
        Data:  "Readout example"
    Attribute: NROWS     scalar
        Type:      native long long
        Data:  10
    Attribute: FIELD_0_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "ADCcount"
    Attribute: FIELD_1_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "TDCcount"
    Attribute: FIELD_2_NAME scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "energy"
    Attribute: FIELD_3_NAME scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "grid_i"
    Attribute: FIELD_4_NAME scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "grid_j"
    Attribute: FIELD_5_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "idnumber"
    Attribute: FIELD_6_NAME scalar
        Type:      5-byte null-terminated ASCII string
        Data:  "name"
    Attribute: FIELD_7_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "pressure"
    Attribute: FLAVOR    scalar
        Type:      5-byte null-terminated ASCII string
        Data:  "numpy"
    Attribute: tempScale scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "Celsius"
    Attribute: temperature scalar
        Type:      native double
        Data:  18.4
    Location:  0:1:0:1952
    Links:     1
    Modified:  2006-12-11 10:35:13 CET
    Chunks:    {85} 3995 bytes
    Storage:   470 logical bytes, 3995 allocated bytes, 11.76% utilization
    Type:      struct {
                   "ADCcount"         +0    native unsigned short
                   "TDCcount"         +2    native unsigned char
                   "energy"           +3    native double
                   "grid_i"           +11   native int
                   "grid_j"           +15   native int
                   "idnumber"         +19   native long long
                   "name"             +27   16-byte null-terminated ASCII string
                   "pressure"         +43   native float
               } 47 bytes</screen>

          <para>Attributes are a useful mechanism to add persistent (meta)
          information to your data.</para>
        </section>

        <section>
          <title>Getting object metadata</title>

          <para>Each object in PyTables has <emphasis>metadata</emphasis>
          information about the data in the file. Normally this
          <emphasis>meta-information</emphasis> is accessible through the node
          instance variables. Let's take a look at some examples:</para>

          <screen>&gt;&gt;&gt; print "Object:", table
Object: /detector/readout (Table(10L,)) 'Readout example'
&gt;&gt;&gt; print "Table name:", table.name
Table name: readout
&gt;&gt;&gt; print "Table title:", table.title
Table title: Readout example
&gt;&gt;&gt; print "Number of rows in table:", table.nrows
Number of rows in table: 10
&gt;&gt;&gt; print "Table variable names with their type and shape:"
Table variable names with their type and shape:
&gt;&gt;&gt; for name in table.colnames:
      print name, ':= %s, %s' % (table.coldtypes[name],
                                 table.coldtypes[name].shape)

ADCcount := uint16, ()
TDCcount := uint8, ()
energy := float64, ()
grid_i := int32, ()
grid_j := int32, ()
idnumber := int64, ()
name := |S16, ()
pressure := float32, ()</screen>

          <para>Here, the <literal>name</literal>, <literal>title</literal>,
          <literal>nrows</literal>, <literal>colnames</literal> and
          <literal>coldtypes</literal> attributes (see <xref
          linkend="TableInstanceVariablesDescr" xrefstyle="select: label" />
          for a complete attribute list) of the <literal>Table</literal>
          object gives us quite a bit of information about the table
          data.</para>

          <para>You can interactively retrieve general information about the
          public objects in PyTables by asking for help:</para>

          <screen>&gt;&gt;&gt; help(table)
Help on Table in module tables.table object:

class Table(tableExtension.Table, tables.leaf.Leaf)
 |  Represent a table in the object tree.
 |
 |  It provides methods to create new tables or open existing ones, as
 |  well as to write/read data to/from table objects over the
 |  file. A method is also provided to iterate over the rows without
 |  loading the entire table or column in memory.
 |
 |  Data can be written or read both as Row instances and as homogeneous
 |  or heterogeneous (record) arrays of the supported flavors (such as
 |  NumPy and numarray, including NestedRecArray objects).
 |
 |  Methods:
 |
 |      __getitem__(key)
 |      __iter__()
 |      __setitem__(key, value)
 |      append(rows)
 |      col(name)
 |      flushRowsToIndex()
 |      iterrows(start, stop, step)
 |      itersequence(sequence)
 |      modifyRows(start, rows)
 |      modifyColumn(columns, names, [start] [, stop] [, step])
 |      modifyColumns(columns, names, [start] [, stop] [, step])
 |      read([start] [, stop] [, step] [, field])
 |      readCoordinates(coords [, field])
 |      readWhere(condition [, condvars] [, field])
 |      reIndex()
 |      reIndexDirty()
 |      removeRows(start [, stop])
 |      where(condition [, condvars] [, start] [, stop] [, step])
 |      whereAppend(dstTable, condition [, condvars] [, start] [, stop] [, step])
 |      getWhereList(condition [, condvars] [, sort])
 |
 |  Instance variables:
 |
 |      description -- the metaobject describing this table
 |      row -- a reference to the Row object associated with this table
 |      nrows -- the number of rows in this table
 |      rowsize -- the size, in bytes, of each row
 |      cols -- accessor to the columns using a natural name schema
 |      colnames -- the top-level field names for the table (list)
 |      colpathnames -- the bottom-level field pathnames for the table (list)
 |      colinstances -- the column instances for the table fields (dictionary)
 |      coldescrs -- Maps the name of a column to its `Col` description.
 |      coldtypes -- the dtype class for the table fields (dictionary)
 |      coltypes -- the PyTables type for the table fields (dictionary)
 |      coldflts -- the defaults for each column (dictionary)
 |      colindexed -- whether the table fields are indexed (dictionary)
 |      indexed -- whether or not some field in Table is indexed
 |      autoIndex -- automatically keep column indexes up to date?
 |      indexFilters -- filters used to compress indexes
 |      indexedcolpathnames -- the pathnames of the indexed columns (list)
[snip]          </screen>

          <para>Try getting help with other object docs by yourself:</para>

          <screen>&gt;&gt;&gt; help(h5file)
&gt;&gt;&gt; help(table.removeRows)</screen>

          <para>To examine metadata in the
          <emphasis>/columns/pressure</emphasis> <literal>Array</literal>
          object:</para>

          <screen>&gt;&gt;&gt; pressureObject = h5file.getNode("/columns", "pressure")
&gt;&gt;&gt; print "Info on the object:", repr(pressureObject)
Info on the object: /columns/pressure (Array(3L,)) 'Pressure column selection'
  atom := Float64Atom(shape=(), dflt=0.0)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'
&gt;&gt;&gt; print "  shape: ==&gt;", pressureObject.shape
  shape: ==&gt; (3L,)
&gt;&gt;&gt; print "  title: ==&gt;", pressureObject.title
  title: ==&gt; Pressure column selection
&gt;&gt;&gt; print "  atom: ==&gt;", pressureObject.atom
  atom: ==&gt; Float64Atom(shape=(), dflt=0.0)</screen>

          <para>Observe that we have used the <literal>getNode()</literal>
          method of the <literal>File</literal> class to access a node in the
          tree, instead of the natural naming method. Both are useful, and
          depending on the context you will prefer one or the other.
          <literal>getNode()</literal> has the advantage that it can get a
          node from the pathname string (as in this example) and can also act
          as a filter to show only nodes in a particular location that are
          instances of class <emphasis>classname</emphasis>. In general,
          however, I consider natural naming to be more elegant and easier to
          use, especially if you are using the name completion capability
          present in interactive console. Try this powerful combination of
          natural naming and completion capabilities present in most Python
          consoles, and see how pleasant it is to browse the object tree
          (well, as pleasant as such an activity can be).</para>

          <para>If you look at the <literal>type</literal> attribute of the
          <literal>pressureObject</literal> object, you can verify that it is
          a "<emphasis>float64</emphasis>" array. By looking at its
          <literal>shape</literal> attribute, you can deduce that the array on
          disk is unidimensional and has 3 elements. See <xref
          linkend="ArrayClassInstanceVariables" xrefstyle="select: label" />
          or the internal doc strings for the complete
          <literal>Array</literal> attribute list.</para>
        </section>

        <section>
          <title>Reading data from <literal>Array</literal> objects</title>

          <para>Once you have found the desired <literal>Array</literal>, use
          the <literal>read()</literal> method of the <literal>Array</literal>
          object to retrieve its data:</para>

          <screen>&gt;&gt;&gt; pressureArray = pressureObject.read()
&gt;&gt;&gt; pressureArray
array([ 25.,  36.,  49.])
&gt;&gt;&gt; print "pressureArray is an object of type:", type(pressureArray)
pressureArray is an object of type: &lt;type 'numpy.ndarray'&gt;
&gt;&gt;&gt; nameArray = h5file.root.columns.name.read()
&gt;&gt;&gt; print "nameArray is an object of type:", type(nameArray)
nameArray is an object of type: &lt;type 'list'&gt;
&gt;&gt;&gt; 
&gt;&gt;&gt; print "Data on arrays nameArray and pressureArray:"
Data on arrays nameArray and pressureArray:
&gt;&gt;&gt; for i in range(pressureObject.shape[0]):
      print nameArray[i], "--&gt;", pressureArray[i]
 
Particle:      5 --&gt; 25.0
Particle:      6 --&gt; 36.0
Particle:      7 --&gt; 49.0</screen>

          <para>You can see that the <literal>read()</literal> method (see
          <xref linkend="readArrayDescr" xrefstyle="select: label" />) returns
          an authentic <literal>NumPy</literal> object for the
          <literal>pressureObject</literal> instance by looking at the output
          of the <literal>type()</literal> call. A <literal>read()</literal>
          of the <literal>nameArray</literal> object instance returns a native
          Python list (of strings). The type of the object saved is stored as
          an HDF5 attribute (named <literal>FLAVOR</literal>) for objects on
          disk. This attribute is then read as <literal>Array</literal>
          meta-information (accessible through in the
          <literal>Array.attrs.FLAVOR</literal> variable), enabling the read
          array to be converted into the original object. This provides a
          means to save a large variety of objects as arrays with the
          guarantee that you will be able to later recover them in their
          original form. See <xref linkend="createArrayDescr" /> for a
          complete list of supported objects for the <literal>Array</literal>
          object class.</para>
        </section>
      </section>

      <section>
        <title>Commiting data to tables and arrays</title>

        <para>We have seen how to create tables and arrays and how to browse
        both data and metadata in the object tree. Let's examine more closely
        now one of the most powerful capabilities of PyTables, namely, how to
        modify already created tables and arrays<footnote>
            <para>Appending data to arrays is also supported, but you need to
            create special objects called <literal>EArray</literal> (see <xref
            linkend="EArrayClassDescr" xrefstyle="select: label" /> for more
            info).</para>
          </footnote>.</para>

        <section>
          <title>Appending data to an existing table</title>

          <para>Now, let's have a look at how we can add records to an
          existing table on disk. Let's use our well-known
          <emphasis>readout</emphasis> <literal>Table</literal> object and
          append some new values to it:</para>

          <screen>&gt;&gt;&gt; table = h5file.root.detector.readout
&gt;&gt;&gt; particle = table.row
&gt;&gt;&gt; for i in xrange(10, 15):
      particle['name']  = 'Particle: %6d' % (i)
      particle['TDCcount'] = i % 256
      particle['ADCcount'] = (i * 256) % (1 &lt;&lt; 16)
      particle['grid_i'] = i
      particle['grid_j'] = 10 - i
      particle['pressure'] = float(i*i)
      particle['energy'] = float(particle['pressure'] ** 4)
      particle['idnumber'] = i * (2 ** 34)
      particle.append()

&gt;&gt;&gt; table.flush()</screen>

          <para>It's the same method we used to fill a new table. PyTables
          knows that this table is on disk, and when you add new records, they
          are appended to the end of the table<footnote>
              <para>Note that you can append not only scalar values to tables,
              but also fully multidimensional array objects.</para>
            </footnote>.</para>

          <para>If you look carefully at the code you will see that we have
          used the <literal>table.row</literal> attribute to create a table
          row and fill it with the new values. Each time that its
          <literal>append()</literal> method is called, the actual row is
          committed to the output buffer and the row pointer is incremented to
          point to the next table record. When the buffer is full, the data is
          saved on disk, and the buffer is reused again for the next
          cycle.</para>

          <para><emphasis>Caveat emptor</emphasis>: Do not forget to always
          call the <literal>flush()</literal> method after a write operation,
          or else your tables will not be updated!</para>

          <para>Let's have a look at some rows in the modified table and
          verify that our new data has been appended:</para>

          <screen>&gt;&gt;&gt; for r in table.iterrows():
     print "%-16s | %11.1f | %11.4g | %6d | %6d | %8d |" % \
         (r['name'], r['pressure'], r['energy'], r['grid_i'], r['grid_j'],
          r['TDCcount'])
 
Particle:      0 |         0.0 |           0 |      0 |     10 |        0 |
Particle:      1 |         1.0 |           1 |      1 |      9 |        1 |
Particle:      2 |         4.0 |         256 |      2 |      8 |        2 |
Particle:      3 |         9.0 |        6561 |      3 |      7 |        3 |
Particle:      4 |        16.0 |   6.554e+04 |      4 |      6 |        4 |
Particle:      5 |        25.0 |   3.906e+05 |      5 |      5 |        5 |
Particle:      6 |        36.0 |    1.68e+06 |      6 |      4 |        6 |
Particle:      7 |        49.0 |   5.765e+06 |      7 |      3 |        7 |
Particle:      8 |        64.0 |   1.678e+07 |      8 |      2 |        8 |
Particle:      9 |        81.0 |   4.305e+07 |      9 |      1 |        9 |
Particle:     10 |       100.0 |       1e+08 |     10 |      0 |       10 |
Particle:     11 |       121.0 |   2.144e+08 |     11 |     -1 |       11 |
Particle:     12 |       144.0 |     4.3e+08 |     12 |     -2 |       12 |
Particle:     13 |       169.0 |   8.157e+08 |     13 |     -3 |       13 |
Particle:     14 |       196.0 |   1.476e+09 |     14 |     -4 |       14 |</screen>
        </section>

        <section id="modifyingTableUsage">
          <title>Modifying data in tables</title>

          <para>Ok, until now, we've been only reading and writing (appending)
          values to our tables. But there are times that you need to modify
          your data once you have saved it on disk (this is specially true
          when you need to modify the real world data to adapt your goals ;).
          Let's see how we can modify the values that were saved in our
          existing tables. We will start modifying single cells in the first
          row of the <literal>Particle</literal> table:</para>

          <screen>&gt;&gt;&gt; print "Before modif--&gt;", table[0]
Before modif--&gt; (0, 0, 0.0, 0, 10, 0L, 'Particle:      0', 0.0)
&gt;&gt;&gt; table.cols.TDCcount[0] = 1
&gt;&gt;&gt; print "After modifying first row of ADCcount--&gt;", table[0]
After modifying first row of ADCcount--&gt; (0, 1, 0.0, 0, 10, 0L, 'Particle:      0', 0.0)
&gt;&gt;&gt; table.cols.energy[0] = 2
&gt;&gt;&gt; print "After modifying first row of energy--&gt;", table[0]
After modifying first row of energy--&gt; (0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)</screen>

          <para>We can modify complete ranges of columns as well:</para>

          <screen>&gt;&gt;&gt; table.cols.TDCcount[2:5] = [2,3,4]
&gt;&gt;&gt; print "After modifying slice [2:5] of TDCcount--&gt;", table[0:5]
After modifying slice [2:5] of TDCcount--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (256, 1, 1.0, 1, 9, 17179869184L, 'Particle:      1', 1.0)
 (512, 2, 256.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)
 (1024, 4, 65536.0, 4, 6, 68719476736L, 'Particle:      4', 16.0)]
&gt;&gt;&gt; table.cols.energy[1:9:3] = [2,3,4]
&gt;&gt;&gt; print "After modifying slice [1:9:3] of energy--&gt;", table[0:9]
After modifying slice [1:9:3] of energy--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (256, 1, 2.0, 1, 9, 17179869184L, 'Particle:      1', 1.0)
 (512, 2, 256.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)
 (1024, 4, 3.0, 4, 6, 68719476736L, 'Particle:      4', 16.0)
 (1280, 5, 390625.0, 5, 5, 85899345920L, 'Particle:      5', 25.0)
 (1536, 6, 1679616.0, 6, 4, 103079215104L, 'Particle:      6', 36.0)
 (1792, 7, 4.0, 7, 3, 120259084288L, 'Particle:      7', 49.0)
 (2048, 8, 16777216.0, 8, 2, 137438953472L, 'Particle:      8', 64.0)]</screen>

          <para>Check that the values have been correctly modified!
          <emphasis>Hint:</emphasis> remember that column
          <literal>TDCcount</literal> is the second one, and that
          <literal>energy</literal> is the third. Look for more info on
          modifying columns in <xref linkend="Column.__setitem__"
          xrefstyle="select: label" />.</para>

          <para>PyTables also lets you modify complete sets of rows at the
          same time. As a demonstration of these capability, see the next
          example:</para>

          <screen>&gt;&gt;&gt; table.modifyRows(start=1, step=3,
                   rows=[(1, 2, 3.0, 4, 5, 6L, 'Particle:   None', 8.0),
                         (2, 4, 6.0, 8, 10, 12L, 'Particle: None*2', 16.0)])
2
&gt;&gt;&gt; print "After modifying the complete third row--&gt;", table[0:5]
After modifying the complete third row--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (1, 2, 3.0, 4, 5, 6L, 'Particle:   None', 8.0)
 (512, 2, 256.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)
 (2, 4, 6.0, 8, 10, 12L, 'Particle: None*2', 16.0)]</screen>

          <para>As you can see, the <literal>modifyRows()</literal> call has
          modified the rows second and fifth, and it returned the number of
          modified rows.</para>

          <para>Apart of <literal>modifyRows()</literal>, there exists another
          method, called <literal>modifyColumn()</literal> to modify specific
          columns as well. Please check sections <xref
          linkend="Table.modifyRows" /> and <xref
          linkend="Table.modifyColumn" /> for a more in-depth description of
          them.</para>

          <para>Finally, it exists another way of modifying tables that is
          generally more handy than the described above. This new way uses the
          method <literal>update()</literal> (see <xref
          linkend="Row.update" />) of the <literal>Row</literal> instance that
          is attached to every table, so it is meant to be used in table
          iterators. Look at the next example:</para>

          <screen>&gt;&gt;&gt; for row in table.where('TDCcount &lt;= 2'):
      row['energy'] = row['TDCcount']*2
      row.update()

&gt;&gt;&gt; print "After modifying energy column (where TDCcount &lt;=2)--&gt;", table[0:4]
After modifying energy column (where TDCcount &lt;=2)--&gt; 
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (1, 2, 4.0, 4, 5, 6L, 'Particle:   None', 8.0)
 (512, 2, 4.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)]</screen>

          <para><emphasis>Note:</emphasis>The authors find this way of
          updating tables (i.e. using <literal>Row.update()</literal>) to be
          both convenient and efficient. Please make sure to use it
          extensively.</para>
        </section>

        <section id="modifyingArrayUsage">
          <title>Modifying data in arrays</title>

          <para>We are going now to see how to modify data in array objects.
          The basic way to do this is through the use of
          <literal>__setitem__</literal> special method (see <xref
          linkend="Array.__setitem__" />). Let's see at how modify data on the
          <literal>pressureObject</literal> array:</para>

          <screen>&gt;&gt;&gt; pressureObject = h5file.root.columns.pressure
&gt;&gt;&gt; print "Before modif--&gt;", pressureObject[:]
Before modif--&gt; [ 25.  36.  49.]
&gt;&gt;&gt; pressureObject[0] = 2
&gt;&gt;&gt; print "First modif--&gt;", pressureObject[:]
First modif--&gt; [  2.  36.  49.]
&gt;&gt;&gt; pressureObject[1:3] = [2.1, 3.5]
&gt;&gt;&gt; print "Second modif--&gt;", pressureObject[:]
Second modif--&gt; [ 2.   2.1  3.5]
&gt;&gt;&gt; pressureObject[::2] = [1,2]
&gt;&gt;&gt; print "Third modif--&gt;", pressureObject[:]
Third modif--&gt; [ 1.   2.1  2. ]</screen>

          <para>So, in general, you can use any combination of
          (multidimensional) extended slicing<footnote>
              <para>With the sole exception that you cannot use negative
              values for <literal>step</literal>.</para>
            </footnote> to refer to indexes that you want to modify. See <xref
          linkend="Array.__getitem__" xrefstyle="select: label" /> for
          more examples on how to use extended slicing in PyTables
          objects.</para>

          <para>Similarly, with and array of strings:</para>

          <screen>&gt;&gt;&gt; nameObject = h5file.root.columns.name
&gt;&gt;&gt; print "Before modif--&gt;", nameObject[:]
Before modif--&gt; ['Particle:      5', 'Particle:      6', 'Particle:      7']
&gt;&gt;&gt; nameObject[0] = 'Particle:   None'
&gt;&gt;&gt; print "First modif--&gt;", nameObject[:]
First modif--&gt; ['Particle:   None', 'Particle:      6', 'Particle:      7']
&gt;&gt;&gt; nameObject[1:3] = ['Particle:      0', 'Particle:      1']
&gt;&gt;&gt; print "Second modif--&gt;", nameObject[:]
Second modif--&gt; ['Particle:   None', 'Particle:      0', 'Particle:      1']
&gt;&gt;&gt; nameObject[::2] = ['Particle:     -3', 'Particle:     -5']
&gt;&gt;&gt; print "Third modif--&gt;", nameObject[:]
Third modif--&gt; ['Particle:     -3', 'Particle:      0', 'Particle:     -5']</screen>
        </section>

        <section>
          <title>And finally... how to delete rows from a table</title>

          <para>We'll finish this tutorial by deleting some rows from the
          table we have. Suppose that we want to delete the the 5th to 9th
          rows (inclusive):</para>

          <screen>&gt;&gt;&gt; table.removeRows(5,10)
5L</screen>

          <para><literal>removeRows(start, stop)</literal> (see <xref
          linkend="removeRowsDescr" />) deletes therows in the range (start,
          stop). It returns the number of rows effectively removed.</para>

          <para>We have reached the end of this first tutorial. Don't forget
          to close the file when you finish:</para>

          <screen>&gt;&gt;&gt; h5file.close()
&gt;&gt;&gt; ^D
$ </screen>

          <para>In <xref linkend="tutorial1-2-tableview"
          xrefstyle="select: label" /> you can see a graphical view of the
          PyTables file with the datasets we have just created. In <xref
          linkend="tutorial1-general" xrefstyle="select: label" /> are
          displayed the general properties of the table
          <literal>/detector/readout</literal>.</para>

          <figure id="tutorial1-2-tableview">
            <title>The final version of the data file for tutorial 1.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial1-2-tableview.png"
                           format="PNG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial1-2-tableview.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="tutorial1-general">
            <title>General properties of the
            <literal>/detector/readout</literal> table.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial1-general.png"
                           format="PNG" scale="75" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial1-general.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section id="secondExample">
        <title>Multidimensional table cells and automatic sanity
        checks</title>

        <para>Now it's time for a more real-life example (i.e. with errors in
        the code). We will create two groups that branch directly from the
        <literal>root</literal> node, <literal>Particles</literal> and
        <literal>Events</literal>. Then, we will put three tables in each
        group. In <literal>Particles</literal> we will put tables based on the
        <literal>Particle</literal> descriptor and in
        <literal>Events</literal>, the tables based the
        <literal>Event</literal> descriptor.</para>

        <para>Afterwards, we will provision the tables with a number of
        records. Finally, we will read the newly-created table
        <literal>/Events/TEvent3</literal> and select some values from it,
        using a comprehension list.</para>

        <para>Look at the next script (you can find it in
        <literal>examples/tutorial2.py</literal>). It appears to do all of the
        above, but it contains some small bugs. Note that this
        <literal>Particle</literal> class is not directly related to the one
        defined in last tutorial; this class is simpler (note, however, the
        <emphasis>multidimensional</emphasis> columns called
        <literal>pressure</literal> and
        <literal>temperature</literal>).</para>

        <para>We also introduce a new manner to describe a
        <literal>Table</literal> as a dictionary, as you can see in the
        <literal>Event</literal> description. See <xref
        linkend="createTableDescr" xrefstyle="select:         label" /> about
        the different kinds of descriptor objects that can be passed to the
        <literal>createTable()</literal> method.</para>

        <screen>from tables import *

# Describe a particle record
class Particle(IsDescription):
    name        = StringCol(itemsize=16)  # 16-character string
    lati        = Int32Col()              # integer
    longi       = Int32Col()              # integer
    pressure    = Float32Col(shape=(2,3)) # array of floats (single-precision)
    temperature = Float64Col(shape=(2,3)) # array of doubles (double-precision)

# Another way to describe the columns of a table
Event = {
    "name"     : StringCol(itemsize=16),
    "TDCcount" : UInt8Col(),
    "ADCcount" : UInt16Col(),
    "xcoord"   : Float32Col(),
    "ycoord"   : Float32Col(),
    }

# Open a file in "w"rite mode
fileh = openFile("tutorial2.h5", mode = "w")
# Get the HDF5 root group
root = fileh.root
# Create the groups:
for groupname in ("Particles", "Events"):
    group = fileh.createGroup(root, groupname)
# Now, create and fill the tables in Particles group
gparticles = root.Particles
# Create 3 new tables
for tablename in ("TParticle1", "TParticle2", "TParticle3"):
    # Create a table
    table = fileh.createTable("/Particles", tablename, Particle,
                              "Particles: "+tablename)
    # Get the record object associated with the table:
    particle = table.row
    # Fill the table with 257 particles
    for i in xrange(257):
        # First, assign the values to the Particle record
        particle['name'] = 'Particle: %6d' % (i)
        particle['lati'] = i
        particle['longi'] = 10 - i
        ########### Detectable errors start here. Play with them!
        particle['pressure'] = array(i*arange(2*3), shape=(2,4))  # Incorrect
        #particle['pressure'] = array(i*arange(2*3), shape=(2,3))  # Correct
        ########### End of errors
        particle['temperature'] = (i**2)     # Broadcasting
        # This injects the Record values
        particle.append()
    # Flush the table buffers
    table.flush()

# Now, go for Events:
for tablename in ("TEvent1", "TEvent2", "TEvent3"):
    # Create a table in Events group
    table = fileh.createTable(root.Events, tablename, Event,
                              "Events: "+tablename)
    # Get the record object associated with the table:
    event = table.row
    # Fill the table with 257 events
    for i in xrange(257):
        # First, assign the values to the Event record
        event['name']  = 'Event: %6d' % (i)
        event['TDCcount'] = i % (1&lt;&lt;8)   # Correct range
        ########### Detectable errors start here. Play with them!
        event['xcoor'] = float(i**2)     # Wrong spelling
        #event['xcoord'] = float(i**2)   # Correct spelling
        event['ADCcount'] = "sss"          # Wrong type
        #event['ADCcount'] = i * 2        # Correct type
        ########### End of errors
        event['ycoord'] = float(i)**4
        # This injects the Record values
        event.append()
    # Flush the buffers
    table.flush()

# Read the records from table "/Events/TEvent3" and select some
table = root.Events.TEvent3
e = [ p['TDCcount'] for p in table
      if p['ADCcount'] &lt; 20 and 4 &lt;= p['TDCcount'] &lt; 15 ]
print "Last record ==&gt;", p
print "Selected values ==&gt;", e
print "Total selected records ==&gt; ", len(e)
# Finally, close the file (this also will flush all the remaining buffers!)
fileh.close()</screen>

        <section>
          <title>Shape checking</title>

          <para>If you look at the code carefully, you'll see that it won't
          work. You will get the following error:</para>

          <screen>$ python tutorial2.py
Traceback (most recent call last):
  File "tutorial2.py", line 51, in ?
    particle['pressure'] = array(i*arange(2*3), shape=(2,4))  # Incorrect
  File ".../numarray/numarraycore.py", line 400, in array
    a.setshape(shape)
  File ".../numarray/generic.py", line 702, in setshape
    raise ValueError("New shape is not consistent with the old shape")
ValueError: New shape is not consistent with the old shape
</screen>

          <para>This error indicates that you are trying to assign an array
          with an incompatible shape to a table cell. Looking at the source,
          we see that we were trying to assign an array of shape
          <literal>(2,4)</literal> to a <literal>pressure</literal> element,
          which was defined with the shape <literal>(2,3)</literal>.</para>

          <para>In general, these kinds of operations are forbidden, with one
          valid exception: when you assign a <emphasis>scalar</emphasis> value
          to a multidimensional column cell, all the cell elements are
          populated with the value of the scalar. For example:</para>

          <screen>particle['temperature'] = (i**2)    # Broadcasting</screen>

          <para>The value <literal>i**2</literal> is assigned to all the
          elements of the <literal>temperature</literal> table cell. This
          capability is provided by the <literal>NumPy</literal> package and
          is known as <emphasis>broadcasting</emphasis>.</para>
        </section>

        <section>
          <title>Field name checking</title>

          <para>After fixing the previous error and rerunning the program, we
          encounter another error:</para>

          <screen>$ python tutorial2.py
Traceback (most recent call last):
  File "tutorial2.py", line 73, in ?
    event['xcoor'] = float(i**2)     # Wrong spelling
  File "tableExtension.pyx", line 1094, in tableExtension.Row.__setitem__
  File "tableExtension.pyx", line 127, in tableExtension.getNestedFieldCache
  File "utilsExtension.pyx", line 331, in utilsExtension.getNestedField
KeyError: 'no such column: xcoor'</screen>

          <para>This error indicates that we are attempting to assign a value
          to a non-existent field in the <emphasis>event</emphasis> table
          object. By looking carefully at the <literal>Event</literal> class
          attributes, we see that we misspelled the <literal>xcoord</literal>
          field (we wrote <literal>xcoor</literal> instead). This is unusual
          behavior for Python, as normally when you assign a value to a
          non-existent instance variable, Python creates a new variable with
          that name. Such a feature can be dangerous when dealing with an
          object that contains a fixed list of field names. PyTables checks
          that the field exists and raises a <literal>KeyError</literal> if
          the check fails.</para>
        </section>

        <section>
          <title>Data type checking</title>

          <para>Finally, the last issue which we will find here is a
          <literal>TypeError</literal> exception:</para>

          <para><screen>$ python tutorial2.py
Traceback (most recent call last):
  File "tutorial2.py", line 75, in ?
    event['ADCcount'] = "sss"          # Wrong type
  File "tableExtension.pyx", line 1111, in tableExtension.Row.__setitem__
TypeError: invalid type (&lt;type 'str'&gt;) for column ``ADCcount``</screen>And,
          if we change the affected line to read:</para>

          <screen>event.ADCcount = i * 2        # Correct type</screen>

          <para>we will see that the script ends well.</para>

          <para>You can see the structure created with this (corrected) script
          in <xref linkend="tutorial2-tableview" xrefstyle="select: label" />.
          In particular, note the multidimensional column cells in table
          <literal>/Particles/TParticle2</literal>.</para>

          <figure id="tutorial2-tableview">
            <title>Table hierarchy for tutorial 2.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial2-tableview.png"
                           format="PNG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial2-tableview.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section id="ThirdExample">
        <title>Exercising the Undo/Redo feature</title>

        <para>PyTables has integrated support for undoing and/or redoing
        actions. This functionality lets you put marks in specific places of
        your hierarchy manipulation operations, so that you can make your HDF5
        file pop back (<emphasis>undo</emphasis>) to a specific mark (for
        example for inspecting how your hierarchy looked at that point). You
        can also go forward to a more recent marker
        (<emphasis>redo</emphasis>). You can even do jumps to the marker you
        want using just one instruction as we will see shortly.</para>

        <para>You can undo/redo all the operations that are related to object
        tree management, like creating, deleting, moving or renaming nodes (or
        complete sub-hierarchies) inside a given object tree. You can also
        undo/redo operations (i.e. creation, deletion or modification) of
        persistent node attributes. However, when actions include
        <emphasis>internal</emphasis> modifications of datasets (that includes
        <literal>Table.append</literal>, <literal>Table.modifyRows</literal>
        or <literal>Table.removeRows</literal> among others), they cannot be
        undone/redone currently.</para>

        <para>This capability can be useful in many situations, like for
        example when doing simulations with multiple branches. When you have
        to choose a path to follow in such a situation, you can put a mark
        there and, if the simulation is not going well, you can go back to
        that mark and start another path. Other possible application is
        defining coarse-grained operations which operate in a
        transactional-like way, i.e. which return the database to its previous
        state if the operation finds some kind of problem while running. You
        can probably devise many other scenarios where the Undo/Redo feature
        can be useful to you <footnote>
            <para>You can even <emphasis>hide</emphasis> nodes temporarily.
            Will you be able to find out how?</para>
          </footnote>.</para>

        <section>
          <title>A basic example</title>

          <para>In this section, we are going to show the basic behavior of
          the Undo/Redo feature. You can find the code used in this example in
          <literal>examples/tutorial3-1.py</literal>. A somewhat more complex
          example will be explained in the next section.</para>

          <para>First, let's create a file:</para>

          <screen>&gt;&gt;&gt; import tables
&gt;&gt;&gt; fileh = tables.openFile("tutorial3-1.h5", "w", title="Undo/Redo demo 1")</screen>

          <para>And now, activate the Undo/Redo feature with the method
          <literal>enableUndo</literal> (see <xref
          linkend="File.enableUndo" />) of <literal>File</literal>:</para>

          <screen>&gt;&gt;&gt; fileh.enableUndo()</screen>

          <para>From now on, all our actions will be logged internally by
          PyTables. Now, we are going to create a node (in this case an
          <literal>Array</literal> object):</para>

          <screen>&gt;&gt;&gt; one = fileh.createArray('/', 'anarray', [3,4], "An array")</screen>

          <para>Now, mark this point:</para>

          <screen>&gt;&gt;&gt; fileh.mark()
1</screen>

          <para>We have marked the current point in the sequence of actions.
          In addition, the <literal>mark()</literal> method has returned the
          identifier assigned to this new mark, that is 1 (mark #0 is reserved
          for the implicit mark at the beginning of the action log). In the
          next section we will see that you can also assign a
          <emphasis>name</emphasis> to a mark (see <xref
          linkend="File.mark" /> for more info on <literal>mark()</literal>).
          Now, we are going to create another array:</para>

          <screen>&gt;&gt;&gt; another = fileh.createArray('/', 'anotherarray', [4,5], "Another array")</screen>

          <para>Right. Now, we can start doing funny things. Let's say that we
          want to pop back to the previous mark (that whose value was 1, do
          you remember?). Let's introduce the <literal>undo()</literal> method
          (see <xref linkend="File.undo" />):</para>

          <screen>&gt;&gt;&gt; fileh.undo()</screen>

          <para>Fine, what do you think it happened? Well, let's have a look
          at the object tree:</para>

          <screen>&gt;&gt;&gt; print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'
/anarray (Array(2,)) 'An array'</screen>

          <para>What happened with the <literal>/anotherarray</literal> node
          we've just created? You guess it, it has disappeared because it was
          created <emphasis>after</emphasis> the mark 1. If you are curious
          enough you may well ask where it has gone. Well, it has not been
          deleted completely; it has been just moved into a special, hidden,
          group of PyTables that renders it invisible and waiting for a chance
          to be reborn.</para>

          <para>Now, unwind once more, and look at the object tree:</para>

          <screen>&gt;&gt;&gt; fileh.undo()
&gt;&gt;&gt; print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'</screen>

          <para>Oops, <literal>/anarray</literal> has disappeared as well!.
          Don't worry, it will revisit us very shortly. So, you might be
          somewhat lost right now; in which mark are we?. Let's ask the
          <literal>getCurrentMark()</literal> method (see <xref
          linkend="File.getCurrentMark" />) in the file handler:</para>

          <screen>&gt;&gt;&gt; print fileh.getCurrentMark()
0</screen>

          <para>So we are at mark #0, remember? Mark #0 is an implicit mark
          that is created when you start the log of actions when calling
          <literal>File.enableUndo()</literal>. Fine, but you are missing your
          too-young-to-die arrays. What can we do about that?
          <literal>File.redo()</literal> (see <xref linkend="File.redo" />) to
          the rescue:</para>

          <screen>&gt;&gt;&gt; fileh.redo()
&gt;&gt;&gt; print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'
/anarray (Array(2,)) 'An array'</screen>

          <para>Great! The <literal>/anarray</literal> array has come into
          life again. Just check that it is alive and well:</para>

          <screen>&gt;&gt;&gt; fileh.root.anarray.read()
[3, 4]
&gt;&gt;&gt; fileh.root.anarray.title
'An array'</screen>

          <para>Well, it looks pretty similar than in its previous life;
          what's more, it is exactly the same object!:</para>

          <screen>&gt;&gt;&gt; fileh.root.anarray is one
True</screen>

          <para>It just was moved to the the hidden group and back again, but
          that's all! That's kind of fun, so we are going to do the same with
          <literal>/anotherarray</literal>:</para>

          <screen>&gt;&gt;&gt; fileh.redo()
&gt;&gt;&gt; print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'
/anarray (Array(2,)) 'An array'
/anotherarray (Array(2,)) 'Another array'</screen>

          <para>Welcome back, <literal>/anotherarray</literal>! Just a couple
          of sanity checks:</para>

          <screen>&gt;&gt;&gt; assert fileh.root.anotherarray.read() == [4,5]
&gt;&gt;&gt; assert fileh.root.anotherarray.title == "Another array"
&gt;&gt;&gt; fileh.root.anotherarray is another
True</screen>

          <para>Nice, you managed to turn your data back into life.
          Congratulations! But wait, do not forget to close your action log
          when you don't need this feature anymore:</para>

          <screen>&gt;&gt;&gt; fileh.disableUndo()</screen>

          <para>That will allow you to continue working with your data without
          actually requiring PyTables to keep track of all your actions, and
          more importantly, allowing your objects to die completely if they
          have to, not requiring to keep them anywhere, and hence saving
          process time and space in your database file.</para>
        </section>

        <section>
          <title>A more complete example</title>

          <para>Now, time for a somewhat more sophisticated demonstration of
          the Undo/Redo feature. In it, several marks will be set in different
          parts of the code flow and we will see how to jump between these
          marks with just one method call. You can find the code used in this
          example in <literal>examples/tutorial3-2.py</literal></para>

          <para>Let's introduce the first part of the code:</para>

          <screen>import tables

# Create an HDF5 file
fileh = tables.openFile('tutorial3-2.h5', 'w', title='Undo/Redo demo 2')

         #'-**-**-**-**-**-**- enable undo/redo log  -**-**-**-**-**-**-**-'
fileh.enableUndo()

# Start undoable operations
fileh.createArray('/', 'otherarray1', [3,4], 'Another array 1')
fileh.createGroup('/', 'agroup', 'Group 1')
# Create a 'first' mark
fileh.mark('first')
fileh.createArray('/agroup', 'otherarray2', [4,5], 'Another array 2')
fileh.createGroup('/agroup', 'agroup2', 'Group 2')
# Create a 'second' mark
fileh.mark('second')
fileh.createArray('/agroup/agroup2', 'otherarray3', [5,6], 'Another array 3')
# Create a 'third' mark
fileh.mark('third')
fileh.createArray('/', 'otherarray4', [6,7], 'Another array 4')
fileh.createArray('/agroup', 'otherarray5', [7,8], 'Another array 5')</screen>

          <para>You can see how we have set several marks interspersed in the
          code flow, representing different states of the database. Also, note
          that we have assigned <emphasis>names</emphasis> to these marks,
          namely <literal>'first'</literal>, <literal>'second'</literal> and
          <literal>'third'</literal>.</para>

          <para>Now, start doing some jumps back and forth in the states of
          the database:</para>

          <screen># Now go to mark 'first'
fileh.goto('first')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' not in fileh
assert '/agroup/otherarray2' not in fileh
assert '/agroup/agroup2/otherarray3' not in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Go to mark 'third'
fileh.goto('third')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Now go to mark 'second'
fileh.goto('second')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' not in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh</screen>

          <para>Well, the code above shows how easy is to jump to a certain
          mark in the database by using the <literal>goto()</literal> method
          (see <xref linkend="File.goto" />).</para>

          <para>There are also a couple of implicit marks for going to the
          beginning or the end of the saved states: 0 and -1. Going to mark #0
          means go to the beginning of the saved actions, that is, when method
          <literal>fileh.enableUndo()</literal> was called. Going to mark #-1
          means go to the last recorded action, that is the last action in the
          code flow.</para>

          <para>Let's see what happens when going to the end of the action
          log:</para>

          <screen># Go to the end
fileh.goto(-1)
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' in fileh
assert '/otherarray4' in fileh
assert '/agroup/otherarray5' in fileh
# Check that objects have come back to life in a sane state
assert fileh.root.otherarray1.read() == [3,4]
assert fileh.root.agroup.otherarray2.read() == [4,5]
assert fileh.root.agroup.agroup2.otherarray3.read() == [5,6]
assert fileh.root.otherarray4.read() == [6,7]
assert fileh.root.agroup.otherarray5.read() == [7,8]</screen>

          <para>Try yourself going to the beginning of the action log
          (remember, the mark #0) and check the contents of the object
          tree.</para>

          <para>We have nearly finished this demonstration. As always, do not
          forget to close the action log as well as the database:</para>

          <screen>#'-**-**-**-**-**-**- disable undo/redo log  -**-**-**-**-**-**-**-'
fileh.disableUndo()

# Close the file
fileh.close()</screen>

          <para>You might want to check other examples on Undo/Redo feature
          that appear in <literal>examples/undo-redo.py</literal>.</para>
        </section>
      </section>

      <section>
        <title>Using enumerated types</title>

        <para>PyTables includes support for handling enumerated types. Those
        types are defined by providing an exhaustive <emphasis>set</emphasis>
        or <emphasis>list</emphasis> of possible, named values for a variable
        of that type. Enumerated variables of the same type are usually
        compared between them for equality and sometimes for order, but are
        not usually operated upon.</para>

        <para>Enumerated values have an associated <emphasis>name</emphasis>
        and <emphasis>concrete value</emphasis>. Every name is unique and so
        are concrete values. An enumerated variable always takes the concrete
        value, not its name. Usually, the concrete value is not used directly,
        and frequently it is entirely irrelevant. For the same reason, an
        enumerated variable is not usually compared with concrete values out
        of its enumerated type. For that kind of use, standard variables and
        constants are more adequate.</para>

        <para>PyTables provides the <literal>Enum</literal> (see <xref
        linkend="EnumClassDescr" xrefstyle="select: label" />) class to
        provide support for enumerated types. Each instance of
        <literal>Enum</literal> is an enumerated type (or
        <emphasis>enumeration</emphasis>). For example, let us create an
        enumeration of colors<footnote>
            <para>All these examples can be found in
            <literal>examples/enum.py</literal>.</para>
          </footnote>:</para>

        <screen>&gt;&gt;&gt; import tables
&gt;&gt;&gt; colorList = ['red', 'green', 'blue', 'white', 'black']
&gt;&gt;&gt; colors = tables.Enum(colorList)</screen>

        <para>Here we used a simple list giving the names of enumerated
        values, but we left the choice of concrete values up to the
        <literal>Enum</literal> class. Let us see the enumerated pairs to
        check those values:</para>

        <screen>&gt;&gt;&gt; print "Colors:", [v for v in colors]
Colors: [('blue', 2), ('black', 4), ('white', 3), ('green', 1), ('red', 0)]</screen>

        <para>Names have been given automatic integer concrete values. We can
        iterate over the values in an enumeration, but we will usually be more
        interested in accessing single values. We can get the concrete value
        associated with a name by accessing it as an attribute or as an item
        (the later can be useful for names not resembling Python
        identifiers):</para>

        <screen>&gt;&gt;&gt; print "Value of 'red' and 'white':", (colors.red, colors.white)
Value of 'red' and 'white': (0, 3)
&gt;&gt;&gt; print "Value of 'yellow':", colors.yellow
Value of 'yellow':
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
  File ".../tables/misc/enum.py", line 230, in __getattr__
    raise AttributeError(*ke.args)
AttributeError: no enumerated value with that name: 'yellow'
&gt;&gt;&gt; 
&gt;&gt;&gt; print "Value of 'red' and 'white':", (colors['red'], colors['white'])
Value of 'red' and 'white': (0, 3)
&gt;&gt;&gt; print "Value of 'yellow':", colors['yellow']
Value of 'yellow':
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
  File ".../tables/misc/enum.py", line 189, in __getitem__
    raise KeyError("no enumerated value with that name: %r" % (name,))
KeyError: "no enumerated value with that name: 'yellow'"</screen>

        <para>See how accessing a value that is not in the enumeration raises
        the appropriate exception. We can also do the opposite action and get
        the name that matches a concrete value by using the
        <literal>__call__()</literal> method of
        <literal>Enum</literal>:</para>

        <screen>&gt;&gt;&gt; print "Name of value %s:" % colors.red, colors(colors.red)
Name of value 0: red
&gt;&gt;&gt; print "Name of value 1234:", colors(1234)
Name of value 1234:
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
  File ".../tables/misc/enum.py", line 320, in __call__
    raise ValueError(
ValueError: no enumerated value with that concrete value: 1234</screen>

        <para>You can see what we made as using the enumerated type to
        <emphasis>convert</emphasis> a concrete value into a name in the
        enumeration. Of course, values out of the enumeration can not be
        converted.</para>

        <section>
          <title>Enumerated columns</title>

          <para>Columns of an enumerated type can be declared by using the
          <literal>EnumCol</literal> (see <xref linkend="ColClassDescr"
          xrefstyle="select: label" />) class. To see how this works, let us
          open a new PyTables file and create a table to collect the simulated
          results of a probabilistic experiment. In it, we have a bag full of
          colored balls; we take a ball out and annotate the time of
          extraction and the color of the ball.</para>

          <screen>&gt;&gt;&gt; h5f = tables.openFile('enum.h5', 'w')
&gt;&gt;&gt; class BallExt(tables.IsDescription):
      ballTime = tables.Time32Col()
      ballColor = tables.EnumCol(colors, 'black', base='uint8')
 
&gt;&gt;&gt; tbl = h5f.createTable(
      '/', 'extractions', BallExt, title="Random ball extractions")
&gt;&gt;&gt; </screen>

          <para>We declared the <literal>ballColor</literal> column to be of
          the enumerated type <literal>colors</literal>, with a default value
          of <literal>black</literal>. We also stated that we are going to
          store concrete values as unsigned 8-bit integer values<footnote>
              <para>In fact, only integer values are supported right now, but
              this may change in the future.</para>
            </footnote>.</para>

          <para>Let us use some random values to fill the table:</para>

          <screen>&gt;&gt;&gt; import time
&gt;&gt;&gt; import random
&gt;&gt;&gt; now = time.time()
&gt;&gt;&gt; row = tbl.row
&gt;&gt;&gt; for i in range(10):
      row['ballTime'] = now + i
      row['ballColor'] = colors[random.choice(colorList)]  # notice this
      row.append()

&gt;&gt;&gt; </screen>

          <para>Notice how we used the <literal>__getitem__()</literal> call
          of <literal>colors</literal> to get the concrete value to store in
          <literal>ballColor</literal>. You should know that this way of
          appending values to a table does automatically check for the
          validity on enumerated values. For instance:</para>

          <screen>&gt;&gt;&gt; row['ballTime'] = now + 42
&gt;&gt;&gt; row['ballColor'] = 1234
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "tableExtension.pyx", line 1086, in tableExtension.Row.__setitem__
  File ".../tables/misc/enum.py", line 320, in __call__
    "no enumerated value with that concrete value: %r" % (value,))
ValueError: no enumerated value with that concrete value: 1234</screen>

          <para>But take care that this check is <emphasis>only</emphasis>
          performed here and not in other methods such as
          <literal>tbl.append()</literal> or
          <literal>tbl.modifyRows()</literal>. Now, after flushing the table
          we can see the results of the insertions:</para>

          <screen>&gt;&gt;&gt; tbl.flush()
&gt;&gt;&gt; for r in tbl:
      ballTime = r['ballTime']
      ballColor = colors(r['ballColor'])  # notice this
      print "Ball extracted on %d is of color %s." % (ballTime, ballColor)

Ball extracted on 1173785568 is of color green.
Ball extracted on 1173785569 is of color black.
Ball extracted on 1173785570 is of color white.
Ball extracted on 1173785571 is of color black.
Ball extracted on 1173785572 is of color black.
Ball extracted on 1173785573 is of color red.
Ball extracted on 1173785574 is of color green.
Ball extracted on 1173785575 is of color red.
Ball extracted on 1173785576 is of color white.
Ball extracted on 1173785577 is of color white.</screen>

          <para>As a last note, you may be wondering how to have access to the
          enumeration associated with <literal>ballColor</literal> once the
          file is closed and reopened. You can call
          <literal>tbl.getEnum('ballColor')</literal> (see <xref
          linkend="Table_methods" xrefstyle="select: label" />) to get the
          enumeration back.</para>
        </section>

        <section>
          <title>Enumerated arrays</title>

          <para><literal>EArray</literal> and <literal>VLArray</literal>
          leaves can also be declared to store enumerated values by means of
          the <literal>EnumAtom</literal> (see <xref linkend="AtomClassDescr"
          xrefstyle="select: label" />) class, which works very much like
          <literal>EnumCol</literal> for tables. Also,
          <literal>Array</literal> leaves can be used to open native HDF
          enumerated arrays.</para>

          <para>Let us create a sample <literal>EArray</literal> containing
          ranges of working days as bidimensional values:</para>

          <screen>&gt;&gt;&gt; workingDays = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5}
&gt;&gt;&gt; dayRange = tables.EnumAtom(workingDays, 'Mon', base='uint16')
&gt;&gt;&gt; earr = h5f.createEArray('/', 'days', dayRange, (0, 2), title="Working day ranges")
&gt;&gt;&gt; earr.flavor = 'python'</screen>

          <para>Nothing surprising, except for a pair of details. In the first
          place, we use a <emphasis>dictionary</emphasis> instead of a list to
          explicitly set concrete values in the enumeration. In the second
          place, there is no explicit <literal>Enum</literal> instance
          created! Instead, the dictionary is passed as the first argument to
          the constructor of <literal>EnumAtom</literal>. If the constructor
          gets a list or a dictionary instead of an enumeration, it
          automatically builds the enumeration from it.</para>

          <para>Now let us feed some data to the array:</para>

          <screen>&gt;&gt;&gt; wdays = earr.getEnum()
&gt;&gt;&gt; earr.append([(wdays.Mon, wdays.Fri), (wdays.Wed, wdays.Fri)])
&gt;&gt;&gt; earr.append([(wdays.Mon, 1234)])</screen>

          <para>Please note that, since we had no explicit
          <literal>Enum</literal> instance, we were forced to use
          <literal>getEnum()</literal> (see <xref linkend="EArrayMethodsDescr"
          xrefstyle="select: label" />) to get it from the array (we could
          also have used <literal>dayRange.enum</literal>).  Also note that we
          were able to append an invalid value (1234). Array methods do not
          check the validity of enumerated values.</para>

          <para>Finally, we will print the contents of the array:</para>

          <screen>&gt;&gt;&gt; for (d1, d2) in earr:
      print "From %s to %s (%d days)." % (wdays(d1), wdays(d2), d2-d1+1)

From Mon to Fri (5 days).
From Wed to Fri (3 days).
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 2, in &lt;module&gt;
  File ".../tables/misc/enum.py", line 320, in __call__
    "no enumerated value with that concrete value: %r" % (value,))
ValueError: no enumerated value with that concrete value: 1234</screen>

          <para>That was an example of operating on concrete values. It also
          showed how the value-to-name conversion failed because of the value
          not belonging to the enumeration.</para>

          <para>Now we will close the file, and this little tutorial on
          enumerated types is done:</para>

          <screen>&gt;&gt;&gt; h5f.close()</screen>
        </section>
      </section>

      <section>
        <title>Dealing with nested structures in tables</title>

        <para>PyTables supports the handling of nested structures (or nested
        datatypes, as you prefer) in table objects, allowing you to define
        arbitrarily nested columns.</para>

        <para>An example will clarify what this means. Let's suppose that you
        want to group your data in pieces of information that are more related
        than others pieces in your table, So you may want to tie them up
        together in order to have your table better structured but also be
        able to retrieve and deal with these groups more easily.</para>

        <para>You can create such a nested substructures by just nesting
        subclasses of <literal>IsDescription</literal>. Let's see one example
        (okay, it's a bit silly, but will serve for demonstration
        purposes):</para>

        <screen>from tables import *

class Info(IsDescription):
    """A sub-structure of Test"""
    _v_pos = 2   # The position in the whole structure
    name = StringCol(10)
    value = Float64Col(pos=0)

colors = Enum(['red', 'green', 'blue'])

class NestedDescr(IsDescription):
    """A description that has several nested columns"""
    color = EnumCol(colors, 'red', base='uint32')
    info1 = Info()
    class info2(IsDescription):
        _v_pos = 1
        name = StringCol(10)
        value = Float64Col(pos=0)
        class info3(IsDescription):
            x = Float64Col(dflt=1)
            y = UInt8Col(dflt=1)</screen>

        <para>The root class is <literal>NestedDescr</literal> and both
        <literal>info1</literal> and <literal>info2</literal> are
        <emphasis>substructures</emphasis> of it. Note how
        <literal>info1</literal> is actually an instance of the class
        <literal>Info</literal> that was defined prior to
        <literal>NestedDescr</literal>. Also, there is a third substructure,
        namely <literal>info3</literal> that hangs from the substructure
        <literal>info2</literal>. You can also define positions of
        substructures in the containing object by declaring the special class
        attribute <literal>_v_pos</literal>.</para>

        <section>
          <title>Nested table creation</title>

          <para>Now that we have defined our nested structure, let's create a
          <emphasis>nested</emphasis> table, that is a table with columns that
          contain other subcolumns.</para>

          <screen>&gt;&gt;&gt; fileh = openFile("nested-tut.h5", "w")
&gt;&gt;&gt; table = fileh.createTable(fileh.root, 'table', NestedDescr)</screen>

          <para>Done! Now, we have to feed the table with some values. The
          problem is how we are going to reference to the nested fields.
          That's easy, just use a <literal>'/'</literal> character to separate
          names in different nested levels. Look at this:</para>

          <screen>&gt;&gt;&gt; row = table.row
&gt;&gt;&gt; for i in range(10):
       row['color'] = colors[['red', 'green', 'blue'][i%3]]
       row['info1/name'] = "name1-%s" % i
       row['info2/name'] = "name2-%s" % i
       row['info2/info3/y'] =  i
       # All the rest will be filled with defaults
       row.append()

&gt;&gt;&gt; table.flush()
&gt;&gt;&gt; table.nrows
10L</screen>

          <para>You see? In order to fill the fields located in the
          substructures, we just need to specify its full path in the table
          hierarchy.</para>
        </section>

        <section>
          <title>Reading nested tables</title>

          <para>Now, what happens if we want to read the table? What kind of
          data container will we get? Well, it's worth trying it:</para>

          <screen>&gt;&gt;&gt; nra = table[::4]
&gt;&gt;&gt; nra
array([(((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L),
       (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L),
       (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)], 
      dtype=[('info2', [('info3', [('x', '&gt;f8'), ('y', '|u1')]),
             ('name', '|S10'), ('value', '&gt;f8')]),
	     ('info1', [('name', '|S10'), ('value', '&gt;f8')]),
	     ('color', '&gt;u4')])</screen>

          <para>What we got is a NumPy array with a <emphasis>compound, nested
          datatype</emphasis> (its <literal>dtype</literal> is a list of
          name-datatype tuples). We read one row for each four in the table,
          giving a result of three rows.</para>

          <note>
            <para>When using the <literal>numarray</literal> flavor, you will
            get an instance of the <literal>NestedRecArray</literal> class
            that lives in the <literal>tables.nra</literal> package.
            <literal>NestedRecArray</literal> is actually a subclass of the
            <literal>RecArray</literal> object of the
            <literal>numarray.records</literal> module. You can get more info
            about <literal>NestedRecArray</literal> object in <xref
            linkend="NestedRecArrayClassDescr"
            xrefstyle="select: label" />.</para>
          </note>

          <para>You can make use of the above object in many different ways.
          For example, you can use it to append new data to the existing table
          object:</para>

          <screen>&gt;&gt;&gt; table.append(nra)
&gt;&gt;&gt; table.nrows
13L</screen>

          <para>Or, to create new tables:</para>

          <screen>&gt;&gt;&gt; table2 = fileh.createTable(fileh.root, 'table2', nra)
&gt;&gt;&gt; table2[:]
array([(((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L),
       (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L),
       (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)], 
      dtype=[('info2', [('info3', [('x', '&lt;f8'), ('y', '|u1')]),
             ('name', '|S10'), ('value', '&lt;f8')]),
	     ('info1', [('name', '|S10'), ('value', '&lt;f8')]),
	     ('color', '&lt;u4')])</screen>

          <para>Finally, we can select nested values that fulfill some
          condition:</para>

          <screen>&gt;&gt;&gt; names = [ x['info2/name'] for x in table if x['color'] == colors.red ]
&gt;&gt;&gt; names
['name2-0', 'name2-3', 'name2-6', 'name2-9', 'name2-0']</screen>

          <para>Note that the row accessor does not provide the natural naming
          feature, so you have to completely specify the path of your desired
          columns in order to reach them.</para>
        </section>

        <section>
          <title>Using Cols accessor</title>

          <para>We can use the <literal>cols</literal> attribute object (see
          <xref linkend="ColsClassDescr" xrefstyle="select: label" />) of the
          table so as to quickly access the info located in the interesting
          substructures:</para>

          <screen>&gt;&gt;&gt; table.cols.info2[1:5]
array([((1.0, 1), 'name2-1', 0.0), ((1.0, 2), 'name2-2', 0.0),
       ((1.0, 3), 'name2-3', 0.0), ((1.0, 4), 'name2-4', 0.0)], 
      dtype=[('info3', [('x', '&lt;f8'), ('y', '|u1')]), ('name', '|S10'),
             ('value', '&lt;f8')])</screen>

          <para>Here, we have made use of the cols accessor to access to the
          <emphasis>info2</emphasis> substructure and an slice operation to
          get access to the subset of data we were interested in; you probably
          have recognized the natural naming approach here. We can continue
          and ask for data in <emphasis>info3</emphasis> substructure:</para>

          <screen>&gt;&gt;&gt; table.cols.info2.info3[1:5]
array([(1.0, 1), (1.0, 2), (1.0, 3), (1.0, 4)], 
      dtype=[('x', '&lt;f8'), ('y', '|u1')])</screen>

          <para>You can also use the <literal>_f_col</literal> method to get a
          handler for a column:</para>

          <screen>&gt;&gt;&gt; table.cols._f_col('info2')
/table.cols.info2 (Cols), 3 columns
  info3 (Cols(), Description)
  name (Column(), |S10)
  value (Column(), float64)</screen>

          <para>Here, you've got another <literal>Cols</literal> object
          handler because <emphasis>info2</emphasis> was a nested column. If
          you select a non-nested column, you will get a regular
          <literal>Column</literal> instance:</para>

          <screen>&gt;&gt;&gt; table.cols._f_col('info2/info3/y')
/table.cols.info2.info3.y (Column(), uint8, idx=None)</screen>

          <para>To sum up, the <literal>cols</literal> accessor is a very
          handy and powerful way to access data in your nested tables. Don't
          be afraid of using it, specially when doing interactive work.</para>
        </section>

        <section>
          <title>Accessing meta-information of nested tables</title>

          <para>Tables have an attribute called <literal>description</literal>
          which points to an instance of the <literal>Description</literal>
          class (see <xref linkend="DescriptionClassDescr" xrefstyle="select:
          label" />) and is useful to discover different meta-information
          about table data.</para>

          <para>Let's see how it looks like:</para>

          <screen>&gt;&gt;&gt; table.description
{
  "info2": {
    "info3": {
      "x": Float64Col(shape=(), dflt=1.0, pos=0),
      "y": UInt8Col(shape=(), dflt=1, pos=1)},
    "name": StringCol(itemsize=10, shape=(), dflt='', pos=1),
    "value": Float64Col(shape=(), dflt=0.0, pos=2)},
  "info1": {
    "name": StringCol(itemsize=10, shape=(), dflt='', pos=0),
    "value": Float64Col(shape=(), dflt=0.0, pos=1)},
  "color": EnumCol(enum=Enum({'blue': 2, 'green': 1, 'red': 0}), dflt='red',
                   base=UInt32Atom(shape=(), dflt=0), shape=(), pos=2)}
</screen>

          <para>As you can see, it provides very useful information on both
          the formats and the structure of the columns in your table.</para>

          <para>This object also provides a natural naming approach to access
          to subcolumns metadata:</para>

          <screen>&gt;&gt;&gt; table.description.info1
{
    "name": StringCol(itemsize=10, shape=(), dflt='', pos=0),
    "value": Float64Col(shape=(), dflt=0.0, pos=1)}
&gt;&gt;&gt; table.description.info2.info3
{
      "x": Float64Col(shape=(), dflt=1.0, pos=0),
      "y": UInt8Col(shape=(), dflt=1, pos=1)}</screen>

          <para>There are other variables that can be interesting for
          you:</para>

          <screen>&gt;&gt;&gt; table.description._v_nestedNames
[('info2', [('info3', ['x', 'y']), 'name', 'value']),
 ('info1', ['name', 'value']), 'color']
&gt;&gt;&gt; table.description.info1._v_nestedNames
['name', 'value']</screen>

          <para><literal>_v_nestedNames</literal> provides the names of the
          columns as well as its structure. You can see that there are the
          same attributes for the different levels of the
          <literal>Description</literal> object, because the levels are
          <emphasis>also</emphasis> <literal>Description</literal> objects
          themselves.</para>

          <para>There is a special attribute, called
          <literal>_v_nestedDescr</literal>, that can be useful to create
          nested record arrays that imitate the structure of the table (or a
          subtable thereof):</para>

          <screen>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; table.description._v_nestedDescr
[('info2', [('info3', [('x', '()f8'), ('y', '()u1')]), ('name', '()S10'),
 ('value', '()f8')]), ('info1', [('name', '()S10'), ('value', '()f8')]),
 ('color', '()u4')]
&gt;&gt;&gt; numpy.rec.array(None, shape=0,
                    dtype=table.description._v_nestedDescr)
recarray([], 
      dtype=[('info2', [('info3', [('x', '&gt;f8'), ('y', '|u1')]),
             ('name', '|S10'), ('value', '&gt;f8')]),
	     ('info1', [('name', '|S10'), ('value', '&gt;f8')]),
	     ('color', '&gt;u4')])
&gt;&gt;&gt; numpy.rec.array(None, shape=0,
                    dtype=table.description.info2._v_nestedDescr)
recarray([], 
      dtype=[('info3', [('x', '&gt;f8'), ('y', '|u1')]), ('name', '|S10'),
             ('value', '&gt;f8')])
&gt;&gt;&gt; from tables import nra
&gt;&gt;&gt; nra.array(None, descr=table.description._v_nestedDescr)
array(
[],
descr=[('info2', [('info3', [('x', '()f8'), ('y', '()u1')]),
       ('name', '()S10'), ('value', '()f8')]), ('info1', [('name', '()S10'),
       ('value', '()f8')]), ('color', '()u4')],
shape=0)</screen>

          <para>You can see we have created two equivalent arrays: one with
          NumPy (the first) and one with the <literal>nra</literal> package
          (the last). The later implements nested record arrays for
          <literal>numarray</literal> (see <xref
          linkend="NestedRecArrayClassDescr"
          xrefstyle="select: label" />).</para>

          <para>Finally, there is a special iterator of the
          <literal>Description</literal> class, called
          <literal>_f_walk</literal> that is able to return you the different
          columns of the table:</para>

          <screen>&gt;&gt;&gt; for coldescr in table.description._f_walk():
      print "column--&gt;",coldescr

column--&gt; Description([('info2', [('info3', [('x', '()f8'), ('y', '()u1')]),
                       ('name', '()S10'), ('value', '()f8')]),
                       ('info1', [('name', '()S10'), ('value', '()f8')]),
                       ('color', '()u4')])
column--&gt; EnumCol(enum=Enum({'blue': 2, 'green': 1, 'red': 0}), dflt='red',
                            base=UInt32Atom(shape=(), dflt=0), shape=(), pos=2)
column--&gt; Description([('info3', [('x', '()f8'), ('y', '()u1')]), ('name', '()S10'),
                       ('value', '()f8')])
column--&gt; StringCol(itemsize=10, shape=(), dflt='', pos=1)
column--&gt; Float64Col(shape=(), dflt=0.0, pos=2)
column--&gt; Description([('name', '()S10'), ('value', '()f8')])
column--&gt; StringCol(itemsize=10, shape=(), dflt='', pos=0)
column--&gt; Float64Col(shape=(), dflt=0.0, pos=1)
column--&gt; Description([('x', '()f8'), ('y', '()u1')])
column--&gt; Float64Col(shape=(), dflt=1.0, pos=0)
column--&gt; UInt8Col(shape=(), dflt=1, pos=1)</screen>

          <para>See the <xref linkend="DescriptionClassDescr"
          xrefstyle="select: label" /> for the complete listing of attributes
          and methods of <literal>Description</literal>.</para>

          <para>Well, this is the end of this tutorial. As always, do not
          forget to close your files:</para>

          <screen>&gt;&gt;&gt; fileh.close() </screen>

          <para>Finally, you may want to have a look at your resulting data
          file:</para>

          <screen>$ ptdump -d nested-tut.h5
/ (RootGroup) ''
/table (Table(13L,)) ''
  Data dump:
[0] (((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L)
[1] (((1.0, 1), 'name2-1', 0.0), ('name1-1', 0.0), 1L)
[2] (((1.0, 2), 'name2-2', 0.0), ('name1-2', 0.0), 2L)
[3] (((1.0, 3), 'name2-3', 0.0), ('name1-3', 0.0), 0L)
[4] (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L)
[5] (((1.0, 5), 'name2-5', 0.0), ('name1-5', 0.0), 2L)
[6] (((1.0, 6), 'name2-6', 0.0), ('name1-6', 0.0), 0L)
[7] (((1.0, 7), 'name2-7', 0.0), ('name1-7', 0.0), 1L)
[8] (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)
[9] (((1.0, 9), 'name2-9', 0.0), ('name1-9', 0.0), 0L)
[10] (((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L)
[11] (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L)
[12] (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)
/table2 (Table(3L,)) ''
  Data dump:
[0] (((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L)
[1] (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L)
[2] (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)</screen>

          <para>Most of the code in this section is also available in
          <literal>examples/nested-tut.py</literal>.</para>

          <para>All in all, PyTables provides a quite comprehensive toolset to
          cope with nested structures and address your classification needs.
          However, caveat emptor, be sure to not nest your data too deeply or
          you will get inevitably messed interpreting too intertwined lists,
          tuples and description objects.</para>
        </section>
      </section>

      <section>
        <title>Other examples in PyTables distribution</title>

        <para>Feel free to examine the rest of examples in directory
        <literal>examples/</literal>, and try to understand them. We have
        written several practical sample scripts to give you an idea of the
        PyTables capabilities, its way of dealing with HDF5 objects, and how
        it can be used in the real world.</para>
      </section>
    </chapter>

    <chapter id="libraryReference">
      <title>Library Reference</title>

      <para></para>

      <para>PyTables implements several classes to represent the different
      nodes in the object tree. They are named <literal>File</literal>,
      <literal>Group</literal>, <literal>Leaf</literal>,
      <literal>Table</literal>, <literal>Array</literal>,
      <literal>CArray</literal>, <literal>EArray</literal>,
      <literal>VLArray</literal> and <literal>UnImplemented</literal>. Another
      one allows the user to complement the information on these different
      objects; its name is <literal>AttributeSet</literal>. Finally, another
      important class called <literal>IsDescription</literal> allows to build
      a <literal>Table</literal> record description by declaring a subclass of
      it. Many other classes are defined in PyTables, but they can be regarded
      as helpers whose goal is mainly to declare the <emphasis>data type
      properties</emphasis> of the different first class objects and will be
      described at the end of this chapter as well.</para>

      <para>An important function, called <literal>openFile</literal> is
      responsible to create, open or append to files. In addition, a few
      utility functions are defined to guess if the user supplied file is a
      <emphasis>PyTables</emphasis> or <emphasis>HDF5</emphasis> file. These
      are called <literal>isPyTablesFile()</literal> and
      <literal>isHDF5File()</literal>, respectively. There exists also a
      function called <literal>whichLibVersion()</literal> that informs about
      the versions of the underlying C libraries (for example, HDF5 or
      <literal>Zlib</literal>) and another called
      <literal>print_versions()</literal> that prints all the versions of the
      software that PyTables relies on. Finally, <literal>test()</literal>
      lets you run the complete test suite from a Python console
      interactively.</para>

      <para>Let's start discussing the first-level variables and functions
      available to the user, then the different classes defined in
      PyTables.</para>

      <section>
        <title><literal>tables</literal> variables and functions</title>

        <section>
          <title>Global variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">__version__</emphasis></glossterm>

              <glossdef>
                <para>The PyTables version number.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">hdf5Version</emphasis></glossterm>

              <glossdef>
                <para>The underlying HDF5 library version number.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">is_pro</emphasis></glossterm>

              <glossdef>
                <para>True for PyTables Professional edition, false
                otherwise.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section id="GlobalFunctDescr">
          <title>Global functions</title>

          <section id="copyFileDescr">
            <title>copyFile(srcfilename, dstfilename, overwrite=False,
            **kwargs)</title>

            <para>An easy way of copying one PyTables file to another.</para>

            <para>This function allows you to copy an existing PyTables file
            named <literal>srcfilename</literal> to another file called
            <literal>dstfilename</literal>. The source file must exist and be
            readable. The destination file can be overwritten in place if
            existing by asserting the <literal>overwrite</literal>
            argument.</para>

            <para>This function is a shorthand for the
            <literal>File.copyFile()</literal> method, which acts on an
            already opened file. <literal>kwargs</literal> takes keyword
            arguments used to customize the copying process. See the
            documentation of <literal>File.copyFile()</literal> (see <xref
            linkend="File.copyFile" />) for a description of those
            arguments.</para>
          </section>

          <section id="isHDF5FileDescr">
            <title>isHDF5File(filename)</title>

            <para>Determine whether a file is in the HDF5 format.</para>

            <para>When successful, it returns a true value if the file is an
            HDF5 file, false otherwise. If there were problems identifying the
            file, an <literal>HDF5ExtError</literal> is raised.</para>
          </section>

          <section id="isPyTablesFileDescr">
            <title>isPyTablesFile(filename)</title>

            <para>Determine whether a file is in the PyTables format.</para>

            <para>When successful, it returns a true value if the file is a
            PyTables file, false otherwise. The true value is the format
            version string of the file. If there were problems identifying the
            file, an <literal>HDF5ExtError</literal> is raised.</para>
          </section>

          <section id="openFileDescr" xreflabel="description">
            <title>openFile(filename, mode='r', title='', trMap={},
            rootUEP="/", filters=None, nodeCacheSize=NODE_MAX_SLOTS)</title>

            <para>Open a PyTables (or generic HDF5) file and return a
            <literal>File</literal> object.</para>

            <para>Arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">filename</emphasis></glossterm>

                <glossdef>
                  <para>The name of the file (supports environment variable
                  expansion). It is suggested that file names have any of the
                  <literal>.h5</literal>, <literal>.hdf</literal> or
                  <literal>.hdf5</literal> extensions, although this is not
                  mandatory.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">mode</emphasis></glossterm>

                <glossdef>
                  <para>The mode in whichto open the file. It can be one of
                  the following:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'r'</emphasis></glossterm>

                      <glossdef>
                        <para>Read-only; no data can be modified.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'w'</emphasis></glossterm>

                      <glossdef>
                        <para>Write; a new file is created (an existing file
                        with the same name would be deleted).</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'a'</emphasis></glossterm>

                      <glossdef>
                        <para>Append; an existing file is opened for reading
                        and writing, and if the file does not exist it is
                        created.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'r+'</emphasis></glossterm>

                      <glossdef>
                        <para>It is similar to <literal>'a'</literal>, but the
                        file must already exist.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>If the file is to be created, a
                  <literal>TITLE</literal> string attribute will be set on the
                  root group with the given value. Otherwise, the title will
                  be read from disk, and this will not have any effect.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">trMap</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary to map names in the object tree into
                  different HDF5 names in file. The keys are the Python names,
                  while the values are the HDF5 names. This is useful when you
                  need to name HDF5 nodes with invalid or reserved words in
                  Python and you want to continue using the natural naming
                  facility on the nodes.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">rootUEP</emphasis></glossterm>

                <glossdef>
                  <para>The root User Entry Point. This is a group in the HDF5
                  hierarchy which will be taken as the starting point to
                  create the object tree. It can be whatever existing group in
                  the file, named by its HDF5 path. If it does not exist, an
                  <literal>HDF5ExtError</literal> is issued. Use this if you
                  do not want to build the <emphasis>entire</emphasis> object
                  tree, but rather only a <emphasis>subtree</emphasis> of
                  it.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>An instance of the <literal>Filters</literal> (see
                  <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />) class that provides
                  information about the desired I/O filters applicable to the
                  leaves that hang directly from the <emphasis>root
                  group</emphasis>, unless other filter properties are
                  specified for these leaves. Besides, if you do not specify
                  filter properties for child groups, they will inherit these
                  ones, which will in turn propagate to child nodes.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">nodeCacheSize</emphasis></glossterm>

                <glossdef>
                  <para>The number of <emphasis>unreferenced</emphasis> nodes
                  to be kept in memory. Least recently used nodes are unloaded
                  from memory when this number of loaded nodes is reached. To
                  load a node again, simply access it as usual. Nodes
                  referenced by user variables are not taken into account nor
                  unloaded.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="print_versions">
            <title>print_versions()</title>

            <para>Print all the versions of software that PyTables relies
            on.</para>
          </section>

          <section id="restrict_flavors">
            <title>restrict_flavors(keep=['python']</title>

            <para>Disable all flavors except those in
            <literal>keep</literal>.</para>

            <para>Providing an empty <literal>keep</literal> sequence implies
            disabling all flavors (but the internal one).  If the sequence is
            not specified, only optional flavors are disabled.</para>

            <important>
              <para>Once you disable a flavor, it can not be enabled
              again.</para>
            </important>
          </section>

          <section>
            <title>split_type(type)</title>

            <para>Split a PyTables <literal>type</literal> into a PyTables
            kind and an item size.</para>

            <para>Returns a tuple of <literal>(kind, itemsize)</literal>. If
            no item size is present in the <literal>type</literal> (in the
            form of a precision), the returned item size is
            <literal>None</literal>.</para>

            <screen>
&gt;&gt;&gt; split_type('int32')
('int', 4)
&gt;&gt;&gt; split_type('string')
('string', None)
&gt;&gt;&gt; split_type('int20')
Traceback (most recent call last):
  ...
ValueError: precision must be a multiple of 8: 20
&gt;&gt;&gt; split_type('foo bar')
Traceback (most recent call last):
  ...
ValueError: malformed type: 'foo bar'</screen>
          </section>

          <section id="testDescr">
            <title>test(verbose=False, heavy=False)</title>

            <para>Run all the tests in the test suite.</para>

            <para>If <literal>verbose</literal> is set, the test suite will
            emit messages with full verbosity (not recommended unless you are
            looking into a certain problem).</para>

            <para>If <literal>heavy</literal> is set, the test suite will be
            run in <emphasis>heavy</emphasis> mode (you should be careful with
            this because it can take a lot of time and resources from your
            computer).</para>
          </section>

          <section id="whichLibVersionDescr">
            <title>whichLibVersion(name)</title>

            <para>Get version information about a C library.</para>

            <para>If the library indicated by <literal>name</literal> is
            available, this function returns a 3-tuple containing the major
            library version as an integer, its full version as a string, and
            the version date as a string. If the library is not available,
            <literal>None</literal> is returned.</para>

            <para>The currently supported library names are
            <literal>hdf5</literal>, <literal>zlib</literal>,
            <literal>lzo</literal> and <literal>bzip2</literal>. If another
            name is given, a <literal>ValueError</literal> is raised.</para>
          </section>
        </section>
      </section>

      <section id="FileClassDescr">
        <title>The <literal>File</literal> class</title>

        <para>In-memory representation of a PyTables file.</para>

        <para>An instance of this class is returned when a PyTables file is
        opened with the <literal>openFile()</literal> (see <xref
        linkend="openFileDescr"/>) function. It offers methods to manipulate
        (create, rename, delete...) nodes and handle their attributes, as well
        as methods to traverse the object tree. The <emphasis>user entry
        point</emphasis> to the object tree attached to the HDF5 file is
        represented in the <literal>rootUEP</literal> attribute. Other
        attributes are available.</para>

        <para><literal>File</literal> objects support an <emphasis>Undo/Redo
        mechanism</emphasis> which can be enabled with the
        <literal>enableUndo()</literal> (see <xref
        linkend="File.enableUndo"/>) method. Once the Undo/Redo mechanism is
        enabled, explicit <emphasis>marks</emphasis> (with an optional unique
        name) can be set on the state of the database using the
        <literal>mark()</literal> (see <xref linkend="File.mark"/>)
        method. There are two implicit marks which are always available: the
        initial mark (0) and the final mark (-1).  Both the identifier of a
        mark and its name can be used in <emphasis>undo</emphasis> and
        <emphasis>redo</emphasis> operations.</para>

        <para>Hierarchy manipulation operations (node creation, movement and
        removal) and attribute handling operations (setting and deleting) made
        after a mark can be undone by using the <literal>undo()</literal> (see
        <xref linkend="File.undo"/>) method, which returns the database to the
        state of a past mark. If <literal>undo()</literal> is not followed by
        operations that modify the hierarchy or attributes, the
        <literal>redo()</literal> (see <xref linkend="File.redo"/>) method can
        be used to return the database to the state of a future mark. Else,
        future states of the database are forgotten.</para>

        <para>Note that data handling operations can not be undone nor redone
        by now. Also, hierarchy manipulation operations on nodes that do not
        support the Undo/Redo mechanism issue an
        <literal>UndoRedoWarning</literal> <emphasis>before</emphasis>
        changing the database.</para>

        <para>The Undo/Redo mechanism is persistent between sessions and can
        only be disabled by calling the <literal>disableUndo()</literal> (see
        <xref linkend="File.disableUndo"/>) method.</para>

        <section>
          <title><literal>File</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">filename</emphasis></glossterm>

              <glossdef>
                <para>The name of the opened file.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">format_version</emphasis></glossterm>

              <glossdef>
                <para>The PyTables version number of this file.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">isopen</emphasis></glossterm>

              <glossdef>
                <para>True if the underlying file is open, false
                otherwise.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">mode
              </emphasis></glossterm>

              <glossdef>
                <para>The mode in which the file was opened.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">title
              </emphasis></glossterm>

              <glossdef>
                <para>The title of the root group in the file.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">trMap</emphasis></glossterm>

              <glossdef>
                <para>A dictionary that maps node names between PyTables and
                HDF5 domain names. Its initial values are set from the
                <literal>trMap</literal> parameter passed to the
                <literal>openFile()</literal> (see <xref
                linkend="openFileDescr"/>) function. You cannot change its
                contents <emphasis>after</emphasis> a file is opened.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">rootUEP</emphasis></glossterm>

              <glossdef>
                <para>The UEP (user entry point) group name in the file (see
                the <literal>openFile()</literal> function in <xref
                linkend="openFileDescr" xrefstyle="select: label" />).</para>

                <!-- manual-only -->
                <para>You can also have a look at <xref
                linkend="rootUEPOptim"/> for a more in-depth
                explanation.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">filters</emphasis></glossterm>

              <glossdef>
                <para>Default filter properties for the root group (see <xref
                linkend="FiltersClassDescr" xrefstyle="select: label"
                />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">root</emphasis></glossterm>

              <glossdef>
                <para>The <emphasis>root</emphasis> of the object tree
                hierarchy (a <literal>Group</literal> instance).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section id="FileMethods_file_handling">
          <title><literal>File</literal> methods — file handling</title>

          <section>
            <title>close()</title>

            <para>Flush all the alive leaves in object tree and close the
            file.</para>
          </section>

          <section id="File.copyFile" xreflabel="description">
            <title>copyFile(dstfilename, overwrite=False, **kwargs)</title>

            <para>Copy the contents of this file to
            <literal>dstfilename</literal>.</para>

            <para><literal>dstfilename</literal> must be a path string
            indicating the name of the destination file. If it already exists,
            the copy will fail with an <literal>IOError</literal>, unless the
            <literal>overwrite</literal> argument is true, in which case the
            destination file will be overwritten in place. In this last case,
            the destination file should be closed or ugly errors will
            happen.</para>

            <para>Additional keyword arguments may be passed to customize the
            copying process. For instance, title and filters may be changed,
            user attributes may be or may not be copied, data may be
            sub-sampled, stats may be collected, etc. Arguments unknown to
            nodes are simply ignored. Check the documentation for copying
            operations of nodes to see which options they support.</para>

            <para>Copying a file usually has the beneficial side effect of
            creating a more compact and cleaner version of the original
            file.</para>
          </section>

          <section>
            <title>flush()</title>

            <para>Flush all the alive leaves in the object tree.</para>
          </section>

          <section id="File.__str__">
            <title>__str__()</title>

            <para>Return a short string representation of the object
            tree.</para>

            <para>Example of use:</para>

            <screen>
>>> f = tables.openFile('data/test.h5')
>>> print f
data/test.h5 (File) 'Table Benchmark'
Last modif.: 'Mon Sep 20 12:40:47 2004'
Object Tree:
/ (Group) 'Table Benchmark'
/tuple0 (Table(100L,)) 'This is the table title'
/group0 (Group) ''
/group0/tuple1 (Table(100L,)) 'This is the table title'
/group0/group1 (Group) ''
/group0/group1/tuple2 (Table(100L,)) 'This is the table title'
/group0/group1/group2 (Group) ''</screen>
          </section>

          <section id="File.__repr__">
            <title>__repr__()</title>

            <para>Return a detailed string representation of the object
            tree.</para>
          </section>
        </section>

        <section id="FileMethods_hierarchy_manipulation">
          <title><literal>File</literal> methods — hierarchy manipulation</title>

          <section id="File.copyChildren">
            <title>copyChildren(srcgroup, dstgroup, overwrite=False,
            recursive=False, createparents=False, **kwargs)</title>

            <para>Copy the children of a group into another group.</para>

            <para>This method copies the nodes hanging from the source group
            <literal>srcgroup</literal> into the destination group
            <literal>dstgroup</literal>. Existing destination nodes can be
            replaced by asserting the <literal>overwrite</literal> argument.
            If the <literal>recursive</literal> argument is true, all
            descendant nodes of <literal>srcnode</literal> are recursively
            copied. If <literal>createparents</literal> is true, the needed
            groups for the given destination parent group path to exist will
            be created.</para>

            <para><literal>kwargs</literal> takes keyword arguments used to
            customize the copying process. See the documentation of
            <literal>Group._f_copyChildren()</literal> (see <xref
            linkend="Group._f_copyChildren" />) for a description of those
            arguments.</para>
          </section>

          <section id="File.copyNode">
            <title>copyNode(where, newparent=None, newname=None, name=None,
            overwrite=False, recursive=False, createparents=False,
            **kwargs)</title>

            <para>Copy the node specified by <literal>where</literal> and
            <literal>name</literal> to
            <literal>newparent/newname</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newparent</emphasis></glossterm>

                <glossdef>
                  <para>The destination group that the node will be copied
                  into (a path name or a <literal>Group</literal>
                  instance). If not specified or <literal>None</literal>, the
                  current parent group is chosen as the new parent.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newname</emphasis></glossterm>

                <glossdef>
                  <para>The name to be assigned to the new copy in its
                  destination (a string).  If it is not specified or
                  <literal>None</literal>, the current name is chosen as the
                  new name.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Additional keyword arguments may be passed to customize the
            copying process. The supported arguments depend on the kind of
            node being copied. See <literal>Group._f_copy()</literal> (<xref
            linkend="Group._f_copy"/>) and <literal>Leaf.copy()</literal>
            (<xref linkend="Leaf.copy"/>) for more information on their
            allowed keyword arguments.</para>

            <para>This method returns the newly created copy of the source
            node (i.e. the destination node).  See
            <literal>Node._f_copy()</literal> (<xref linkend="Node._f_copy"/>)
            for further details on the semantics of copying nodes.</para>
          </section>

          <section id="createArrayDescr" xreflabel="description">
            <title>createArray(where, name, object, title='', byteorder=None,
            createparents=False)</title>

            <para>Create a new array with the given <literal>name</literal> in
            <literal>where</literal> location.  See the
            <literal>Array</literal> class (in <xref linkend="ArrayClassDescr"
            xrefstyle="select: label" />) for more information on
            arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">object</emphasis></glossterm>

                <glossdef>
                  <para>The array or scalar to be saved.  Accepted types are
                  NumPy arrays and scalars, <literal>numarray</literal> arrays
                  and string arrays, Numeric arrays and scalars, as well as
                  native Python sequences and scalars, provided that values
                  are regular (i.e. they are not like
                  <literal>[[1,2],2]</literal>) and homogeneous (i.e. all the
                  elements are of the same type).</para>

                  <para>Also, objects that have some of their dimensions equal
                  to 0 are not supported (use an <literal>EArray</literal>
                  node (see <xref linkend="EArrayClassDescr"
                  xrefstyle="select: label" />) if you want to store an array
                  with one of its dimensions equal to 0).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">byteorder</emphasis></glossterm>

                <glossdef>
                  <para>The byteorder of the data <emphasis>on
                  disk</emphasis>, specified as <literal>'little'</literal> or
                  <literal>'big'</literal>.  If this is not specified, the
                  byteorder is that of the given
                  <literal>object</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createCArrayDescr">
            <title>createCArray(where, name, atom, shape, title='',
            filters=None, chunkshape=None, byteorder=None,
            createparents=False)</title>

            <para>Create a new chunked array with the given
            <literal>name</literal> in <literal>where</literal> location.  See
            the <literal>CArray</literal> class (in <xref
            linkend="CArrayClassDescr" xrefstyle="select: label" />) for more
            information on chunked arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">atom</emphasis></glossterm>

                <glossdef>
                  <para>An <literal>Atom</literal> (see <xref
                  linkend="AtomClassDescr" xrefstyle="select: label" />)
                  instance representing the <emphasis>type</emphasis> and
                  <emphasis>shape</emphasis> of the atomic objects to be
                  saved.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the new array.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation.  Filters are applied to those
                  chunks of data.  The dimensionality of
                  <literal>chunkshape</literal> must be the same as that of
                  <literal>shape</literal>.  If <literal>None</literal>, a
                  sensible value is calculated (which is recommended).</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createEArrayDescr">
            <title>createEArray(where, name, atom, shape, title='',
            filters=None, expectedrows=1000, chunkshape=None, byteorder=None,
            createparents=False)</title>

            <para>Create a new enlargeable array with the given
            <literal>name</literal> in <literal>where</literal> location.  See
            the <literal>EArray</literal> (in <xref linkend="EArrayClassDescr"
            xrefstyle="select: label" />) class for more information on
            enlargeable arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">atom</emphasis></glossterm>

                <glossdef>
                  <para>An <literal>Atom</literal> (see <xref
                  linkend="AtomClassDescr" xrefstyle="select: label" />)
                  instance representing the <emphasis>type</emphasis> and
                  <emphasis>shape</emphasis> of the atomic objects to be
                  saved.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the new array.  One (and only one) of the
                  shape dimensions <emphasis>must</emphasis> be 0.  The
                  dimension being 0 means that the resulting
                  <literal>EArray</literal> object can be extended along it.
                  Multiple enlargeable dimensions are not supported right
                  now.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">expectedrows</emphasis></glossterm>

                <glossdef>
                  <para>A user estimate about the number of row elements that
                  will be added to the growable dimension in the
                  <literal>EArray</literal> node.  If not provided, the
                  default value is 1000 rows.  If you plan to create either a
                  much smaller or a much bigger array try providing a guess;
                  this will optimize the HDF5 B-Tree creation and management
                  process time and the amount of memory used.  If you want to
                  specify your own chunk size for I/O purposes, see also the
                  <literal>chunkshape</literal> parameter below.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation.  Filters are applied to those
                  chunks of data.  The dimensionality of
                  <literal>chunkshape</literal> must be the same as that of
                  <literal>shape</literal> (beware: no dimension should be 0
                  this time!).  If <literal>None</literal>, a sensible value
                  is calculated (which is recommended).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">byteorder</emphasis></glossterm>

                <glossdef>
                  <para>The byteorder of the data <emphasis>on
                  disk</emphasis>, specified as <literal>'little'</literal> or
                  <literal>'big'</literal>. If this is not specified, the
                  byteorder is that of the platform.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createGroupDescr" xreflabel="description">
            <title>createGroup(where, name, title='', filters=None,
            createparents=False)</title>

            <para>Create a new group with the given <literal>name</literal> in
            <literal>where</literal> location.  See the
            <literal>Group</literal> class (in <xref linkend="GroupClassDescr"
            xrefstyle="select: label" />) for more information on
            groups.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>An instance of the <literal>Filters</literal> class
                  (see <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />) that provides information
                  about the desired I/O filters applicable to the leaves that
                  hang directly from this new group (unless other filter
                  properties are specified for these leaves). Besides, if you
                  do not specify filter properties for its child groups, they
                  will inherit these ones.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createTableDescr" xreflabel="description">
            <title>createTable(where, name, description, title='',
            filters=None, expectedrows=10000, chunkshape=None, byteorder=None,
            createparents=False)</title>

            <para>Create a new table with the given <literal>name</literal> in
            <literal>where</literal> location.  See the
            <literal>Table</literal> (in <xref linkend="TableClassDescr"
            xrefstyle="select: label" />) class for more information on
            tables.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where</emphasis></glossterm>

                <glossdef>
                  <para>The parent group where the new table will hang from.
                  It can be a path string (for example
                  <literal>'/level1/leaf5'</literal>), or a
                  <literal>Group</literal> instance (see <xref
                  linkend="GroupClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">name</emphasis></glossterm>

                <glossdef>
                  <para>The name of the new table.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">description</emphasis></glossterm>

                <glossdef>
                  <para></para>

                  <para>This is an object that describes the table, i.e. how
                  many columns it has, their names, types, shapes, etc.  It
                  can be any of the following:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis role="bold">A user-defined
                      class</emphasis></glossterm>

                      <glossdef>
                        <para>This should inherit from the
                        <literal>IsDescription</literal> class (see <xref
                        linkend="IsDescriptionClassDescr" xrefstyle="select:
                        label" />) where table fields are specified.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      dictionary</emphasis></glossterm>

                      <glossdef>
                        <para>For example, when you do not know beforehand
                        which structure your table will have).</para>

                        <!-- manual-only -->
                        <para>See <xref linkend="secondExample"
                        xrefstyle="select: label" /> for an example of using a
                        dictionary to describe a table.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>Description</literal>
                      instance</emphasis></glossterm>

                      <glossdef>
                        <para>You can use the <literal>description</literal>
                        attribute of another table to create a new one with
                        the same structure.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A NumPy (record)
                      array</emphasis></glossterm>

                      <glossdef>
                        <para>You can use a NumPy array, whether nested or
                        not, and its field structure will be reflected in the
                        new <literal>Table</literal> object. Moreover, if the
                        array has actual data it will be injected into the
                        newly created table. If you are using
                        <literal>numarray</literal> instead of NumPy, you may
                        use one of the objects below for the same
                        purpose.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>RecArray</literal>
                      instance</emphasis></glossterm>

                      <glossdef>
                        <para>This object from the <literal>numarray</literal>
                        package is also accepted, but it does not give you the
                        possibility to create a nested table. Array data is
                        injected into the new table.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>NestedRecArray</literal>
                      instance</emphasis></glossterm>

                      <glossdef>
                        <para>Finally, if you want to have nested columns in
                        your table and you are using
                        <literal>numarray</literal>, you can use this
                        object. Array data is injected into the new
                        table.</para>

                        <!-- manual-only -->
                        <para>See <xref linkend="NestedRecArrayClassDescr"
                        xrefstyle="select: label" /> for a description of the
                        <literal>NestedRecArray</literal> class.
                        </para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>A description for this node (it sets the
                  <literal>TITLE</literal> HDF5 attribute on disk).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>An instance of the <literal>Filters</literal> class
                  (see <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />) that provides information
                  about the desired I/O filters to be applied during the life
                  of this object.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">expectedrows</emphasis></glossterm>

                <glossdef>
                  <para>A user estimate of the number of records that will be
                  in the table. If not provided, the default value is
                  appropriate for tables up to 10 MB in size (more or
                  less). If you plan to create a bigger table try providing a
                  guess; this will optimize the HDF5 B-Tree creation and
                  management process time and memory used. If you want to
                  specify your own chunk size for I/O purposes, see also the
                  <literal>chunkshape</literal> parameter below.</para>

                  <!-- manual-only -->
                  <para>See <xref linkend="expectedRowsOptim"
                  xrefstyle="select: label" /> for a discussion on the issue
                  of providing a number of expected rows.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation. Filters are applied to those
                  chunks of data. The rank of the
                  <literal>chunkshape</literal> for tables must be 1. If
                  <literal>None</literal>, a sensible value is calculated
                  (which is recommended).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">byteorder</emphasis></glossterm>

                <glossdef>
                  <para>The byteorder of data <emphasis>on disk</emphasis>,
                  specified as <literal>'little'</literal> or
                  <literal>'big'</literal>. If this is not specified, the
                  byteorder is that of the platform, unless you passed an
                  array as the <literal>description</literal>, in which case
                  its byteorder will be used.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">createparents</emphasis></glossterm>

                <glossdef>
                  <para>Whether to create the needed groups for the parent
                  path to exist (not done by default).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="createVLArrayDescr">
            <title>createVLArray(where, name, atom, title='', filters=None,
            expectedsizeinMB=1.0, chunkshape=None, byteorder=None,
            createparents=False)</title>

            <para>Create a new variable-length array with the given
            <literal>name</literal> in <literal>where</literal> location.  See
            the <literal>VLArray</literal> (in <xref
            linkend="VLArrayClassDescr" xrefstyle="select: label" />) class
            for more information on variable-length arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">atom</emphasis></glossterm>

                <glossdef>
                  <para>An <literal>Atom</literal> (see <xref
                  linkend="AtomClassDescr" xrefstyle="select: label" />)
                  instance representing the <emphasis>type</emphasis> and
                  <emphasis>shape</emphasis> of the atomic objects to be
                  saved.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">expectedsizeinMB</emphasis></glossterm>

                <glossdef>
                  <para>An user estimate about the size (in MB) in the final
                  <literal>VLArray</literal> node. If not provided, the
                  default value is 1 MB. If you plan to create either a much
                  smaller or a much bigger array try providing a guess; this
                  will optimize the HDF5 B-Tree creation and management
                  process time and the amount of memory used. If you want to
                  specify your own chunk size for I/O purposes, see also the
                  <literal>chunkshape</literal> parameter below.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation. Filters are applied to those
                  chunks of data. The dimensionality of
                  <literal>chunkshape</literal> must be 1. If
                  <literal>None</literal>, a sensible value is calculated
                  (which is recommended).</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="File.moveNode">
            <title>moveNode(where, newparent=None, newname=None, name=None,
            overwrite=False, createparents=False)</title>

            <para>Move the node specified by <literal>where</literal> and
            <literal>name</literal> to
            <literal>newparent/newname</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newparent</emphasis></glossterm>

                <glossdef>
                  <para>The destination group the node will be moved into (a
                  path name or a <literal>Group</literal> instance). If it is
                  not specified or <literal>None</literal>, the current parent
                  group is chosen as the new parent.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newname</emphasis></glossterm>

                <glossdef>
                  <para>The new name to be assigned to the node in its
                  destination (a string). If it is not specified or
                  <literal>None</literal>, the current name is chosen as the
                  new name.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>The other arguments work as in
            <literal>Node._f_move()</literal> (see <xref
            linkend="Node._f_move" />).</para>
          </section>

          <section id="File.removeNode" xreflabel="description">
            <title>removeNode(where, name=None, recursive=False)</title>

            <para>Remove the object node <emphasis>name</emphasis> under
            <emphasis>where</emphasis> location.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">recursive</emphasis></glossterm>

                <glossdef>
                  <para>If not supplied or false, the node will be removed
                  only if it has no children; if it does, a
                  <literal>NodeError</literal> will be raised. If supplied
                  with a true value, the node and all its descendants will be
                  completely removed.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.renameNode">
            <title>renameNode(where, newname, name=None)</title>

            <para>Change the name of the node specified by
            <literal>where</literal> and <literal>name</literal> to
            <literal>newname</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newname</emphasis></glossterm>

                <glossdef>
                  <para>The new name to be assigned to the node (a
                  string).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>
        </section>

        <section id="FileMethods_tree_traversal">
          <title><literal>File</literal> methods — tree traversal</title>

          <section id="File.getNode" xreflabel="description">
            <title>getNode(where, name=None, classname=None)</title>

            <para>Get the node under <literal>where</literal> with the given
            <literal>name</literal>.</para>

            <para><literal>where</literal> can be a <literal>Node</literal>
            instance (see <xref linkend="NodeClassDescr" xrefstyle="select:
            label" />) or a path string leading to a node. If no
            <literal>name</literal> is specified, that node is
            returned.</para>

            <para>If a <literal>name</literal> is specified, this must be a
            string with the name of a node under <literal>where</literal>.  In
            this case the <literal>where</literal> argument can only lead to a
            <literal>Group</literal> (see <xref linkend="GroupClassDescr"
            xrefstyle="select: label" />) instance (else a
            <literal>TypeError</literal> is raised). The node called
            <literal>name</literal> under the group <literal>where</literal>
            is returned.</para>

            <para>In both cases, if the node to be returned does not exist, a
            <literal>NoSuchNodeError</literal> is raised. Please note that
            hidden nodes are also considered.</para>

            <para>If the <literal>classname</literal> argument is specified,
            it must be the name of a class derived from
            <literal>Node</literal>. If the node is found but it is not an
            instance of that class, a <literal>NoSuchNodeError</literal> is
            also raised.</para>
          </section>

          <section id="File.isVisibleNode">
            <title>isVisibleNode(path)</title>

            <para>Is the node under <literal>path</literal> visible?</para>

            <para>If the node does not exist, a
            <literal>NoSuchNodeError</literal> is raised.</para>
          </section>



          <section id="File.iterNodes" xreflabel="description">
            <title>iterNodes(where, classname=None)</title>

            <para>Iterate over children nodes hanging from
            <literal>where</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where</emphasis></glossterm>

                <glossdef>
                  <para>This argument works as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">classname</emphasis></glossterm>

                <glossdef>
                  <para>If the name of a class derived from
                  <literal>Node</literal> (see <xref linkend="NodeClassDescr"
                  xrefstyle="select: label" />) is supplied, only instances of
                  that class (or subclasses of it) will be returned.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>The returned nodes are alphanumerically sorted by their
            name.  This is an iterator version of
            <literal>File.listNodes()</literal> (see <xref
            linkend="File.listNodes"/>).</para>
          </section>

          <section id="File.listNodes" xreflabel="description">
            <title>listNodes(where, classname=None)</title>

            <para>Return a <emphasis>list</emphasis> with children nodes
            hanging from <literal>where</literal>.</para>

            <para>This is a list-returning version of
            <literal>File.iterNodes()</literal> (see <xref
            linkend="File.iterNodes"/>).</para>
          </section>

          <section id="walkGroupsDescr" xreflabel="description">
            <title>walkGroups(where='/')</title>

            <para>Recursively iterate over groups (not leaves) hanging from
            <literal>where</literal>.</para>

            <para>The <literal>where</literal> group itself is listed first
            (preorder), then each of its child groups (following an
            alphanumerical order) is also traversed, following the same
            procedure.  If <literal>where</literal> is not supplied, the root
            group is used.</para>

            <para>The <literal>where</literal> argument can be a path string
            or a <literal>Group</literal> instance (see <xref
            linkend="GroupClassDescr" xrefstyle="select: label" />).</para>
          </section>

          <section id="File.walkNodes" xreflabel="description">
            <title>walkNodes(where="/", classname="")</title>

            <para>Recursively iterate over nodes hanging from
            <literal>where</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where</emphasis></glossterm>

                <glossdef>
                  <para>If supplied, the iteration starts from (and includes)
                  this group. It can be a path string or a
                  <literal>Group</literal> instance (see <xref
                  linkend="GroupClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">classname</emphasis></glossterm>

                <glossdef>
                  <para>If the name of a class derived from
                  <literal>Node</literal> (see <xref linkend="GroupClassDescr"
                  xrefstyle="select: label" />) is supplied, only instances of
                  that class (or subclasses of it) will be returned.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Example of use:</para>

            <screen>
# Recursively print all the nodes hanging from '/detector'.
print "Nodes hanging from group '/detector':"
for node in h5file.walkNodes('/detector', classname='EArray'):
    print node</screen>
          </section>

          <section id="File.__contains__">
            <title>__contains__(path)</title>

            <para>Is there a node with that <literal>path</literal>?</para>

            <para>Returns <literal>True</literal> if the file has a node with
            the given <literal>path</literal> (a string),
            <literal>False</literal> otherwise.</para>
          </section>

          <section id="File.__iter__">
            <title>__iter__()</title>

            <para>Recursively iterate over the nodes in the tree.</para>

            <para>This is equivalent to calling
            <literal>File.walkNodes()</literal> (see <xref
            linkend="File.walkNodes" />) with no arguments.</para>

            <para>Example of use:</para>

            <screen>
# Recursively list all the nodes in the object tree.
h5file = tables.openFile('vlarray1.h5')
print "All nodes in the object tree:"
for node in h5file:
    print node</screen>
          </section>
        </section>

        <section id="FileMethods_undo_redo_support">
          <title><literal>File</literal> methods — Undo/Redo support</title>

          <section id="File.disableUndo" xreflabel="description">
            <title>disableUndo()</title>

            <para>Disable the Undo/Redo mechanism.</para>

            <para>Disabling the Undo/Redo mechanism leaves the database in the
            current state and forgets past and future database states. This
            makes <literal>File.mark()</literal> (see <xref
            linkend="File.mark" />), <literal>File.undo()</literal> (see <xref
            linkend="File.undo" />), <literal>File.redo()</literal> (see <xref
            linkend="File.redo" />) and other methods fail with an
            <literal>UndoRedoError</literal>.</para>

            <para>Calling this method when the Undo/Redo mechanism is already
            disabled raises an <literal>UndoRedoError</literal>.</para>
          </section>

          <section id="File.enableUndo" xreflabel="description">
            <title>enableUndo(filters=Filters( complevel=1))</title>

            <para>Enable the Undo/Redo mechanism.</para>

            <para>This operation prepares the database for undoing and redoing
            modifications in the node hierarchy. This allows
            <literal>File.mark()</literal> (see <xref linkend="File.mark" />),
            <literal>File.undo()</literal> (see <xref linkend="File.undo" />),
            <literal>File.redo()</literal> (see <xref linkend="File.redo" />)
            and other methods to be called.</para>

            <para>The <literal>filters</literal> argument, when specified,
            must be an instance of class <literal>Filters</literal> (see <xref
            linkend="FiltersClassDescr" xrefstyle="select: label" />) and is
            meant for setting the compression values for the action log. The
            default is having compression enabled, as the gains in terms of
            space can be considerable. You may want to disable compression if
            you want maximum speed for Undo/Redo operations.</para>

            <para>Calling this method when the Undo/Redo mechanism is already
            enabled raises an <literal>UndoRedoError</literal>.</para>
          </section>

          <section id="File.getCurrentMark" xreflabel="description">
            <title>getCurrentMark()</title>

            <para>Get the identifier of the current mark.</para>

            <para>Returns the identifier of the current mark. This can be used
            to know the state of a database after an application crash, or to
            get the identifier of the initial implicit mark after a call to
            <literal>File.enableUndo()</literal> (see <xref
            linkend="File.enableUndo" />).</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.goto" xreflabel="description">
            <title>goto(mark)</title>

            <para>Go to a specific mark of the database.</para>

            <para>Returns the database to the state associated with the
            specified <literal>mark</literal>. Both the identifier of a mark
            and its name can be used.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.isUndoEnabled" xreflabel="description">
            <title>isUndoEnabled()</title>

            <para>Is the Undo/Redo mechanism enabled?</para>

            <para>Returns <literal>True</literal> if the Undo/Redo mechanism
            has been enabled for this file, <literal>False</literal>
            otherwise. Please note that this mechanism is persistent, so a
            newly opened PyTables file may already have Undo/Redo
            support enabled.</para>
          </section>

          <section id="File.mark" xreflabel="description">
            <title>mark(name=None)</title>

            <para>Mark the state of the database.</para>

            <para>Creates a mark for the current state of the database. A
            unique (and immutable) identifier for the mark is returned. An
            optional <literal>name</literal> (a string) can be assigned to the
            mark. Both the identifier of a mark and its name can be used in
            <literal>File.undo()</literal> (see <xref linkend="File.undo" />)
            and <literal>File.redo()</literal> (see <xref linkend="File.redo"
            />) operations. When the <literal>name</literal> has already been
            used for another mark, an <literal>UndoRedoError</literal> is
            raised.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.redo" xreflabel="description">
            <title>redo(mark=None)</title>

            <para>Go to a future state of the database.</para>

            <para>Returns the database to the state associated with the
            specified <literal>mark</literal>. Both the identifier of a mark
            and its name can be used. If the <literal>mark</literal> is
            omitted, the next created mark is used. If there are no future
            marks, or the specified <literal>mark</literal> is not newer than
            the current one, an <literal>UndoRedoError</literal> is
            raised.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.undo" xreflabel="description">
            <title>undo(mark=None)</title>

            <para>Go to a past state of the database.</para>

            <para>Returns the database to the state associated with the
            specified <literal>mark</literal>. Both the identifier of a mark
            and its name can be used. If the <literal>mark</literal> is
            omitted, the last created mark is used. If there are no past
            marks, or the specified <literal>mark</literal> is not older than
            the current one, an <literal>UndoRedoError</literal> is
            raised.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>
        </section>

        <section id="FileMethods_attribute_handling">
          <title><literal>File</literal> methods — atttribute handling</title>

          <section id="File.copyNodeAttrs">
            <title>copyNodeAttrs(where, dstnode, name=None)</title>

            <para>Copy PyTables attributes from one node to another.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">dstnode</emphasis></glossterm>

                <glossdef>
                  <para>The destination node where the attributes will be
                  copied to. It can be a path string or a
                  <literal>Node</literal> instance (see <xref
                  linkend="NodeClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.delNodeAttr" xreflabel="description">
            <title>delNodeAttr(where, attrname, name=None)</title>

            <para>Delete a PyTables attribute from the given node.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrname</emphasis></glossterm>

                <glossdef>
                  <para>The name of the attribute to delete.  If the named
                  attribute does not exist, an
                  <literal>AttributeError</literal> is raised.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.getNodeAttr" xreflabel="description">
            <title>getNodeAttr(where, attrname, name=None)</title>

            <para>Get a PyTables attribute from the given node.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrname</emphasis></glossterm>

                <glossdef>
                  <para>The name of the attribute to retrieve.  If the named
                  attribute does not exist, an
                  <literal>AttributeError</literal> is raised.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.setNodeAttr" xreflabel="description">
            <title>setNodeAttr(where, attrname, attrvalue, name=None)</title>

            <para>Set a PyTables attribute for the given node.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrname</emphasis></glossterm>

                <glossdef>
                  <para>The name of the attribute to set.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrvalue</emphasis></glossterm>

                <glossdef>
                  <para>The value of the attribute to set. Any kind of Python
                  object (like strings, ints, floats, lists, tuples, dicts,
                  small NumPy/Numeric/numarray objects...) can be stored as an
                  attribute. However, if necessary, <literal>cPickle</literal>
                  is automatically used so as to serialize objects that you
                  might want to save. See the <literal>AttributeSet</literal>
                  class (in <xref linkend="AttributeSetClassDescr"
                  xrefstyle="select: label" />) for details.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>If the node already has a large number of attributes, a
            <literal>PerformanceWarning</literal> is issued.</para>
          </section>
        </section>
      </section>

      <section id="NodeClassDescr">
        <title>The <literal>Node</literal> class</title>

        <para>Abstract base class for all PyTables nodes.</para>

        <para>This is the base class for <emphasis>all</emphasis> nodes in a
        PyTables hierarchy. It is an abstract class, i.e. it may not be
        directly instantiated; however, every node in the hierarchy is an
        instance of this class.</para>

        <para>A PyTables node is always hosted in a PyTables
        <emphasis>file</emphasis>, under a <emphasis>parent group</emphasis>,
        at a certain <emphasis>depth</emphasis> in the node hierarchy. A node
        knows its own <emphasis>name</emphasis> in the parent group and its
        own <emphasis>path name</emphasis> in the file. When using a
        translation map (see the <literal>File</literal> class in <xref
        linkend="FileClassDescr" xrefstyle="select: label" />), its
        <emphasis>HDF5 name</emphasis> might differ from its PyTables
        name.</para>

        <para>All the previous information is location-dependent, i.e. it may
        change when moving or renaming a node in the hierarchy. A node also
        has location-independent information, such as its <emphasis>HDF5
        object identifier</emphasis> and its <emphasis>attribute
        set</emphasis>.</para>

        <para>This class gathers the operations and attributes (both
        location-dependent and independent) which are common to all PyTables
        nodes, whatever their type is. Nonetheless, due to natural naming
        restrictions, the names of all of these members start with a reserved
        prefix (see the <literal>Group</literal> class in <xref
        linkend="GroupClassDescr" xrefstyle="select: label" />).</para>

        <para>Sub-classes with no children (i.e. <emphasis>leaf
        nodes</emphasis>) may define new methods, attributes and properties to
        avoid natural naming restrictions. For instance,
        <literal>_v_attrs</literal> may be shortened to
        <literal>attrs</literal> and <literal>_f_rename</literal> to
        <literal>rename</literal>. However, the original methods and
        attributes should still be available.</para>

        <section>
          <title><literal>Node</literal> instance variables — location
          dependent</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_depth</emphasis></glossterm>

              <glossdef>
                <para>The depth of this node in the tree (an non-negative
                integer value).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">_v_file</emphasis></glossterm>

              <glossdef>
                <para>The hosting <literal>File</literal> instance (see <xref
                linkend="FileClassDescr" xrefstyle="select: label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_hdf5name</emphasis></glossterm>

              <glossdef>
                <para>The name of this node in the hosting HDF5 file (a
                string).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_isopen</emphasis></glossterm>

              <glossdef>
                <para>Whether this node is open or not.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_name</emphasis></glossterm>

              <glossdef>
                <para>The name of this node in its parent group (a
                string).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_parent</emphasis></glossterm>

              <glossdef>
                <para>The parent <literal>Group</literal> instance (see <xref
                linkend="GroupClassDescr" xrefstyle="select: label"
                />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_pathname</emphasis></glossterm>

              <glossdef>
                <para>The path of this node in the tree (a string).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section id="NodeClassInstanceVariables">
          <title><literal>Node</literal> instance variables — location
          independent</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">_v_attrs</emphasis></glossterm>

              <glossdef>
                <para>The associated <literal>AttributeSet</literal> instance
                (see <xref linkend="AttributeSetClassDescr" xrefstyle="select:
                label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_objectID</emphasis></glossterm>

              <glossdef>
                <para>The identifier of this node in the hosting HDF5
                file.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Node</literal> instance variables — attribute
          shorthands</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">_v_title</emphasis></glossterm>

              <glossdef>
                <para>A description of this node. A shorthand for
                <literal>TITLE</literal> attribute.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Node</literal> methods — hierarchy manipulation</title>

          <section id="Node._f_close" xreflabel="description">
            <title>_f_close()</title>

            <para>Close this node in the tree.</para>

            <para>This releases all resources held by the node, so it should
            not be used again. On nodes with data, it may be flushed to
            disk.</para>

            <para>The closing operation is <emphasis>not</emphasis> recursive,
            i.e. closing a group does not close its children.</para>
          </section>

          <section id="Node._f_copy" xreflabel="description">
            <title>_f_copy(newparent=None, newname=None, overwrite=False,
            recursive=False, createparents=False, **kwargs)</title>

            <para>Copy this node and return the new node.</para>

            <para>Creates and returns a copy of the node, maybe in a different
            place in the hierarchy. <literal>newparent</literal> can be a
            <literal>Group</literal> object (see <xref
            linkend="GroupClassDescr" xrefstyle="select: label" />) or a
            pathname in string form. If it is not specified or
            <literal>None</literal>, the current parent group is chosen as the
            new parent.  <literal>newname</literal> must be a string with a
            new name. If it is not specified or <literal>None</literal>, the
            current name is chosen as the new name. If
            <literal>recursive</literal> copy is stated, all descendants are
            copied as well. If <literal>createparents</literal> is true, the
            needed groups for the given new parent group path to exist will be
            created.</para>

            <para>Copying a node across databases is supported but can not be
            undone. Copying a node over itself is not allowed, nor it is
            recursively copying a node into itself. These result in a
            <literal>NodeError</literal>. Copying over another existing node
            is similarly not allowed, unless the optional
            <literal>overwrite</literal> argument is true, in which case that
            node is recursively removed before copying.</para>

            <para>Additional keyword arguments may be passed to customize the
            copying process. For instance, title and filters may be changed,
            user attributes may be or may not be copied, data may be
            sub-sampled, stats may be collected, etc. See the documentation
            for the particular node type.</para>

            <para>Using only the first argument is equivalent to copying the
            node to a new location without changing its name. Using only the
            second argument is equivalent to making a copy of the node in the
            same group.</para>
          </section>

          <section id="Node._f_isVisible" xreflabel="description">
            <title>_f_isVisible()</title>

            <para>Is this node visible?</para>
          </section>

          <section id="Node._f_move" xreflabel="description">
            <title>_f_move(newparent=None, newname=None, overwrite=False,
            createparents=False)</title>

            <para>Move or rename this node.</para>

            <para>Moves a node into a new parent group, or changes the name of
            the node. <literal>newparent</literal> can be a
            <literal>Group</literal> object (see <xref
            linkend="GroupClassDescr" xrefstyle="select: label" />) or a
            pathname in string form. If it is not specified or
            <literal>None</literal>, the current parent group is chosen as the
            new parent.  <literal>newname</literal> must be a string with a
            new name. If it is not specified or <literal>None</literal>, the
            current name is chosen as the new name. If
            <literal>createparents</literal> is true, the needed groups for
            the given new parent group path to exist will be created.</para>

            <para>Moving a node across databases is not allowed, nor it is
            moving a node <emphasis>into</emphasis> itself. These result in a
            <literal>NodeError</literal>. However, moving a node
            <emphasis>over</emphasis> itself is allowed and simply does
            nothing. Moving over another existing node is similarly not
            allowed, unless the optional <literal>overwrite</literal> argument
            is true, in which case that node is recursively removed before
            moving.</para>

            <para>Usually, only the first argument will be used, effectively
            moving the node to a new location without changing its name.
            Using only the second argument is equivalent to renaming the node
            in place.</para>
          </section>

          <section id="Node._f_remove" xreflabel="description">
            <title>_f_remove(recursive=False)</title>

            <para>Remove this node from the hierarchy.</para>

            <para>If the node has children, recursive removal must be stated
            by giving <literal>recursive</literal> a true value; otherwise, a
            <literal>NodeError</literal> will be raised.</para>
          </section>

          <section id="Node._f_rename" xreflabel="description">
            <title>_f_rename(newname)</title>

            <para>Rename this node in place.</para>

            <para>Changes the name of a node to <emphasis>newname</emphasis>
            (a string).</para>
          </section>
        </section>

        <section>
          <title><literal>Node</literal> methods — attribute handling</title>

          <section id="Node._f_delAttr" xreflabel="description">
            <title>_f_delAttr(name)</title>

            <para>Delete a PyTables attribute from this node.</para>

            <para>If the named attribute does not exist, an
            <literal>AttributeError</literal> is raised.</para>
          </section>

          <section id="Node._f_getAttr" xreflabel="description">
            <title>_f_getAttr(name)</title>

            <para>Get a PyTables attribute from this node.</para>

            <para>If the named attribute does not exist, an
            <literal>AttributeError</literal> is raised.</para>
          </section>

          <section id="Node._f_setAttr" xreflabel="description">
            <title>_f_setAttr(name, value)</title>

            <para>Set a PyTables attribute for this node.</para>

            <para>If the node already has a large number of attributes, a
            <literal>PerformanceWarning</literal> is issued.</para>
          </section>
        </section>
      </section>

      <section id="GroupClassDescr">
        <title>The <literal>Group</literal> class</title>

        <para>Basic PyTables grouping structure.</para>

        <para>Instances of this class are grouping structures containing
        <emphasis>child</emphasis> instances of zero or more groups or leaves,
        together with supporting metadata. Each group has exactly one
        <emphasis>parent</emphasis> group.</para>

        <para>Working with groups and leaves is similar in many ways to
        working with directories and files, respectively, in a Unix
        filesystem. As with Unix directories and files, objects in the object
        tree are often described by giving their full (or absolute) path
        names. This full path can be specified either as a string (like in
        <literal>'/group1/group2'</literal>) or as a complete object path
        written in <emphasis>natural naming</emphasis> schema (like in
        <literal>file.root.group1.group2</literal>).
        <!-- manual-only -->
        See <xref linkend="ObjectTreeSection" xrefstyle="select: label"
        /> for more information on natural naming.</para>

        <para>A collateral effect of the <emphasis>natural naming</emphasis>
        schema is that the names of members in the <literal>Group</literal>
        class and its instances must be carefully chosen to avoid colliding
        with existing children node names.  For this reason and to avoid
        polluting the children namespace all members in a
        <literal>Group</literal> start with some reserved prefix, like
        <literal>_f_</literal> (for public methods), <literal>_g_</literal>
        (for private ones), <literal>_v_</literal> (for instance variables) or
        <literal>_c_</literal> (for class variables). Any attempt to create a
        new child node whose name starts with one of these prefixes will raise
        a <literal>ValueError</literal> exception.</para>

        <para>Another effect of natural naming is that children named after
        Python keywords or having names not valid as Python identifiers (e.g.
        <literal>class</literal>, <literal>$a</literal> or
        <literal>44</literal>) can not be accessed using the
        <literal>node.child</literal> syntax. You will be forced to use
        <literal>node._f_getChild(child)</literal> to access them (which is
        recommended for programmatic accesses). You can also make use of the
        <literal>trMap</literal> (translation map dictionary) parameter in the
        <literal>openFile()</literal> function (see <xref
        linkend="openFileDescr" />) in order to translate HDF5 names not
        suited for natural naming into more convenient ones, so that you can
        go on using <literal>file.root.group1.group2</literal> syntax or
        <literal>getattr()</literal>.</para>

        <para>You will also need to use <literal>_f_getChild()</literal> to
        access an existing child node if you set a Python attribute in the
        <literal>Group</literal> with the same name as that node (you will get
        a <literal>NaturalNameWarning</literal> when doing this).</para>

        <section>
          <title><literal>Group</literal> instance variables</title>

          <para>The following instance variables are provided in addition to
          those in <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />):</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_nchildren</emphasis></glossterm>

              <glossdef>
                <para>The number of children hanging from this group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_filters</emphasis></glossterm>

              <glossdef>
                <para>Default filter properties for child nodes.</para>

                <para>You can (and are encouraged to) use this property to
                get, set and delete the <literal>FILTERS</literal> HDF5
                attribute of the group, which stores a
                <literal>Filters</literal> instance (see <xref
                linkend="FiltersClassDescr" xrefstyle="select:label" />). When
                the group has no such attribute, a default
                <literal>Filters</literal> instance is used.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_groups</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all groups hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_leaves</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all hidden nodes hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_leaves</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all leaves hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_children</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all nodes hanging from this
                group.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Group</literal> methods</title>

          <para><emphasis>Caveat: </emphasis>The following methods are
          documented for completeness, and they can be used without any
          problem. However, you should use the high-level counterpart methods
          in the <literal>File</literal> class (see <xref
          linkend="FileClassDescr" xrefstyle="select: label" />, because they
          are most used in documentation and examples, and are a bit more
          powerful than those exposed here.</para>

          <para>The following methods are provided in addition to those in
          <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />):</para>

          <section id="Group._f_close">
            <title>_f_close()</title>

            <para>Close this node in the tree.</para>

            <para>This method has the behavior described in
            <literal>Node._f_close()</literal> (see <xref
            linkend="Node._f_close" />). It should be noted that this
            operation disables access to nodes descending from this group.
            Therefore, if you want to explicitly close them, you will need to
            walk the nodes hanging from this group <emphasis>before</emphasis>
            closing it.</para>
          </section>

          <section id="Group._f_copy" xreflabel="description">
            <title>_f_copy(newparent, newname, overwrite=False,
            recursive=False, createparents=False, **kwargs)</title>

            <para>Copy this node and return the new one.</para>

            <para>This method has the behavior described in
            <literal>Node._f_copy()</literal> (see <xref
            linkend="Node._f_copy" />). In addition, it recognizes the
            following keyword arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>The new title for the destination. If omitted or
                  <literal>None</literal>, the original title is used. This
                  only applies to the topmost node in recursive copies.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>Specifying this parameter overrides the original
                  filter properties in the source node. If specified, it must
                  be an instance of the <literal>Filters</literal> class (see
                  <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />). The default is to copy the
                  filter properties from the source node.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">copyuserattrs</emphasis></glossterm>

                <glossdef>
                  <para>You can prevent the user attributes from being copied
                  by setting this parameter to <literal>False</literal>. The
                  default is to copy them.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stats</emphasis></glossterm>

                <glossdef>
                  <para>This argument may be used to collect statistics on the
                  copy process. When used, it should be a dictionary with keys
                  <literal>'groups'</literal>, <literal>'leaves'</literal> and
                  <literal>'bytes'</literal> having a numeric value. Their
                  values will be incremented to reflect the number of groups,
                  leaves and bytes, respectively, that have been copied during
                  the operation.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="Group._f_copyChildren" xreflabel="description">
            <title>_f_copyChildren(dstgroup, overwrite=False, recursive=False,
            createparents=False, **kwargs)</title>

            <para>Copy the children of this group into another group.</para>

            <para>Children hanging directly from this group are copied into
            <literal>dstgroup</literal>, which can be a
            <literal>Group</literal> (see <xref linkend="GroupClassDescr"
            xrefstyle="select: label" />) object or its pathname in string
            form. If <literal>createparents</literal> is true, the needed
            groups for the given destination group path to exist will be
            created.</para>

            <para>The operation will fail with a <literal>NodeError</literal>
            if there is a child node in the destination group with the same
            name as one of the copied children from this one, unless
            <literal>overwrite</literal> is true; in this case, the former
            child node is recursively removed before copying the later.</para>

            <para>By default, nodes descending from children groups of this
            node are not copied. If the <literal>recursive</literal> argument
            is true, all descendant nodes of this node are recursively
            copied.</para>

            <para>Additional keyword arguments may be passed to customize the
            copying process. For instance, title and filters may be changed,
            user attributes may be or may not be copied, data may be
            sub-sampled, stats may be collected, etc. Arguments unknown to
            nodes are simply ignored. Check the documentation for copying
            operations of nodes to see which options they support.</para>
          </section>

          <section id="Group._f_getChild" xreflabel="description">
            <title>_f_getChild(childname)</title>

            <para>Get the child called <literal>childname</literal> of this
            group.</para>

            <para>If the child exists (be it visible or not), it is returned.
            Else, a <literal>NoSuchNodeError</literal> is raised.</para>

            <para>Using this method is recommended over
            <literal>getattr()</literal> when doing programmatic accesses to
            children if the <literal>childname</literal> is unknown beforehand
            or when its name is not a valid Python identifier.</para>
          </section>

          <section id="Group._f_iterNodes" xreflabel="description">
            <title>_f_iterNodes(classname=None)</title>

            <para>Iterate over children nodes.</para>

            <para>Child nodes are yielded alphanumerically sorted by node
            name.  If the name of a class derived from <literal>Node</literal>
            (see <xref linkend="NodeClassDescr" xrefstyle="select: label" />)
            is supplied in the <literal>classname</literal> parameter, only
            instances of that class (or subclasses of it) will be
            returned.</para>

            <para>This is an iterator version of
            <literal>Group._f_listNodes()</literal> (see <xref
            linkend="Group._f_listNodes" />).</para>
          </section>

          <section id="Group._f_listNodes" xreflabel="description">
            <title>_f_listNodes(classname=None)</title>

            <para>Return a <emphasis>list</emphasis> with children
            nodes.</para>

            <para>This is a list-returning version of
            <literal>Group._f_iterNodes()</literal> (see <xref
            linkend="Group._f_iterNodes" />).</para>
          </section>

          <section id="Group._f_walkGroups">
            <title>_f_walkGroups()</title>

            <para>Recursively iterate over descendent groups (not
            leaves).</para>

            <para>This method starts by yielding <emphasis>self</emphasis>,
            and then it goes on to recursively iterate over all child groups
            in alphanumerical order, top to bottom (preorder), following the
            same procedure.</para>
          </section>

          <section id="Group._f_walkNodes" xreflabel="description">
            <title>_f_walkNodes(classname=None, recursive=True)</title>

            <para>Iterate over descendent nodes.</para>

            <para>This method recursively walks <emphasis>self</emphasis> top
            to bottom (preorder), iterating over child groups in
            alphanumerical order, and yielding nodes.  If
            <literal>classname</literal> is supplied, only instances of the
            named class are yielded.</para>

            <para>If <literal>recursive</literal> is false, it behaves like
            <literal>Group._f_iterNodes()</literal> (see <xref
            linkend="Group._f_iterNodes" />), yielding only children hanging
            immediately under the group.  If <literal>classname</literal> is
            <literal>'Group'</literal> and <literal>recursive</literal> is
            true, it behaves like <literal>Group._f_walkGroups()</literal>
            (see <xref linkend="Group._f_iterNodes" />), yielding only
            groups.</para>

            <para>Example of use:</para>

            <screen>
# Recursively print all the arrays hanging from '/'
print "Arrays in the object tree '/':"
for array in h5file.root._f_walkNodes('Array', recursive=True):
    print array</screen>
          </section>
        </section>

        <section>
          <title><literal>Group</literal> special methods</title>

          <para>Following are described the methods that automatically trigger
          actions when a <literal>Group</literal> instance is accessed in a
          special way.</para>

          <para>This class defines the <literal>__setattr__</literal>,
          <literal>__getattr__</literal> and <literal>__delattr__</literal>
          methods, and they set, get and delete <emphasis>ordinary Python
          attributes</emphasis> as normally intended. In addition to that,
          <literal>__getattr__</literal> allows getting <emphasis>child
          nodes</emphasis> by their name for the sake of easy interaction on
          the command line, as long as there is no Python attribute with the
          same name. Groups also allow the interactive completion (when using
          <literal>readline</literal>) of the names of child nodes. For
          instance:</para>

          <screen>
nchild = group._v_nchildren  # get a Python attribute

# Add a Table child called 'table' under 'group'.
h5file.createTable(group, 'table', myDescription)

table = group.table          # get the table child instance
group.table = 'foo'          # set a Python attribute
# (PyTables warns you here about using the name of a child node.)
foo = group.table            # get a Python attribute
del group.table              # delete a Python attribute
table = group.table          # get the table child instance again</screen>

          <section id="Group.__contains__">
            <title>__contains__(name)</title>

            <para>Is there a child with that <literal>name</literal>?</para>

            <para>Returns a true value if the group has a child node (visible
            or hidden) with the given <emphasis>name</emphasis> (a string),
            false otherwise.</para>
          </section>

          <section id="Group.__delattr__">
            <title>__delattr__(name)</title>

            <para>Delete a Python attribute called
            <literal>name</literal>.</para>

            <para>This method deletes an <emphasis>ordinary Python
            attribute</emphasis> from the object. It does
            <emphasis>not</emphasis> remove children nodes from this group;
            for that, use <literal>File.removeNode()</literal> (see <xref
            linkend="File.removeNode" />) or
            <literal>Node._f_remove()</literal> (see <xref
            linkend="Node._f_remove" />). It does <emphasis>neither</emphasis>
            delete a PyTables node attribute; for that, use
            <literal>File.delNodeAttr()</literal> (see <xref
            linkend="File.delNodeAttr" />),
            <literal>Node._f_delAttr()</literal> (see <xref
            linkend="Node._f_delAttr" />) or <literal>Node._v_attrs</literal>
            (see <xref linkend="NodeClassInstanceVariables" xrefstyle="select:
            label" />).</para>

            <para>If there is an attribute and a child node with the same
            <literal>name</literal>, the child node will be made accessible
            again via natural naming.</para>
          </section>

          <section id="Group.__getattr__">
            <title>__getattr__(name)</title>

            <para>Get a Python attribute or child node called
            <literal>name</literal>.</para>

            <para>If the object has a Python attribute called
            <literal>name</literal>, its value is returned. Else, if the node
            has a child node called <literal>name</literal>, it is returned.
            Else, an <literal>AttributeError</literal> is raised.</para>
          </section>

          <section id="Group.__iter__">
            <title>__iter__()</title>

            <para>Iterate over the child nodes hanging directly from the
            group.</para>

            <para>This iterator is <emphasis>not</emphasis> recursive.
            Example of use:</para>

            <screen>
# Non-recursively list all the nodes hanging from '/detector'
print "Nodes in '/detector' group:"
for node in h5file.root.detector:
    print node</screen>
          </section>

          <section id="Group.__repr__">
            <title>__repr__()</title>

            <para>Return a detailed string representation of the group.</para>

            <para>Example of use:</para>

            <screen>
>>> f = tables.openFile('data/test.h5')
>>> f.root.group0
/group0 (Group) 'First Group'
  children := ['tuple1' (Table), 'group1' (Group)]</screen>
          </section>

          <section id="Group.__setattr__">
            <title>__setattr__(name, value)</title>

            <para>Set a Python attribute called <literal>name</literal> with
            the given <literal>value</literal>.</para>

            <para>This method stores an <emphasis>ordinary Python
            attribute</emphasis> in the object. It does
            <emphasis>not</emphasis> store new children nodes under this
            group; for that, use the <literal>File.create*()</literal> methods
            (see the <literal>File</literal> class in <xref
            linkend="FileClassDescr" xrefstyle="select: label" />). It does
            <emphasis>neither</emphasis> store a PyTables node attribute; for
            that, use <literal>File.setNodeAttr()</literal> (see <xref
            linkend="File.setNodeAttr" />),
            <literal>Node._f_setAttr()</literal> (see <xref
            linkend="Node._f_setAttr" />) or <literal>Node._v_attrs</literal>
            (see <xref linkend="NodeClassInstanceVariables" xrefstyle="select:
            label" />).</para>

            <para>If there is already a child node with the same
            <literal>name</literal>, a <literal>NaturalNameWarning</literal>
            will be issued and the child node will not be accessible via
            natural naming nor <literal>getattr()</literal>. It will still be
            available via <literal>File.getNode()</literal> (see <xref
            linkend="File.getNode" />), <literal>Group._f_getChild()</literal>
            (see <xref linkend="Group._f_getChild" />) and children
            dictionaries in the group (if visible).</para>
          </section>

          <section id="Group.__str__">
            <title>__str__()</title>

            <para>Return a short string representation of the group.</para>

            <para>Example of use:</para>

            <screen>
>>> f=tables.openFile('data/test.h5')
>>> print f.root.group0
/group0 (Group) 'First Group'</screen>
          </section>
        </section>
      </section>

      <section id="LeafClassDescr">
        <title>The <literal>Leaf</literal> class</title>

        <para>Abstract base class for all PyTables leaves.</para>

        <para>A leaf is a node (see the <literal>Node</literal> class in <xref
        linkend="NodeClassDescr" xrefstyle="select: label" />) which hangs
        from a group (see the <literal>Group</literal> class in <xref
        linkend="GroupClassDescr" xrefstyle="select: label" />) but, unlike a
        group, it can not have any further children below it (i.e. it is an
        end node).</para>

        <para>This definition includes all nodes which contain actual data
        (datasets handled by the <literal>Table</literal> —see <xref
        linkend="TableClassDescr" xrefstyle="select: label" />—,
        <literal>Array</literal> —see <xref linkend="ArrayClassDescr"
        xrefstyle="select: label" />—, <literal>CArray</literal> —see <xref
        linkend="CArrayClassDescr" xrefstyle="select: label" />—,
        <literal>EArray</literal> —see <xref linkend="EArrayClassDescr"
        xrefstyle="select: label" />— and <literal>VLArray</literal> —see
        <xref linkend="VLArrayClassDescr" xrefstyle="select: label" />—
        classes) and unsupported nodes (the <literal>UnImplemented</literal>
        class —<xref linkend="UnImplementedClassDescr" xrefstyle="select:
        label" />) —these classes do in fact inherit from
        <literal>Leaf</literal>.</para>

        <section>
          <title><literal>Leaf</literal> instance variables</title>

          <para>These instance variables are provided in addition to those in
          <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />):</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">byteorder</emphasis></glossterm>

              <glossdef>
                <para>The byte ordering of the leaf data <emphasis>on
                disk</emphasis>.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">chunkshape</emphasis></glossterm>

              <glossdef>
                <para>The HDF5 chunk size for chunked leaves (a tuple).</para>

                <para>This is read-only because you cannot change the chunk
                size of a leaf once it has been created.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">filters</emphasis></glossterm>

              <glossdef>
                <para>Filter properties for this leaf —see
                <literal>Filters</literal> in <xref
                linkend="FiltersClassDescr" xrefstyle="select: label"
                />.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">flavor</emphasis></glossterm>

              <glossdef>
                <para>The type of data object read from this leaf.</para>

                <para>It can be any of <literal>'numpy'</literal>,
                <literal>'numarray'</literal>, <literal>'numeric'</literal> or
                <literal>'python'</literal> (the set of supported flavors
                depends on which packages you have installed on your
                system).</para>

                <para>You can (and are encouraged to) use this property to
                get, set and delete the <literal>FLAVOR</literal> HDF5
                attribute of the leaf. When the leaf has no such attribute,
                the default flavor is used.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">maindim</emphasis></glossterm>

              <glossdef>
                <para>The dimension along which iterators work.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrows</emphasis></glossterm>

              <glossdef>
                <para>The length of the main dimension of the leaf
                data.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">nrowsinbuf</emphasis></glossterm>

              <glossdef>
                <para>The number of rows that fit in internal input
                buffers.</para>

                <para>You can change this to fine-tune the speed or memory
                requirements of your application.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">shape</emphasis></glossterm>

              <glossdef>
                <para>The shape of data in the leaf.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Leaf</literal> instance variables — aliases</title>

          <para>The following are just easier-to-write aliases to their
          <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />) counterparts (indicated between
          parentheses):</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">attrs</emphasis></glossterm>

              <glossdef>
                <para>The associated <literal>AttributeSet</literal> instance
                —see <xref linkend="AttributeSetClassDescr" xrefstyle="select:
                label" />— (<literal>Node._v_attrs</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">hdf5name</emphasis></glossterm>

              <glossdef>
                <para>The name of this node in the hosting HDF5 file
                (<literal>Node._v_hdf5name</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">name</emphasis></glossterm>

              <glossdef>
                <para>The name of this node in its parent group
                (<literal>Node._v_name</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">objectID</emphasis></glossterm>

              <glossdef>
                <para>The identifier of this node in the hosting HDF5 file
                (<literal>Node._v_objectID</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">title</emphasis></glossterm>

              <glossdef>
                <para>A description for this node
                (<literal>Node._v_title</literal>).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Leaf</literal> methods</title>

          <section id="Leaf.close">
            <title>close(flush=True)</title>

            <para>Close this node in the tree.</para>

            <para>This method is completely equivalent to
            <literal>Leaf._f_close()</literal> (see <xref
            linkend="Leaf._f_close" />).</para>
          </section>

          <section id="Leaf.copy" xreflabel="description">
            <title>copy(newparent, newname, overwrite=False,
            createparents=False, **kwargs)</title>

            <para>Copy this node and return the new one.</para>

            <para>This method has the behavior described in
            <literal>Node._f_copy()</literal> (see <xref
            linkend="Node._f_copy" />). Please note that there is no
            <literal>recursive</literal> flag since leaves do not have child
            nodes. In addition, this method recognizes the following keyword
            arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>The new title for the destination. If omitted or
                  <literal>None</literal>, the original title is used.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>Specifying this parameter overrides the original
                  filter properties in the source node. If specified, it must
                  be an instance of the <literal>Filters</literal> class (see
                  <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />). The default is to copy the
                  filter properties from the source node.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">copyuserattrs</emphasis></glossterm>

                <glossdef>
                  <para>You can prevent the user attributes from being copied
                  by setting this parameter to <literal>False</literal>. The
                  default is to copy them.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">start, stop,
                step</emphasis></glossterm>

                <glossdef>
                  <para>Specify the range of rows to be copied; the default is
                  to copy all the rows.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stats</emphasis></glossterm>

                <glossdef>
                  <para>This argument may be used to collect statistics on the
                  copy process. When used, it should be a dictionary with keys
                  <literal>'groups'</literal>, <literal>'leaves'</literal> and
                  <literal>'bytes'</literal> having a numeric value. Their
                  values will be incremented to reflect the number of groups,
                  leaves and bytes, respectively, that have been copied during
                  the operation.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="Leaf.delAttr">
            <title>delAttr(name)</title>

            <para>Delete a PyTables attribute from this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_delAttr()</literal> (see <xref
            linkend="Node._f_delAttr" />).</para>
          </section>

          <section id="Leaf.flush">
            <title>flush()</title>

            <para>Flush pending data to disk.</para>

            <para>Saves whatever remaining buffered data to disk. It also
            releases I/O buffers, so if you are filling many datasets in the
            same PyTables session, please call <literal>flush()</literal>
            extensively so as to help PyTables to keep memory requirements
            low.</para>
          </section>

          <section id="Leaf.getAttr">
            <title>getAttr(name)</title>

            <para>Get a PyTables attribute from this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_getAttr()</literal> (see <xref
            linkend="Node._f_getAttr" />).</para>
          </section>

          <section id="Leaf.isVisible">
            <title>isVisible()</title>

            <para>Is this node visible?</para>

            <para>This method has the behavior described in
            <literal>Node._f_isVisible()</literal> (see <xref
            linkend="Node._f_isVisible" />).</para>
          </section>

          <section id="Leaf.move">
            <title>move(newparent=None, newname=None, overwrite=False,
            createparents=False)</title>

            <para>Move or rename this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_move()</literal> (see <xref
            linkend="Node._f_move" />).</para>
          </section>

          <section id="Leaf.rename">
            <title>rename(newname)</title>

            <para>Rename this node in place.</para>

            <para>This method has the behavior described in
            <literal>Node._f_rename()</literal> (see <xref
            linkend="Node._f_rename" />).</para>
          </section>

          <section id="Leaf.remove">
            <title>remove()</title>

            <para>Remove this node from the hierarchy.</para>

            <para>This method has the behavior described in
            <literal>Node._f_remove()</literal> (see <xref
            linkend="Node._f_remove" />). Please note that there is no
            <literal>recursive</literal> flag since leaves do not have child
            nodes.</para>
          </section>

          <section id="Leaf.setAttr">
            <title>setAttr(name, value)</title>

            <para>Set a PyTables attribute for this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_setAttr()</literal> (see <xref
            linkend="Node._f_setAttr" />).</para>
          </section>

          <section id="Leaf.__len__">
            <title>__len__()</title>
            <para>Return the length of the main dimension of the leaf
            data.</para>
          </section>

          <section id="Leaf._f_close" xreflabel="description">
            <title>_f_close(flush=True)</title>

            <para>Close this node in the tree.</para>

            <para>This method has the behavior described in
            <literal>Node._f_close()</literal> (see <xref
            linkend="Node._f_close" />).  Besides that, the optional argument
            <literal>flush</literal> tells whether to flush pending data to
            disk or not before closing.</para>
          </section>
        </section>
      </section>

      <section id="TableClassDescr">
        <title>The <literal>Table</literal> class</title>

        <para>This class represents heterogeneous datasets in an HDF5
        file.</para>

        <para>Tables are leaves (see the <literal>Leaf</literal> class in
        <xref linkend="LeafClassDescr" xrefstyle="select: label" />) whose
        data consists of a unidimensional sequence of
        <emphasis>rows</emphasis>, where each row contains one or more
        <emphasis>fields</emphasis>.  Fields have an associated unique
        <emphasis>name</emphasis> and <emphasis>position</emphasis>, with the
        first field having position 0.  All rows have the same fields, which
        are arranged in <emphasis>columns</emphasis>.</para>

        <para>Fields can have any type supported by the <literal>Col</literal>
        class (see <xref linkend="ColClassDescr" xrefstyle="select: label" />)
        and its descendants, which support multidimensional data.  Moreover, a
        field can be <emphasis>nested</emphasis> (to an arbitrary depth),
        meaning that it includes further fields inside.  A field named
        <literal>x</literal> inside a nested field <literal>a</literal> in a
        table can be accessed as the field <literal>a/x</literal> (its
        <emphasis>path name</emphasis>) from the table.</para>

        <para>The structure of a table is declared by its description, which
        is made available in the <literal>Table.description</literal>
        attribute (see <xref linkend="TableInstanceVariablesDescr"
        xrefstyle="select: label" />).</para>

        <para>This class provides new methods to read, write and search table
        data efficiently.  It also provides special Python methods to allow
        accessing the table as a normal sequence or array (with extended
        slicing supported).</para>

        <para>PyTables supports <emphasis>in-kernel</emphasis> searches
        working simultaneously on several columns using complex conditions.
        These are faster than selections using Python expressions.  See the
        <literal>Tables.where()</literal> method —<xref linkend="Table.where"
        />— for more information on in-kernel searches.
        <!-- manual-only -->
        See also <xref linkend="inkernelSearch" xrefstyle="select: label" />
        for a detailed review of the advantages and shortcomings of in-kernel
        searches.
        </para>

        <para>Non-nested columns can be <emphasis>indexed</emphasis>.
        Searching an indexed column can be several times faster than searching
        a non-nested one.  Search methods automatically take advantage of
        indexing where available.</para>

        <note>
          <!-- XXXPRO -->
          <para>Column indexing is only available in PyTables Pro.</para>
        </note>

        <para>When iterating a table, an object from the
        <literal>Row</literal> (see <xref linkend="RowClassDescr"
        xrefstyle="select: label" />) class is used.  This object allows to
        read and write data one row at a time, as well as to perform queries
        which are not supported by in-kernel syntax (at a much lower speed, of
        course).  You can get new row iterators whenever you want by accessing
        the <literal>Table.row</literal> property (see <xref
        linkend="TableInstanceVariablesDescr" xrefstyle="select: label"
        />).
        <!-- manual-only -->
        See the tutorial sections in <xref linkend="usage" xrefstyle="select:
        label" /> on how to use the <literal>Row</literal> interface.</para>

        <para>Objects of this class support access to individual columns via
        <emphasis>natural naming</emphasis> through the
        <literal>Table.cols</literal> accessor (see <xref
        linkend="TableInstanceVariablesDescr" xrefstyle="select: label" />).
        Nested columns are mapped to <literal>Cols</literal> instances, and
        non-nested ones to <literal>Column</literal> instances.  See the
        <literal>Column</literal> class in <xref linkend="ColumnClassDescr"
        xrefstyle="select: label" /> for examples of this feature.</para>

        <section id="TableInstanceVariablesDescr">
          <title><literal>Table</literal> instance variables</title>

          <para>The following instance variables are provided in addition to
          those in <literal>Leaf</literal> (see <xref linkend="LeafClassDescr"
          xrefstyle="select: label" />).  Please note that there are several
          <literal>col*</literal> dictionaries to ease retrieving information
          about a column directly by its path name, avoiding the need to walk
          through <literal>Table.description</literal> or
          <literal>Table.cols</literal>.</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">autoIndex</emphasis></glossterm>

              <glossdef>
                <para>Automatically keep column indexes up to date?</para>

                <para>Setting this value states whether existing indexes
                should be automatically updated after an append operation or
                recomputed after an index-invalidating operation (i.e. removal
                and modification of rows). The default is true.</para>

                <para>This value gets into effect whenever a column is
                altered. For an immediate update use
                <literal>Table.flushRowsToIndex()</literal> (see <xref
                linkend="Table.flushRowsToIndex"
                xrefstyle="select: label" />); for immediate reindexing of
                invalidated indexes, use
                <literal>Table.reIndexDirty()</literal> (see <xref
                linkend="Table.reIndexDirty"
                xrefstyle="select: label" />).</para>

                <para>This value is persistent.
                <note>
                  <!-- XXXPRO -->
                  <para>Column indexing is only available in PyTables
                  Pro.</para>
                </note>
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">coldescrs</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its <literal>Col</literal>
                description (see <xref linkend="ColClassDescr"
                xrefstyle="select: label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">coldflts</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its default value.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">coldtypes</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its NumPy data type.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colindexed</emphasis></glossterm>

              <glossdef>
                <para>Is the column which name is used as a key indexed?
                <note>
                  <!-- XXXPRO -->
                  <para>Column indexing is only available in PyTables
                  Pro.</para>
                </note>
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colinstances</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its
                <literal>Column</literal> (see <xref
                linkend="ColumnClassDescr" xrefstyle="select: label" />) or
                <literal>Cols</literal> (see <xref linkend="ColsClassDescr"
                xrefstyle="select: label" />) instance.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">colnames</emphasis></glossterm>

              <glossdef>
                <para>A list containing the names of
                <emphasis>top-level</emphasis> columns in the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colpathnames</emphasis></glossterm>

              <glossdef>
                <para>A list containing the pathnames of
                <emphasis>bottom-level</emphasis> columns in the table.</para>

                <para>These are the leaf columns obtained when walking the
                table description left-to-right, bottom-first. Columns inside
                a nested column have slashes (<literal>/</literal>) separating
                name components in their pathname.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">cols</emphasis></glossterm>

              <glossdef>
                <para>A <literal>Cols</literal> instance that provides
                <emphasis>natural naming</emphasis> access to non-nested
                (<literal>Column</literal>, see <xref
                linkend="ColumnClassDescr" xrefstyle="select: label" />) and
                nested (<literal>Cols</literal>, see <xref
                linkend="ColsClassDescr" xrefstyle="select: label" />)
                columns.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">coltypes</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its PyTables
                data type.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">description</emphasis></glossterm>

              <glossdef>
                <para>A <literal>Description</literal> instance (see <xref
                linkend="DescriptionClassDescr" xrefstyle="select: label" />)
                reflecting the structure of the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">extdim</emphasis></glossterm>

              <glossdef>
                <para>The index of the enlargeable dimension (always 0 for
                tables)</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">indexed</emphasis></glossterm>

              <glossdef>
                <para>Does this table have any indexed columns?
                <note>
                  <!-- XXXPRO -->
                  <para>Column indexing is only available in PyTables
                  Pro.</para>
                </note>
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">indexedcolpathnames</emphasis></glossterm>

              <glossdef>
                <para>List of the pathnames of indexed columns in the
                table.
                <note>
                  <!-- XXXPRO -->
                  <para>Column indexing is only available in PyTables
                  Pro.</para>
                </note>
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">indexFilters</emphasis></glossterm>

              <glossdef>
                <para>Filters used to compress indexes.</para>

                <para>Setting this value to a <literal>Filters</literal> (see
                <xref linkend="FiltersClassDescr"
                xrefstyle="select: label" />) instance determines the
                compression to be used for indexes. Setting it to
                <literal>None</literal> means that no filters will be used for
                indexes. The default is zlib compression level 1 with
                shuffling.</para>

                <para>This value is used when creating new indexes or
                recomputing old ones. To apply it to existing indexes, use
                <literal>Table.reIndex()</literal> (see <xref
                linkend="Table.reIndex" xrefstyle="select: label" />).</para>

                <para>This value is persistent.
                <note>
                  <!-- XXXPRO -->
                  <para>Column indexing is only available in PyTables
                  Pro.</para>
                </note>
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrows</emphasis></glossterm>

              <glossdef>
                <para>The current number of rows in the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">row</emphasis></glossterm>

              <glossdef>
                <para>A new <literal>Row</literal> instance (see <xref
                linkend="RowClassDescr" xrefstyle="select: label" />) for
                iterating over the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">rowsize</emphasis></glossterm>

              <glossdef>
                <para>The size in bytes of each row in the table.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <!-- XXX REVIEWED UP TO HERE - Ivan -->
        <section id="Table_methods">
          <title><literal>Table</literal> methods</title>

          <section id="Table.getEnum">
            <title>getEnum(colname)</title>

            <para>Get the enumerated type associated with the named
            column.</para>

            <para>If the column named <literal>colname</literal> (a string)
            exists and is of an enumerated type, the corresponding
            <literal>Enum</literal> instance (see <xref
            linkend="EnumClassDescr" xrefstyle="select: label" />) is
            returned. If it is not of an enumerated type, a
            <literal>TypeError</literal> is raised. If the column does not
            exist, a <literal>KeyError</literal> is raised.</para>
          </section>

          <section id="Table.append">
            <title>append(rows)</title>

            <para>Append a series of rows to this <literal>Table</literal>
            instance. <emphasis>rows</emphasis> is an object that can keep the
            rows to be append in several formats, like a
            <literal>NestedRecArray</literal> (see <xref
            linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />),
            a <literal>RecArray</literal>, a <literal>NumPy</literal> object,
            a list of tuples, list of
            NumPy/<literal>Numeric</literal>/<literal>numarray</literal>
            objects, string, Python buffer or <literal>None</literal> (no
            append will result). Of course, this <emphasis>rows</emphasis>
            object has to be compliant with the underlying format of the
            <literal>Table</literal> instance or a
            <literal>TypeError</literal> will be issued.</para>

            <para>Example of use: <screen>from tables import *
class Particle(IsDescription):
    name        = StringCol(16, pos=1) # 16-character String
    lati        = IntCol(pos=2)        # integer
    longi       = IntCol(pos=3)        # integer
    pressure    = Float32Col(pos=4)    # float  (single-precision)
    temperature = FloatCol(pos=5)      # double (double-precision)

fileh = openFile("test4.h5", mode = "w")
table = fileh.createTable(fileh.root, 'table', Particle, "A table")
# Append several rows in only one call
table.append([("Particle:     10", 10, 0, 10*10, 10**2),
              ("Particle:     11", 11, -1, 11*11, 11**2),
              ("Particle:     12", 12, -2, 12*12, 12**2)])
fileh.close()</screen></para>
          </section>

          <section id="Table.col" xreflabel="description">
            <title>col(name)</title>

            <para>Get a column from the table.</para>

            <para>If a column called <literal>name</literal> exists in the
            table, it is read and returned as a NumPy object, or as a
            <literal>numarray</literal> object (whatever is more appropriate
            depending on the flavor of the table). If it does not exist, a
            <literal>KeyError</literal> is raised.</para>

            <para>Example of use:</para>

            <screen>narray = table.col('var2')</screen>

            <para>That statement is equivalent to:</para>

            <screen>narray = table.read(field='var2')</screen>

            <para>Here you can see how this method can be used as a shorthand
            for <literal>the read()</literal> (see <xref
            linkend="Table.read" />) method.</para>
          </section>

          <section id="Table.iterrows" xreflabel="description">
            <title>iterrows(start=None, stop=None, step=1)</title>

            <para>Returns an iterator yielding <literal>Row</literal> (see
            <xref linkend="RowClassDescr" xrefstyle="select: label" />)
            instances built from rows in table. If a range is supplied (i.e.
            some of the <emphasis>start</emphasis>, <emphasis>stop</emphasis>
            or <emphasis>step</emphasis> parameters are passed), only the
            appropriate rows are returned. Else, all the rows are returned.
            See also the <literal>__iter__()</literal> special method in <xref
            linkend="TableSpecialMethods" xrefstyle="select: label" /> for a
            shorter way to call this iterator.</para>

            <para>The meaning of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> and <emphasis>step</emphasis> parameters
            is the same as in the <literal>range()</literal> python function,
            except that negative values of <literal>step</literal> are not
            allowed. Moreover, if only <literal>start</literal> is specified,
            then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <emphasis>start</emphasis> nor <emphasis>stop</emphasis>, then
            <emphasis>all the rows</emphasis> in the object are
            selected.</para>

            <para>Example of use:</para>

            <screen>result = [ row['var2'] for row in table.iterrows(step=5)
if row['var1'] &lt;= 20 ]</screen>

            <para><emphasis>Note:</emphasis> This iterator can be nested (see
            example in <xref linkend="Table.where" />).</para>
          </section>

          <section id="itersequenceDescr">
            <title>itersequence(sequence, sort=True)</title>

            <para>Iterate over a <emphasis>sequence</emphasis> of row
            coordinates.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">sequence</emphasis></glossterm>

                <glossdef>
                  <para>Can be any object that supports the
                  <literal>__getitem__</literal> special method, like lists,
                  tuples, NumPy/Numeric/numarray objects, etc.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">sort</emphasis></glossterm>

                <glossdef>
                  <para>If true, means that <emphasis>sequence</emphasis> will
                  be sorted out so that the I/O process would get better
                  performance. If your sequence is already sorted or you don't
                  want to sort it, put this parameter to 0. The default is to
                  sort the <emphasis>sequence</emphasis>.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para><emphasis>Note:</emphasis> This iterator can be nested (see
            example in <xref linkend="Table.where" />).</para>
          </section>

          <section id="Table.read" xreflabel="description">
            <title>read(start=None, stop=None, step=1, field=None)</title>

            <para>Returns the actual data in <literal>Table</literal>. If
            <emphasis>field</emphasis> is not supplied, it returns the data as
            a NumPy record array.</para>

            <para>The meaning of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> and <emphasis>step</emphasis> parameters
            is the same as in the <literal>range()</literal> python function,
            except that negative values of <emphasis>step</emphasis> are not
            allowed. Moreover, if only <emphasis>start</emphasis> is
            specified, then <emphasis>stop</emphasis> will be set to
            <emphasis>start+1</emphasis>. If you do not specify neither
            <emphasis>start</emphasis> nor <emphasis>stop</emphasis>, then all
            the rows in the object are selected.</para>

            <para>The rest of the parameters are described next:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">field</emphasis></glossterm>

                <glossdef>
                  <para>If specified, only the column
                  <emphasis>field</emphasis> is returned as an homogeneous
                  NumPy/<literal>numarray</literal>/<literal>Numeric</literal>
                  or Python object, depending on the current
                  <emphasis>flavor</emphasis>. If this is not supplied, all
                  the fields are selected and a
                  <literal>NestedRecArray</literal> (see <xref
                  linkend="NestedRecArrayClassDescr"
                  xrefstyle="select: label" />) or <literal>NumPy</literal>
                  object is returned. Nested fields can be specified in the
                  <emphasis>field</emphasis> parameter by using a
                  <literal>'/'</literal> character as a separator between
                  fields (e.g. <literal>Info/value</literal>).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="Table.readWhere" xreflabel="description">
            <title>readWhere(condition, condvars=None, field=None)</title>

            <para>Read table data fulfilling the given
            <emphasis>condition</emphasis>.</para>

            <para>This method is similar to <literal>read()</literal> (see
            <xref linkend="Table.read" />), having their common arguments and
            return values the same meanings. However, only the rows fulfilling
            the <emphasis>condition</emphasis> are included in the
            result.</para>

            <para>The meaning of the <emphasis>condition</emphasis> and
            <emphasis>condvars</emphasis> arguments is the same as in the
            <literal>where()</literal> (see <xref linkend="Table.where" />)
            method.</para>
          </section>

          <section id="Table.readCoordinates">
            <title>readCoordinates(coords, field=None)</title>

            <para>Read a set of rows given their indexes into an in-memory
            object.</para>

            <para>This method works much like the <literal>read()</literal>
            method (see <xref linkend="Table.read" />), but it uses a sequence
            (<literal>coords</literal>) of row indexes to select the wanted
            columns, instead of a column range.</para>

            <para>It returns the selected rows in a record array of the
            current flavor.</para>
          </section>

          <section id="Table.modifyRows" xreflabel="description">
            <title>modifyRows(start=None, stop=None, step=1,
            rows=None)</title>

            <para>Modify a series of rows in the
            <literal>[start:stop:step]</literal> <emphasis>extended
            slice</emphasis> range. If you pass <literal>None</literal> to
            <emphasis>stop</emphasis>, all the rows existing in
            <emphasis>rows</emphasis> will be used.</para>

            <para><emphasis>rows</emphasis> can be either a
            <emphasis>recarray</emphasis> or a structure that is able to be
            converted to any of them and compliant with the table
            format.</para>

            <para>Returns the number of modified rows.</para>

            <para>It raises an <literal>TypeError</literal> in case the rows
            parameter could not be converted to an object compliant with table
            description.</para>

            <para>It raises an <literal>IndexError</literal> in case the
            modification will exceed the length of the table.</para>
          </section>

          <section id="Table.modifyColumn" xreflabel="description">
            <title>modifyColumn(start=None, stop=None, step=1, column=None,
            colname=None)</title>

            <para>Modify one single column in the row slice
            <literal>[start:stop:step]</literal>.</para>

            <para><emphasis>column</emphasis> can be either a NumPy record
            array, <literal>NestedRecArray</literal> (see <xref
            linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />),
            <literal>RecArray</literal>, NumPy array,
            <literal>NumArray</literal>, list or tuple that is able to be
            converted into a record array compliant with the specified
            <emphasis>colname</emphasis> column of the table.</para>

            <para><emphasis>colname</emphasis> specifies the column name of
            the table to be modified.</para>

            <para>Returns the number of modified rows.</para>

            <para>It raises an <literal>ValueError</literal> in case the
            <emphasis>column</emphasis> parameter could not be converted into
            an object compliant with <literal>column</literal>
            description.</para>

            <para>It raises an <literal>IndexError</literal> in case the
            modification will exceed the length of the table.</para>
          </section>

          <section id="Table.modifyColumns">
            <title>modifyColumns(start=None, stop=None, step=1, columns=None,
            names=None)</title>

            <para>Modify a series of columns in the row slice
            <literal>[start:stop:step]</literal>.</para>

            <para><emphasis>columns</emphasis> can be either a Numpy record
            array, <literal>NestedRecArray</literal> (see <xref
            linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />),
            <literal>RecArray</literal>, list of arrays or list or tuples (the
            columns) that are able to be converted into a record array
            compliant with the format of the specified column
            <emphasis>names</emphasis> in the table.</para>

            <para><emphasis>names</emphasis> specifies the column names of the
            table to be modified.</para>

            <para>Returns the number of modified rows.</para>

            <para>It raises an <literal>TypeError</literal> in case the
            <emphasis>columns</emphasis> parameter could not be converted to
            an object compliant with table description.</para>

            <para>It raises an <literal>IndexError</literal> in case the
            modification will exceed the length of the table.</para>
          </section>

          <section id="removeRowsDescr" xreflabel="description">
            <title>removeRows(start, stop=None)</title>

            <para>Removes a range of rows in the table. If only
            <emphasis>start</emphasis> is supplied, this row is to be deleted.
            If a range is supplied, i.e. both the <emphasis>start</emphasis>
            and <emphasis>stop</emphasis> parameters are passed, all the rows
            in the range are removed. A <emphasis>step</emphasis> parameter is
            not supported, and it is not foreseen to implement it anytime
            soon.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">start</emphasis></glossterm>

                <glossdef>
                  <para>Sets the starting row to be removed. It accepts
                  negative values meaning that the count starts from the end.
                  A value of 0 means the first row.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stop</emphasis></glossterm>

                <glossdef>
                  <para>Sets the last row to be removed to
                  <emphasis>stop</emphasis> - 1, i.e. the end point is omitted
                  (in the Python <literal>range</literal> tradition). It
                  accepts, likewise <emphasis>start</emphasis>, negative
                  values. A special value of <literal>None</literal> (the
                  default) means removing just the row supplied in
                  start.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="Table.flushRowsToIndex">
            <title>flushRowsToIndex()</title>

            <para>Add remaining rows in buffers to non-dirty indexes. This can
            be useful when you have chosen non-automatic indexing for the
            table (see the <literal>Table.autoIndex</literal> property in
            <xref linkend="TableInstanceVariablesDescr" xrefstyle="select:
            label" />) and want to update the indexes on it.</para>
          </section>

          <section id="Table.reIndex">
            <title>reIndex()</title>

            <para>Recompute all the existing indexes in table. This can be
            useful when you suspect that, for any reason, the index
            information for columns is no longer valid and want to rebuild the
            indexes on it.</para>
          </section>

          <section id="Table.reIndexDirty">
            <title>reIndexDirty()</title>

            <para>Recompute the existing indexes in table, but
            <emphasis>only</emphasis> if they are dirty. This can be useful
            when you have set <literal>Table.autoIndex</literal> (see <xref
            linkend="TableInstanceVariablesDescr" xrefstyle="select: label"
            />) to false for the table and want to update the indexes after a
            invalidating index operation (<literal>Table.removeRows</literal>,
            for example).</para>
          </section>

          <section id="Table.willQueryUseIndexing" xreflabel="description">
            <title>willQueryUseIndexing(condition, condvars=None)</title>

            <para>Will a query for the <emphasis>condition</emphasis> use
            indexing?</para>

            <para>The meaning of the <emphasis>condition</emphasis> and
            <emphasis>condvars</emphasis> arguments is the same as in the
            <literal>where()&gt;</literal> method (see <xref
            linkend="Table.where" />). If the <emphasis>condition</emphasis>
            can use indexing, this method returns the path name of the column
            whose index is usable. Otherwise, it returns
            <literal>None</literal>.</para>

            <para>This method is mainly intended for testing. Keep in mind
            that changing the set of indexed columns or their dirtyness may
            make this method return different values for the same arguments at
            different times.</para>
          </section>

          <section id="Table.where" xreflabel="description">
            <title>where(condition, condvars=None, start=None, stop=None,
            step=None)</title>

            <para>Iterate over values fulfilling a
            <emphasis>condition</emphasis>.</para>

            <para>This method returns an iterator yielding
            <literal>Row</literal> (see <xref linkend="RowClassDescr"
            xrefstyle="select: label" />) instances built from rows in the
            table that satisfy the given <emphasis>condition</emphasis> (an
            expression-like string). For more information on condition syntax,
            see <xref linkend="conditionSyntax" xrefstyle="select: label"
            />.</para>

            <para>The <emphasis>condvars</emphasis> mapping may be used to
            define the variable names appearing in the
            <emphasis>condition</emphasis>. <emphasis>condvars</emphasis>
            should consist of identifier-like strings pointing to
            <literal>Column</literal> instances <emphasis>of this
            table</emphasis>, or to other values (which will be converted to
            arrays). A default set of condition variables is provided where
            each top-level, non-nested column with an identifier-like name
            appears. Variables in <emphasis>condvars</emphasis> override the
            default ones.</para>

            <para>When <emphasis>condvars</emphasis> is not provided or
            <literal>None</literal>, the current local and global namespace is
            sought instead of <emphasis>condvars</emphasis>. The previous
            mechanism is mostly intended for interactive usage. To disable it,
            just specify a (maybe empty) mapping as
            <emphasis>condvars</emphasis>.</para>

            <para>If a range is supplied (by setting some of the
            <emphasis>start</emphasis>, <emphasis>stop</emphasis> or
            <emphasis>step</emphasis> parameters), only the rows in that range
            <emphasis>and</emphasis> fulfilling the
            <emphasis>condition</emphasis> are used. The meaning of the
            <emphasis>start</emphasis>, <emphasis>stop</emphasis> and
            <emphasis>step</emphasis> parameters is the same as in the
            <literal>range()</literal> Python function, except that negative
            values of <emphasis>step</emphasis> are <emphasis>not</emphasis>
            allowed. Moreover, if only <emphasis>start</emphasis> is
            specified, then <emphasis>stop</emphasis> will be set to
            <literal>start+1</literal>.</para>

            <para>When possible, indexed columns participating in the
            condition will be used to speed up the search. It is recommended
            that you place the indexed columns as left and out in the
            condition as possible. Anyway, this method has always better
            performance than standard Python selections on the table. Please
            check the <xref linkend="searchOptim" xrefstyle="select: label" />
            for more information about the performance of the different
            searching modes.</para>

            <para>You can mix this method with standard Python selections in
            order to have complex queries. It is strongly recommended that you
            pass the most restrictive condition as the parameter to this
            method if you want to achieve maximum performance.</para>

            <para>Example of use:</para>

            <screen>&gt;&gt;&gt; passvalues = [ row['col3'] for row in
...                table.where('(col1 &gt; 0) &amp; (col2 &lt;= 20)', step=5)
...                if your_function(row['col2']) ]
&gt;&gt;&gt; print "Values that pass the cuts:", passvalues</screen>

            <para>Note that, from PyTables 1.1 on, you can nest several
            iterators over the same table. For example:</para>

            <screen>for p in rout.where('pressure &lt; 16'):
    for q in rout.where('pressure &lt; 9'):
        for n in rout.where('energy &lt; 10'):
            print "pressure, energy:", p['pressure'], n['energy']</screen>

            <para>In this example, iterators returned by
            <literal>where()</literal> have been used, but you may as well use
            any of the other reading iterators that <literal>Table</literal>
            objects offer. See <literal>examples/nested-iter.py</literal> for
            the full code.</para>
          </section>

          <section id="Table.whereAppend">
            <title>whereAppend(dstTable, condition, condvars=None, start=None,
            stop=None, step=None)</title>

            <para>Append rows fulfilling the <emphasis>condition</emphasis> to
            the <emphasis>dstTable</emphasis> table.</para>

            <para><emphasis>dstTable</emphasis> must be capable of taking the
            rows resulting from the query, i.e. it must have columns with the
            expected names and compatible types. The meaning of the other
            arguments is the same as in the <literal>where()</literal> method
            (see <xref linkend="Table.where" />).</para>

            <para>The number of rows appended to <emphasis>dstTable</emphasis>
            is returned as a result.</para>
          </section>

          <section id="getWhereListTableDescr">
            <title>getWhereList(condition, condvars=None, sort=False)</title>

            <para>Get the row coordinates fulfilling the given
            <emphasis>condition</emphasis>.</para>

            <para>The coordinates are returned as a list of the current
            flavor.</para>

            <para><emphasis>sort</emphasis> means that you want to retrieve
            the coordinates ordered. The default is to not sort them.</para>
          </section>
        </section>

        <section id="TableSpecialMethods">
          <title><literal>Table</literal> special methods</title>

          <para>Following are described the methods that automatically trigger
          actions when a <literal>Table</literal> instance is accessed in a
          special way (e.g., <literal>table["var2"]</literal> will be
          equivalent to a call to
          <literal>table.__getitem__("var2")</literal>).</para>

          <section id="Table.__iter__">
            <title>__iter__()</title>

            <para>It returns the same iterator than
            <literal>Table.iterrows(0,0,1)</literal>. However, this does not
            accept parameters.</para>

            <para>Example of use:</para>

            <screen>result = [ row['var2'] for row in table if row['var1'] &lt;= 20 ]</screen>

            <para>Which is equivalent to:</para>

            <screen>result = [ row['var2'] for row in table.iterrows()
                       if row['var1'] &lt;= 20 ]</screen>

            <para><emphasis>Note:</emphasis> This iterator can be nested (see
            example in <xref linkend="Table.where" />).</para>
          </section>

          <section id="Table.__getitem__">
            <title>__getitem__(key)</title>

            <para>Get a row or a range of rows from the table.</para>

            <para>If the <literal>key</literal> argument is an integer, the
            corresponding table row is returned as a record of the current
            flavor. If <literal>key</literal> is a slice, the range of rows
            determined by it is returned as a record array of the current
            flavor.</para>

            <para>Example of use:</para>

            <screen>record = table[4]
recarray = table[4:1000:2]</screen>

            <para>Those statements are equivalent to:</para>

            <screen>record = table.read(start=4)[0]
recarray = table.read(start=4, stop=1000, step=2)</screen>

            <para>Here you can see how indexing and slicing can be used as
            shorthands for the <literal>read()</literal> (see <xref
            linkend="Table.read" />) method.</para>
          </section>

          <section id="Table.__setitem__">
            <title>__setitem__(key, value)</title>

            <para>Sets a table row or table slice.</para>

            <para>It takes different actions depending on the type of the
            <literal>key</literal> parameter: if it is an integer, the
            corresponding table row is set to <literal>value</literal> (a
            record, list or tuple capable of being converted to the table
            field format). If the <literal>key</literal> is a slice, the row
            slice determined by it is set to <literal>value</literal> (a NumPy
            record array, <literal>NestedRecArray</literal> (see <xref
            linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />)
            or list of rows).</para>

            <para>Example of use:</para>

            <screen># Modify just one existing row
table[2] = [456,'db2',1.2]
# Modify two existing rows
rows = numpy.rec.array([[457,'db1',1.2],[6,'de2',1.3]],
                       formats="i4,a3,f8")
table[1:3:2] = rows</screen>

            <para>Which is equivalent to:</para>

            <screen>table.modifyRows(start=2, rows=[456,'db2',1.2])
rows = numpy.rec.array([[457,'db1',1.2],[6,'de2',1.3]],
                       formats="i4,a3,f8")
table.modifyRows(start=1, stop=3, step=2, rows=rows)</screen>
          </section>
        </section>

        <section id="DescriptionClassDescr">
          <title>The <literal>Description</literal> class</title>

          <para>This class represents descriptions of the structure of
          tables.</para>

          <para>An instance of this class is automatically bound to
          <literal>Table</literal> (see <xref linkend="TableClassDescr"
          xrefstyle="select: label" />) objects when they are created.  It
          provides a browseable representation of the structure of the table,
          made of non-nested (<literal>Col</literal> —see <xref
          linkend="ColClassDescr" xrefstyle="select: label" />) and nested
          (<literal>Description</literal>) columns. It also contains
          information that will allow you to build
          <literal>NestedRecArray</literal> (see <xref
          linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />)
          objects suited for the different columns in a table (be they nested
          or not).</para>

          <para>Column definitions (see <literal>Col</literal> class in <xref
          linkend="ColClassDescr" xrefstyle="select: label" />) under a
          description can be accessed as attributes of it. For instance, if
          <literal>table.description</literal> is a
          <literal>Description</literal> instance with a column named
          <literal>col1</literal> under it, the later can be accessed as
          <literal>table.description.col1</literal>. If
          <literal>col1</literal> is nested and contains a
          <literal>col2</literal> column, this can be accessed as
          <literal>table.description.col1.col2</literal>.</para>

          <section>
            <title><literal>Description</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_name</emphasis></glossterm>

                <glossdef>
                  <para>The name of this description group. The name of the
                  root group is <literal>'/'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_names</emphasis></glossterm>

                <glossdef>
                  <para>A list of the names of the columns hanging directly
                  from the associated table or nested column. The order of the
                  names matches the order of their respective columns in the
                  containing table.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_pathnames</emphasis></glossterm>

                <glossdef>
                  <para>A list of the pathnames of the columns hanging
                  directly from the associated table or nested column. If the
                  table does not contain nested columns, this is exactly the
                  same as the <literal>_v_names</literal> attribute.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedNames</emphasis></glossterm>

                <glossdef>
                  <para>A nested list of the names of all the columns under
                  this table or nested column. You can use this for the
                  <literal>names</literal> argument of
                  <literal>NestedRecArray</literal> factory functions.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedFormats</emphasis></glossterm>

                <glossdef>
                  <para>A nested list of the NumPy string formats (and shapes)
                  of all the columns under this table or nested column. You
                  can use this for the <literal>formats</literal> argument of
                  <literal>NestedRecArray</literal> factory functions.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedDescr</emphasis></glossterm>

                <glossdef>
                  <para>A nested list of pairs of <literal>(name,
                  format)</literal> tuples for all the columns under this
                  table or nested column. You can use this as the
                  <literal>dtype</literal> argument of NumPy array factories,
                  as well as the <literal>descr</literal> argument of
                  <literal>NestedRecArray</literal> factories.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_types</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of non-nested columns
                  hanging directly from the associated table or nested column
                  to their respective PyTables types.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_dtypes</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of non-nested columns
                  hanging directly from the associated table or nested column
                  to their respective NumPy types.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_dflts</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of non-nested columns
                  hanging directly from the associated table or nested column
                  to their respective default values.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_colObjects</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of the columns hanging
                  directly from the associated table or nested column to their
                  respective descriptions (<literal>Col</literal> —see <xref
                  linkend="ColClassDescr" xrefstyle="select: label" />— or
                  <literal>Description</literal> —see <xref
                  linkend="DescriptionClassDescr" xrefstyle="select: label"
                  />— instances).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_itemsize</emphasis></glossterm>

                <glossdef>
                  <para>The size in bytes of an item with this
                  structure.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedlvl</emphasis></glossterm>

                <glossdef>
                  <para>The level of the associated table or nested column in
                  the nested datatype.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_is_nested</emphasis></glossterm>

                <glossdef>
                  <para>Whether the associated table has nested columns or not
                  (boolean).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Description</literal> methods</title>

            <section>
              <title>_f_walk(type='All')</title>

              <para>Iterate over nested columns.</para>

              <para>If <literal>type</literal> is <literal>'All'</literal>
              (the default), all column description objects
              (<literal>Col</literal> and <literal>Description</literal>
              instances) are returned in top-to-bottom order
              (preorder).</para>

              <para>If <literal>type</literal> is <literal>'Col'</literal> or
              <literal>'Description'</literal>, only column descriptions of
              that type are returned.</para>
            </section>
          </section>
        </section>

        <section id="RowClassDescr">
          <title>The <literal>Row</literal> class</title>

          <para>This class is used to fetch and set values on the table
          fields. It works very much like a dictionary, where the keys are the
          field names or positions (extended slicing is supported) of the
          fields in the associated table in a specific row.</para>

          <para>This object turns out to actually be an extension type, so you
          won't be able to access its documentation interactively. However,
          you will be able to access some of its internal attributes through
          the use of Python properties. In addition, there are some important
          methods that are useful for acessing, adding and modifying values in
          tables.</para>

          <section>
            <title><literal>Row</literal> attributes</title>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">nrow</emphasis></glossterm>

                <glossdef>
                  <para>Property that returns the current row number in the
                  table. It is useful to know which row is being dealt with in
                  the middle of a loop or iterator.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="RowMethods">
            <title><literal>Row</literal> methods</title>

            <section id="Row.append" xreflabel="description">
              <title>append()</title>

              <para>Once you have filled the proper fields for the current
              row, calling this method actually append these new data to the
              disk (actually data are written to the output buffer).</para>

              <para>Example of use: <screen>row = table.row
for i in xrange(nrows):
    row['col1'] = i-1
    row['col2'] = 'a'
    row['col3'] = -1.0
    row.append()
table.flush()</screen> <emphasis>Warning:</emphasis> after completion of the
              loop in which <literal>Row.append()</literal> has been called,
              it is always convenient to make a call to
              <literal>Table.flush()</literal> in order to avoid losing the
              last rows that can be in internal buffers.</para>
            </section>

            <section id="Row.update" xreflabel="description">
              <title>update()</title>

              <para>This allows you to modify values of your tables when you
              are in the middle of table iterators, like
              <literal>Table.iterrows()</literal> (see <xref
              linkend="Table.iterrows" xrefstyle="select: label" />) or
              <literal>Table.where()</literal> (see <xref
              linkend="Table.where" />). Once you have filled the proper
              fields for the current row, calling this method actually commits
              these data to the disk (actually data is written to the output
              buffer, so the same warning noted in
              <literal>Row.append()</literal> applies).</para>

              <para>Example of use: <screen>for row in table.iterrows(step=10):
    row['col1'] = row.nrow
    row['col2'] = 'b'
    row['col3'] = 0.0
    row.update()</screen> which modifies every tenth row in table. Or:
              <screen>for row in table.where('col1 &gt; 3'):
    row['col1'] = row.nrow
    row['col2'] = 'b'
    row['col3'] = 0.0
    row.update()</screen> which just updates the rows with values in first
              column bigger than 3.</para>
            </section>

            <section id="Row.fetch_all_fields" xreflabel="description">
              <title>fetch_all_fields()</title>

              <para>Retrieve all the fields in the current row.</para>

              <para>Contrarily to <literal>row[:]</literal> (see <xref
              linkend="RowSpecialMethods" xrefstyle="select: label" />), this
              returns the data as NumPy void scalars.</para>

              <para>Example of use: <screen>[ row.fetch_all_fields() for row in table.where('col1 &lt; 3') ]</screen>
              which select all the rows that fullfills a certain
              condition.</para>
            </section>
          </section>

          <section id="RowSpecialMethods">
            <title><literal>Row</literal> special methods</title>

            <section>
              <title>__getitem__(key)</title>

              <para>Returns the row field specified in
              <literal>key</literal>.</para>

              <para><literal>key</literal> can be a string (the name of the
              field), an integer (the position of the field) or a slice (the
              range of field positions). When <literal>key</literal> is a
              slice, the returned value is a <emphasis>tuple</emphasis>
              containing the values of the specified fields.</para>

              <para>Examples of use: <screen>res = [ row['var3'] for row in table.where('var2 &lt; 20') ]</screen>
              which selects the <literal>var3</literal> field for all the rows
              that fullfill the condition. Or: <screen>res = [ row[4] for row in table if row[1] &lt; 20 ]</screen>
              which selects the field in <literal>4th</literal> position for
              all the rows that fullfill the condition. Or: <screen>res = [ row[:] for row in table if row['var2'] &lt; 20 ]</screen>
              which selects the all the fields (in the form of a
              <emphasis>tuple</emphasis>) for all the rows that fullfill the
              condition. Or: <screen>res = [ row[1::2] for row in table.iterrows(2, 3000, 3) ]</screen>
              which selects the all the fields in even positions (in the form
              of a <emphasis>tuple</emphasis>) for all the rows in the slice
              <literal>[2:3000:3]</literal>.</para>
            </section>

            <section>
              <title>__setitem__(key, value)</title>

              <para>Set the <literal>key</literal> row field to the specified
              <literal>value</literal>.</para>

              <para>Differently from its <literal>__getitem__()</literal>
              counterpart, in this case <literal>key</literal> can only be a
              string (the name of the field). <literal>__setitem__()</literal>
              will not take effect on the table data on-disk until any of the
              <xref linkend="Row.append" xrefstyle="select: label" /> or
              <xref linkend="Row.update" xrefstyle="select: label" />
              methods would be called.</para>

              <para>Example of use: <screen>for row in table.iterrows(step=10):
    row['col1'] = row.nrow
    row['col2'] = 'b'
    row['col3'] = 0.0
    row.update()</screen> which modifies every tenth row in table.</para>
            </section>
          </section>
        </section>

        <section id="ColsClassDescr">
          <title>The <literal>Cols</literal> class</title>

          <para>This class is used as an <emphasis>accessor</emphasis> to the
          table columns following the natural name convention, so that you can
          access the different columns because there exists one attribute with
          the name of the columns for each associated column, which can be a
          <literal>Column</literal> instance (non-nested column) or another
          <literal>Cols</literal> instance (nested column).</para>

          <para>Columns under a <literal>Cols</literal> accessor can be
          accessed as attributes of it. For instance, if
          <literal>table.cols</literal> is a <literal>Cols</literal> instance
          with a column named <literal>col1</literal> under it, the later can
          be accessed as <literal>table.cols.col1</literal>. If
          <literal>col1</literal> is nested and contains a
          <literal>col2</literal> column, this can be accessed as
          <literal>table.cols.col1.col2</literal> and so on and so
          forth.</para>

          <section>
            <title><literal>Cols</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_colnames</emphasis></glossterm>

                <glossdef>
                  <para>A list of the names of the columns (or nested columns)
                  hanging directly from this <literal>Cols</literal> instance.
                  The order of the names matches the order of their respective
                  columns in the containing table.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_colpathnames</emphasis></glossterm>

                <glossdef>
                  <para>A list of the complete pathnames of the columns
                  hanging directly from this <literal>Cols</literal> instance.
                  If the table does not contain nested columns, this is
                  exactly the same as <literal>_v_colnames</literal>
                  attribute.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_table</emphasis></glossterm>

                <glossdef>
                  <para>The parent <literal>Table</literal> instance.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_desc</emphasis></glossterm>

                <glossdef>
                  <para>The associated Description (see <xref
                  linkend="ColumnClassDescr" xrefstyle="select: label" />)
                  instance.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Cols</literal> methods</title>

            <section id="Cols._f_col">
              <title>_f_col(colname)</title>

              <para>Return a handler to the <emphasis>colname</emphasis>
              column. If <emphasis>colname</emphasis> is a nested column, a
              <literal>Cols</literal> instance is returned. If
              <emphasis>colname</emphasis> is a non-nested column a
              <literal>Column</literal> object is returned instead.</para>
            </section>

            <section id="Cols.__getitem__">
              <title>__getitem__(key)</title>

              <para>Get a row or a range of rows from a (nested)
              column.</para>

              <para>If the <literal>key</literal> argument is an integer, the
              corresponding nested type row is returned as a record of the
              current flavor. If <literal>key</literal> is a slice, the range
              of rows determined by it is returned as a record array of the
              current flavor.</para>

              <para>Example of use:</para>

              <screen>record = table.cols[4]  # equivalent to table[4]
recarray = table.cols.Info[4:1000:2]</screen>

              <para>Those statements are equivalent to:</para>

              <screen>nrecord = table.read(start=4)[0]
nrecarray = table.read(start=4, stop=1000, step=2).field('Info')</screen>

              <para>Here you can see how a mix of natural naming, indexing and
              slicing can be used as shorthands for the
              <literal>read()</literal> (see <xref linkend="Table.read" />)
              method.</para>
            </section>

            <section>
              <title>__setitem__(key)</title>

              <para>Set a row or a range of rows to the
              <literal>Cols</literal> accessor.</para>

              <para>If the <literal>key</literal> argument is an integer, the
              corresponding <literal>Cols</literal> row is set to the
              <literal>value</literal> object. If <literal>key</literal> is a
              slice, the range of rows determined by it is set to the
              <literal>value</literal> object.</para>

              <para>Example of use:</para>

              <screen>table.cols[4] = record
table.cols.Info[4:1000:2] = recarray</screen>

              <para>Those statements are equivalent to:</para>

              <screen>table.modifyRows(4, rows=record)
table.modifyColumn(4, 1000, 2, colname='Info', column=recarray)</screen>

              <para>Here you can see how a mix of natural naming, indexing and
              slicing can be used as shorthands for the
              <literal>modifyRows()</literal> and
              <literal>modifyColumn()</literal> (see <xref
              linkend="Table.modifyRows" /> and <xref
              linkend="Table.modifyColumn" />) methods.</para>
            </section>
          </section>
        </section>

        <section id="ColumnClassDescr">
          <title>The <literal>Column</literal> class</title>

          <para>Each instance of this class is associated with one column of
          every table. These instances are mainly used to fetch and set actual
          data from the table columns, but there are a few other associated
          methods to deal with indexes.</para>

          <section>
            <title><literal>Column</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">table</emphasis></glossterm>

                <glossdef>
                  <para>The parent <literal>Table</literal> instance.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">name</emphasis></glossterm>

                <glossdef>
                  <para>The name of the associated column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">pathname</emphasis></glossterm>

                <glossdef>
                  <para>The complete pathname of the associated column. This
                  is mainly useful in nested columns; for non-nested ones this
                  value is the same a <literal>name</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">type</emphasis></glossterm>

                <glossdef>
                  <para>The data type of the column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stype</emphasis></glossterm>

                <glossdef>
                  <para>The data type, in string format, of the column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">index</emphasis></glossterm>

                <glossdef>
                  <para>The associated <literal>Index</literal> object (see
                  <xref linkend="IndexClassDescr" xrefstyle="select: label"
                  />) to this column (<literal>None</literal> if it does not
                  exist).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Column</literal> methods</title>

            <section id="createIndexColumnDescr" xreflabel="description">
              <title><literal>createIndex(optlevel=5,
              filters=None)</literal></title>

              <para>Create an <literal>Index</literal> (see <xref
              linkend="IndexClassDescr" xrefstyle="select: label" />) object
              for this column. You can select the level of the optimization of
              the index by setting <literal>optlevel</literal> from 0 to 9. A
              value of 0 means that the index will be created, but not
              optimized (fastest time for index creation and very low memory
              consumption, but slowest lookup times) and a value of 9 means
              maximum optimization (slowest time for index creation and more
              memory used, but fastest lookup times).</para>

              <para>You can use the <literal>filters</literal> argument to set
              the <literal>Filters</literal> (see <xref
              linkend="FiltersClassDescr" xrefstyle="select: label" />) used
              to compress the index. If <literal>None</literal>, default index
              filters will be used (currently, zlib level 1 with
              shuffling).</para>
            </section>

            <section id="optimizeIndexColumnDescr" xreflabel="description">
              <title><literal>optimizeIndex(optlevel=9)</literal></title>

              <para>Optimize an already created index for this column. You can
              select the level of the optimization of the index by setting
              <literal>optlevel</literal> from 0 to 9. A value of 0 means no
              optimization at all (i.e. it does nothing in this case), and a
              value of 9 means maximum optimization (slowest time for index
              creation and more memory used, but fastest lookup times).</para>
            </section>

            <section id="reIndexColumnDescr">
              <title><literal>reIndex()</literal></title>

              <para>Recompute the index associated with this column. This can
              be useful when you suspect that, for any reason, the index
              information is no longer valid and want to rebuild it.</para>
            </section>

            <section id="reIndexDirtyColumnDescr">
              <title><literal>reIndexDirty()</literal></title>

              <para>Recompute the existing index only if it is dirty. This can
              be useful when you have set <literal>Table.autoIndex</literal>
              (see <xref linkend="TableInstanceVariablesDescr"
              xrefstyle="select: label" />) to false for the table and want to
              update the column's index after a invalidating index operation
              (<literal>Table.removeRows</literal>, for example).</para>
            </section>

            <section id="removeIndexColumnDescr">
              <title><literal>removeIndex()</literal></title>

              <para>Remove the index associated with this column.</para>

              <para>If the column is not indexed, nothing happens. The index
              can be created again by calling the
              <literal>createIndex()</literal> method (see <xref
              linkend="createIndexColumnDescr" />).</para>
            </section>
          </section>

          <section>
            <title><literal>Column</literal> special methods</title>

            <section id="Column.__getitem__">
              <title><literal>__getitem__(key)</literal></title>

              <para>Returns a column element or slice. It takes different
              actions depending on the type of the <emphasis>key</emphasis>
              parameter:</para>

              <glosslist>
                <glossentry>
                  <glossterm><emphasis
                  role="bold"><literal>key</literal></emphasis> is an
                  <literal>Integer</literal></glossterm>

                  <glossdef>
                    <para>The corresponding element in the column is returned
                    as a scalar object or as a NumPy object, depending on its
                    shape.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold"><literal>key</literal></emphasis> is a
                  <literal>Slice</literal></glossterm>

                  <glossdef>
                    <para>The row range determined by this slice is returned
                    as a NumPy object.</para>
                  </glossdef>
                </glossentry>
              </glosslist>

              <para>Example of use: <screen>print "Column handlers:"
for name in table.colnames:
    print table.cols[name]
print
print "Some selections:"
print "Select table.cols.name[1]--&gt;", table.cols.name[1]
print "Select table.cols.name[1:2]--&gt;", table.cols.name[1:2]
print "Select table.cols.lati[1:3]--&gt;", table.cols.lati[1:3]
print "Select table.cols.pressure[:]--&gt;", table.cols.pressure[:]
print "Select table.cols['temperature'][:]--&gt;", table.cols['temperature'][:]</screen>
              and the output of this for a certain arbitrary table is:
              <screen>Column handlers:
/table.cols.name (Column(1,), CharType)
/table.cols.lati (Column(2,), Int32)
/table.cols.longi (Column(1,), Int32)
/table.cols.pressure (Column(1,), Float32)
/table.cols.temperature (Column(1,), Float64)

Some selections:
Select table.cols.name[1]--&gt; Particle:     11
Select table.cols.name[1:2]--&gt; ['Particle:     11']
Select table.cols.lati[1:3]--&gt; [[11 12]
 [12 13]]
Select table.cols.pressure[:]--&gt; [  90.  110.  132.]
Select table.cols['temperature'][:]--&gt; [ 100.  121.  144.]</screen> See the
              <literal>examples/table2.py</literal> for a more complete
              example.</para>
            </section>

            <section id="Column.__setitem__">
              <title>__setitem__(key, value)</title>

              <para>It takes different actions depending on the type of the
              <literal>key</literal> parameter:</para>

              <glosslist>
                <glossentry>
                  <glossterm><emphasis
                  role="bold"><literal>key</literal></emphasis> is an
                  <literal>Integer</literal></glossterm>

                  <glossdef>
                    <para>The corresponding element in the column is set to
                    <emphasis>value</emphasis>. <emphasis>value</emphasis>
                    must be a scalar or NumPy/<literal>numarray</literal>
                    object, depending on column's shape.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold"><literal>key</literal></emphasis> is a
                  <literal>Slice</literal></glossterm>

                  <glossdef>
                    <para>The row slice determined by <emphasis>key</emphasis>
                    is set to <emphasis>value</emphasis>.
                    <emphasis>value</emphasis> must be a list of elements or a
                    NumPy/<literal>numarray</literal>.</para>
                  </glossdef>
                </glossentry>
              </glosslist>

              <para>Example of use:</para>

              <screen># Modify row 1
table.cols.col1[1] = -1
# Modify rows 1 and 3
table.cols.col1[1::2] = [2,3]</screen>

              <para>Which is equivalent to:</para>

              <screen># Modify row 1
table.modifyColumns(start=1, columns=[[-1]], names=["col1"])
# Modify rows 1 and 3
columns = numpy.rec.fromarrays([[2,3]], formats="i4")
table.modifyColumns(start=1, step=2, columns=columns, names=["col1"])</screen>
            </section>
          </section>
        </section>
      </section>

      <section id="ArrayClassDescr">
        <title>The <literal>Array</literal> class</title>

        <para>This class represents homogeneous datasets in an HDF5
        file.</para>

        <para>This class provides methods to write or read data to or from
        array objects in the file. This class does not allow you neither to
        enlarge nor compress the datasets on disk; use the
        <literal>EArray</literal> (see <xref linkend="EArrayClassDescr"
        xrefstyle="select: label" />) class if you want enlargeable dataset
        support or compression features, or <literal>CArray</literal> (see
        <xref linkend="CArrayClassDescr" xrefstyle="select: label" />) if you
        just want compression.</para>

        <para>An interesting property of the <literal>Array</literal> class is
        that it remembers the <emphasis>flavor</emphasis> of the object that
        has been saved so that if you saved, for example, a
        <literal>list</literal>, you will get a <literal>list</literal> during
        readings afterwards; if you saved a NumPy array, you will get a NumPy
        object, and so forth.</para>

        <para>Note that this class inherits all the public attributes and
        methods that <literal>Leaf</literal> (see <xref
        linkend="LeafClassDescr" xrefstyle="select: label" />) already
        provides. However, as <literal>Array</literal> instances have no
        internal I/O buffers, it is not necessary to use the
        <literal>flush()</literal> method they inherit from
        <literal>Leaf</literal> in order to save their internal state to disk.
        When a writing method call returns, all the data is already on
        disk.</para>

        <section id="ArrayClassInstanceVariables">
          <title><literal>Array</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">atom</emphasis></glossterm>

              <glossdef>
                <para>An <literal>Atom</literal> instance representing the
                shape and type of the atomic objects. See <xref
                linkend="AtomClassDescr" xrefstyle="select: label" /> for a
                better explanation.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrow</emphasis></glossterm>

              <glossdef>
                <para>On iterators, this is the index of the current
                row.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">maindim</emphasis></glossterm>

              <glossdef>
                <para>The dimension along which iterators work. Its value is 0
                (i.e. the first dimension) when the dataset is not extendable,
                and <literal>self.extdim</literal> (where available) for
                extendable ones.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Array</literal> methods</title>

          <section id="Array.getEnum">
            <title>getEnum()</title>

            <para>Get the enumerated type associated with this array.</para>

            <para>If this array is of an enumerated type, the corresponding
            <literal>Enum</literal> instance (see <xref
            linkend="EnumClassDescr" xrefstyle="select: label" />) is
            returned. If it is not of an enumerated type, a
            <literal>TypeError</literal> is raised.</para>
          </section>

          <section id="iterrowsArrayDescr">
            <title>iterrows(start=None, stop=None, step=1)</title>

            <para>Returns an iterator yielding NumPy instances built from rows
            in array. The return rows are taken from the
            <emphasis>main</emphasis> dimension. If a range is supplied (i.e.
            some of the <emphasis>start</emphasis>, <emphasis>stop</emphasis>
            or <emphasis>step</emphasis> parameters are passed), only the
            appropriate rows are returned. Else, all the rows are returned.
            See also the and <literal>__iter__()</literal> special methods in
            <xref linkend="ArraySpecialMethods" xrefstyle="select: label" />
            for a shorter way to call this iterator.</para>

            <para>The meaning of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> and <emphasis>step</emphasis> parameters
            is the same as in the <literal>range()</literal> python function,
            except that negative values of <literal>step</literal> are not
            allowed. Moreover, if only <literal>start</literal> is specified,
            then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <emphasis>start</emphasis> nor <emphasis>stop</emphasis>, then all
            the rows in the object are selected.</para>

            <para>Example of use:</para>

            <screen>result = [ row for row in arrayInstance.iterrows(step=4) ]</screen>
          </section>

          <section id="readArrayDescr">
            <title>read(start=None, stop=None, step=1)</title>

            <para>Read the array from disk and return it as a NumPy (default)
            object, or an object with the same original
            <emphasis>flavor</emphasis> that it was saved. It accepts
            <emphasis>start</emphasis>, <emphasis>stop</emphasis> and
            <emphasis>step</emphasis> parameters to select rows (in the
            <emphasis>main</emphasis> dimension) for reading.</para>

            <para>The meaning of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> and <emphasis>step</emphasis> parameters
            is the same as in the <literal>range()</literal> python function,
            except that negative values of <literal>step</literal> are not
            allowed. Moreover, if only <literal>start</literal> is specified,
            then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <emphasis>start</emphasis> nor <emphasis>stop</emphasis>, then all
            the rows in the object are selected.</para>
          </section>
        </section>

        <section id="ArraySpecialMethods">
          <title><literal>Array</literal> special methods</title>

          <para>Following are described the methods that automatically trigger
          actions when an <literal>Array</literal> instance is accessed in a
          special way (e.g., <literal>array[2:3,...,::2]</literal> will be
          equivalent to a call to <literal>array.__getitem__(slice(2,3, None),
          Ellipsis, slice(None, None, 2))</literal>).</para>

          <section id="Array.__iter__">
            <title>__iter__()</title>

            <para>It returns the same iterator than
            <literal>Array.iterrows(0,0,1)</literal>. However, this does not
            accept parameters.</para>

            <para>Example of use:</para>

            <screen>result = [ row[2] for row in array ]</screen>

            <para>Which is equivalent to:</para>

            <screen>result = [ row[2] for row in array.iterrows(0, 0, 1) ]</screen>
          </section>

          <section id="Array.__getitem__">
            <title>__getitem__(key)</title>

            <para>It returns a NumPy (default) object (or an object with the
            same original <emphasis>flavor</emphasis> that it was saved)
            containing the slice of rows stated in the <literal>key</literal>
            parameter. The set of allowed tokens in <literal>key</literal> is
            the same as extended slicing in python (the
            <literal>Ellipsis</literal> token included).</para>

            <para>Example of use:</para>

            <screen>array1 = array[4]   # array1.shape == array.shape[1:]
array2 = array[4:1000:2]  # len(array2.shape) == len(array.shape)
array3 = array[::2, 1:4, :]
array4 = array[1, ..., ::2, 1:4, 4:] # General slice selection</screen>
          </section>

          <section id="Array.__setitem__" xreflabel="description">
            <title>__setitem__(key, value)</title>

            <para>Sets an Array element, row or extended slice. It takes
            different actions depending on the type of the
            <literal>key</literal> parameter:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold"><literal>key</literal> is an
                integer:</emphasis></glossterm>

                <glossdef>
                  <para>The corresponding row is assigned to value. If needed,
                  this <literal>value</literal> is broadcasted to fit the
                  specified row.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold"><literal>key</literal> is a
                slice:</emphasis></glossterm>

                <glossdef>
                  <para>The row slice determined by it is assigned to
                  <literal>value</literal>. If needed, this
                  <literal>value</literal> is broadcasted to fit in the
                  desired range. If the slice to be updated exceeds the actual
                  shape of the array, only the values in the existing range
                  are updated, i.e. the index error will be silently ignored.
                  If <literal>value</literal> is a multidimensional object,
                  then its shape must be compatible with the slice specified
                  in <literal>key</literal>, otherwise, a
                  <literal>ValueError</literal> will be issued.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Example of use:</para>

            <screen>a1[0] = 333       # Assign an integer to a Integer Array row
a2[0] = "b"       # Assign a string to a string Array row
a3[1:4] = 5       # Broadcast 5 to slice 1:4
a4[1:4:2] = "xXx" # Broadcast "xXx" to slice 1:4:2
# General slice update (a5.shape = (4,3,2,8,5,10)
a5[1, ..., ::2, 1:4, 4:] = arange(1728, shape=(4,3,2,4,3,6))</screen>
          </section>
        </section>
      </section>

      <section id="CArrayClassDescr">
        <title>The <literal>CArray</literal> class</title>

        <para>This class represents homogeneous datasets in an HDF5
        file.</para>

        <para>The difference between a normal <literal>Array</literal> (see
        <xref linkend="ArrayClassDescr" xrefstyle="select: label" />) and a
        <literal>CArray</literal> is that a <literal>CArray</literal> has a
        chunked layout and, as a consequence, it supports compression.  You
        can use datasets of this class to easily save or load arrays to or
        from disk, with compression support included.</para>

        <section id="CArrayClassInstanceVariables">
          <title><literal>CArray</literal> instance variables</title>

          <para>In addition to the attributes that <literal>CArray</literal>
          inherits from <literal>Array</literal>, it supports some more that
          provide information about the filters used.</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">extdim</emphasis></glossterm>

              <glossdef>
                <para>The <emphasis>enlargeable dimension</emphasis>, i.e. the
                dimension this array can be extended or shrunken along (-1 if
                it is not extendable).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title>Example of use</title>

          <para>See below a small example of <literal>CArray</literal> class.
          The code is available in
          <literal>examples/carray1.py</literal>.</para>

          <screen>import numpy
import tables

fileName = 'carray1.h5'
shape = (200,300)
atom = tables.UInt8Atom()
filters = tables.Filters(complevel=5, complib='zlib')

h5f = tables.openFile(fileName,'w')
ca = h5f.createCArray(h5f.root, 'carray', atom, shape,
                      filters=filters, chunkshape=(128,128))
# Fill a hyperslab in ca
ca[10:60,20:70] = numpy.ones((50,50))  # Will be converted to UInt8 elements
h5f.close()

# Re-open a read another hyperslab
h5f = tables.openFile(fileName)
print h5f
print h5f.root.carray[8:12, 18:22]
h5f.close()</screen>

          <para>The output for the previous script is something like:</para>

          <screen>carray1.h5 (File) ''
Last modif.: 'Thu Jun 16 10:47:18 2005'
Object Tree:
/ (RootGroup) ''
/carray (CArray(200L, 300L)) ''

[[0 0 0 0]
 [0 0 0 0]
 [0 0 1 1]
 [0 0 1 1]]</screen>
        </section>
      </section>

      <section id="EArrayClassDescr">
        <title>The <literal>EArray</literal> class</title>

        <para>This class represents extendible, homogeneous datasets in an
        HDF5 file.</para>

        <para>The main difference between an <literal>EArray</literal> and a
        <literal>CArray</literal> (see <xref linkend="CArrayClassDescr"
        xrefstyle="select: label" />) is that the former can be enlarged along
        one of its dimensions (the <emphasis>enlargeable dimension</emphasis>)
        using the <literal>append()</literal> method (multiple enlargeable
        dimensions might be supported in the future). An
        <literal>EArray</literal> dataset can also be shrunken along its
        enlargeable dimension using the <literal>truncate()</literal>
        method.</para>

        <section id="EArrayMethodsDescr">
          <title><literal>EArray</literal> methods</title>

          <section id="EArrayAppendDescr">
            <title>append(sequence)</title>

            <para>Appends a <literal>sequence</literal> to the underlying
            dataset. Obviously, this sequence must have the same type as the
            <literal>EArray</literal> instance; otherwise a
            <literal>TypeError</literal> is issued. In the same way, the
            dimensions of the <literal>sequence</literal> have to conform to
            those of <literal>EArray</literal>, that is, all the dimensions
            have to be the same except, of course, that of the enlargeable
            dimension which can be of any length (even 0!).</para>

            <para>Example of use (code available in
            <literal>examples/earray1.py</literal>):</para>

            <screen>import tables
import numpy

fileh = tables.openFile("earray1.h5", mode = "w")
a = tables.StringAtom(itemsize=8)
# Use 'a' as the object type for the enlargeable array
array_c = fileh.createEArray(fileh.root, 'array_c', a, (0,), "Chars")
array_c.append(numpy.array(['a'*2, 'b'*4], dtype='S8'))
array_c.append(numpy.array(['a'*6, 'b'*8, 'c'*10], dtype='S8'))

# Read the string EArray we have created on disk
for s in array_c:
    print "array_c[%s] =&gt; '%s'" % (array_c.nrow, s)
# Close the file
fileh.close()</screen>

            <para>and the output is:</para>

            <screen>array_c[0] =&gt; 'aa'
array_c[1] =&gt; 'bbbb'
array_c[2] =&gt; 'aaaaaa'
array_c[3] =&gt; 'bbbbbbbb'
array_c[4] =&gt; 'cccccccc'</screen>
          </section>
        </section>
      </section>

      <section id="VLArrayClassDescr">
        <title>The <literal>VLArray</literal> class</title>

        <para>This class represents variable length (ragged) arrays in an HDF5
        file.</para>

        <para>Instances of this class represent array objects in the object
        tree with the property that their rows can have a
        <emphasis>variable</emphasis> number of homogeneous elements, called
        <emphasis>atoms</emphasis>. Like <literal>Table</literal> (see <xref
        linkend="TableClassDescr" xrefstyle="select: label" />) datasets,
        variable length arrays can have only one dimension, and the elements
        (atoms) of their rows can be fully multidimensional.
        <literal>VLArray</literal> objects do also support compression.</para>

        <para>This class provides methods to write or read data to or from
        variable length array objects in the file. Note that it also inherits
        all the public attributes and methods that <literal>Leaf</literal>
        (see <xref linkend="LeafClassDescr" xrefstyle="select: label" />)
        already provides.</para>

        <section>
          <title><literal>VLArray</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">atom</emphasis></glossterm>

              <glossdef>
                <para>An <literal>Atom</literal> instance representing the
                shape and type of the atomic objects. See <xref
                linkend="AtomClassDescr" xrefstyle="select: label" /> for a
                better explanation.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">flavor</emphasis></glossterm>

              <glossdef>
                <para>The type of data object read from this leaf.</para>

                <para>Please note that when reading rows of
                <literal>VLArray</literal> data, <literal>flavor</literal>
                only applies to the <emphasis>components</emphasis> of the
                returned Python list, not to the list itself.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrows</emphasis></glossterm>

              <glossdef>
                <para>The total number of rows of the array.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrow</emphasis></glossterm>

              <glossdef>
                <para>On iterators, this is the index of the current
                row.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>VLArray</literal> methods</title>

          <section id="VLArray.getEnum">
            <title>getEnum()</title>

            <para>Get the enumerated type associated with this array.</para>

            <para>If this array is of an enumerated type, the corresponding
            <literal>Enum</literal> instance (see <xref
            linkend="EnumClassDescr" xrefstyle="select: label" />) is
            returned. If it is not of an enumerated type, a
            <literal>TypeError</literal> is raised.</para>
          </section>

          <section id="VLArray.append">
            <title>append(sequence, *objects)</title>

            <para>Append objects in the <literal>sequence</literal> to the
            array.</para>

            <para>This method appends the objects in the
            <literal>sequence</literal> to a <emphasis>single row</emphasis>
            in this array. The type of individual objects must be compliant
            with the type of atoms in the array. In the case of variable
            length strings, the very string to append is the
            <literal>sequence</literal>.</para>

            <para>Example of use (code available in
            <literal>examples/vlarray1.py</literal>):</para>

            <screen>import tables
from numpy import *

# Create a VLArray:
fileh = tables.openFile("vlarray1.h5", mode = "w")
vlarray = fileh.createVLArray(fileh.root, 'vlarray1',
                              tables.Int32Atom(shape=1),
                              "ragged array of ints",
                              filters = tables.Filters(1))
# Append some (variable length) rows:
vlarray.append(array([5, 6]))
vlarray.append(array([5, 6, 7]))
vlarray.append([5, 6, 9, 8])

# Now, read it through an iterator:
print "--&gt;", vlarray.title
for x in vlarray:
    print vlarray.name+"["+str(vlarray.nrow)+"]--&gt;", x

# Now, do the same with native python strings
vlarray2 = fileh.createVLArray(fileh.root, 'vlarray2',
                              tables.StringAtom(itemsize=2),
                              "ragged array of strings",
                              filters = tables.Filters(1))
vlarray2.flavor = "python"
# Append some (variable length) rows:
print "--&gt;", vlarray2.title
vlarray2.append(["5", "66"])
vlarray2.append(["5", "6", "77"])
vlarray2.append(["5", "6", "9", "88"])

# Now, read it through an iterator:
for x in vlarray2:
    print vlarray2.name+"["+str(vlarray2.nrow)+"]--&gt;", x


# Close the file
fileh.close()</screen>

            <para>The output of the previous program looks like this:</para>

            <screen>--&gt; ragged array of ints
vlarray1[0]--&gt; [5 6]
vlarray1[1]--&gt; [5 6 7]
vlarray1[2]--&gt; [5 6 9 8]
--&gt; ragged array of strings
vlarray2[0]--&gt; ['5', '66']
vlarray2[1]--&gt; ['5', '6', '77']
vlarray2[2]--&gt; ['5', '6', '9', '88']</screen>

            <para>The <literal>objects</literal> argument is only retained for
            backwards compatibility; please do <emphasis>not</emphasis> use
            it.</para>
          </section>

          <section id="iterrowsVLArrayDescr">
            <title>iterrows(start=None, stop=None, step=1)</title>

            <para>Returns an iterator yielding one row per iteration. If a
            range is supplied (i.e. some of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> or <emphasis>step</emphasis> parameters
            are passed), only the appropriate rows are returned. Else, all the
            rows are returned. See also the <literal>__iter__()</literal>
            special methods in <xref linkend="VLArraySpecialMethods"
            xrefstyle="select: label" /> for a shorter way to call this
            iterator.</para>

            <para>The meaning of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> and <emphasis>step</emphasis> parameters
            is the same as in the <literal>range()</literal> python function,
            except that negative values of <literal>step</literal> are not
            allowed. Moreover, if only <literal>start</literal> is specified,
            then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <emphasis>start</emphasis> nor <emphasis>stop</emphasis>, then all
            the rows in the object are selected.</para>

            <para>Example of use:</para>

            <screen>for row in vlarray.iterrows(step=4):
    print vlarray.name+"["+str(vlarray.nrow)+"]--&gt;", row</screen>
          </section>

          <section id="readVLArrayDescr">
            <title>read(start=None, stop=None, step=1)</title>

            <para>Returns the actual data in <literal>VLArray</literal>. As
            the lengths of the different rows are variable, the returned value
            is a python list, with as many entries as specified rows in the
            range parameters.</para>

            <para>The meaning of the <emphasis>start</emphasis>,
            <emphasis>stop</emphasis> and <emphasis>step</emphasis> parameters
            is the same as in the <literal>range()</literal> python function,
            except that negative values of <literal>step</literal> are not
            allowed. Moreover, if only <literal>start</literal> is specified,
            then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <emphasis>start</emphasis> nor <emphasis>stop</emphasis>, then all
            the rows in the object are selected.</para>
          </section>
        </section>

        <section id="VLArraySpecialMethods">
          <title><literal>VLArray</literal> special methods</title>

          <para>Following are described the methods that automatically trigger
          actions when a <literal>VLArray</literal> instance is accessed in a
          special way (e.g., <literal>vlarray[2:5]</literal> will be
          equivalent to a call to
          <literal>vlarray.__getitem__(slice(2,5,None)</literal>).</para>

          <section id="VLArray.__iter__">
            <title>__iter__()</title>

            <para>It returns the same iterator than
            <literal>VLArray.iterrows(0,0,1)</literal>. However, this does not
            accept parameters.</para>

            <para>Example of use:</para>

            <screen>result = [ row for row in vlarray ]</screen>

            <para>Which is equivalent to:</para>

            <screen>result = [ row for row in vlarray.iterrows() ]</screen>
          </section>

          <section id="VLArray.__getitem__">
            <title>__getitem__(key)</title>

            <para>It returns the slice of rows determined by
            <literal>key</literal>, which can be an integer index or an
            extended slice. The returned value is a list of objects of type
            <literal>array.atom.type</literal>.</para>

            <para>Example of use:</para>

            <screen>list1 = vlarray[4]
list2 = vlarray[4:1000:2]</screen>
          </section>

          <section id="VLArray.__setitem__">
            <title>__setitem__(keys, value)</title>

            <para>Updates a vlarray row described by <literal>keys</literal>
            by setting it to <literal>value</literal>. Depending on the value
            of <literal>keys</literal>, the action taken is different:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold"><literal>keys</literal> is an
                integer:</emphasis></glossterm>

                <glossdef>
                  <para>It refers to the number of row to be modified. The
                  <literal>value</literal> object must be type and shape
                  compatible with the object that exists in the vlarray
                  row.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold"><literal>keys</literal> is a
                tuple:</emphasis></glossterm>

                <glossdef>
                  <para>The first element refers to the row to be modified,
                  and the second element to the range (so, it can be an
                  integer or an slice) of the row that will be updated. As
                  above, the <literal>value</literal> object must be type and
                  shape compatible with the object specified in the vlarray
                  row <emphasis>and</emphasis> range.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para><emphasis>Note:</emphasis> When updating
            <literal>vlstring</literal> (codification UTF-8) or
            <literal>object</literal> pseudo-atoms, there is a problem: one
            can only update values with <emphasis>exactly</emphasis> the same
            bytes than in the original row. With UTF-8 encoding this is
            problematic because, for instance, '<literal>c</literal>' takes 1
            byte, but '<literal> </literal>' takes two. The same applies when
            using <literal>Objects</literal> atoms, because when cPickle
            applies to a class instance (for example), it does not guarantee
            to return the same number of bytes than over other instance, even
            of the same class than the former. These facts effectively limit
            the number of objects than can be updated in
            <literal>VLArray</literal>s.</para>

            <para>Example of use:</para>

            <screen>vlarray[0] = vlarray[0]*2+3
vlarray[99,3:] = arange(96)*2+3
# Negative values for start and stop (but not step) are supported
vlarray[99,-99:-89:2] = vlarray[5]*2+3</screen>
          </section>
        </section>
      </section>

      <section id="UnImplementedClassDescr">
        <title>The <literal>UnImplemented</literal> class</title>

        <para>Instances of this class represents an unimplemented dataset in a
        generic HDF5 file. When reading such a file (i.e. one that has not
        been created with PyTables, but with some other HDF5 library based
        tool), chances are that the specific combination of
        <emphasis>datatypes</emphasis> and/or <emphasis>dataspaces</emphasis>
        in some dataset might not be supported by PyTables yet. In such a
        case, this dataset will be mapped into the
        <literal>UnImplemented</literal> class and hence, the user will still
        be able to build the complete object tree of this generic HDF5 file,
        as well as enabling the access (both read and
        <emphasis>write</emphasis>) of the attributes of this dataset and some
        metadata. Of course, the user won't be able to read the actual data on
        it.</para>

        <para>This is an elegant way to allow users to work with generic HDF5
        files despite the fact that some of its datasets would not be
        supported by PyTables. However, if you are really interested in having
        access to an unimplemented dataset, please get in contact with the
        developer team.</para>

        <para>This class does not have any public instance variables, except
        those inherited from the <literal>Leaf</literal> class (see <xref
        linkend="LeafClassDescr" xrefstyle="select: label" />).</para>
      </section>

      <section id="AttributeSetClassDescr">
        <title>The <literal>AttributeSet</literal> class</title>

        <para>Represents the set of attributes of a node (Leaf or Group). It
        provides methods to create new attributes, open, rename or delete
        existing ones.</para>

        <para>Like in <literal>Group</literal> instances,
        <literal>AttributeSet</literal> instances make use of the
        <emphasis>natural naming</emphasis> convention, i.e. you can access
        the attributes on disk like if they were <emphasis>normal</emphasis>
        <literal>AttributeSet</literal> attributes. This offers the user a
        very convenient way to access (but also to set and delete) node
        attributes by simply specifying them like a
        <emphasis>normal</emphasis> attribute class.</para>

        <para><emphasis>Caveat emptor:</emphasis> All Python data types are
        supported. In particular, multidimensional NumPy objects are saved
        natively as multidimensional objects in the HDF5 file. Python strings
        are also saved natively as HDF5 strings, and loaded back as Python
        strings. However, the rest of the data types including the Python
        scalar ones (i.e. Int, Long and Float) and more general objects (like
        <literal>NumPy</literal> or <literal>Numeric</literal>) are serialized
        using <literal>cPickle</literal>, so you will be able to correctly
        retrieve them only from a Python-aware HDF5 library. So, if you want
        to save Python scalar values and be able to read them with generic
        HDF5 tools, you should make use of scalar NumPy objects (for example
        <literal>numpy.array(1, dtype='int64')</literal>). In the same way,
        attributes in HDF5 native files will be always mapped into NumPy
        objects. Specifically, a multidimensional attribute will be mapped
        into a multidimensional <literal>ndarray</literal> and an scalar will
        be mapped into a scalar NumPy object (for example, an attribute of
        type <literal>H5T_NATIVE_LLONG</literal> will be read and returned as
        a <literal>numpy.array(X, dtype='int64')</literal> scalar).</para>

        <para>One more warning: because of the various potential difficulties
        in restoring a Python object stored in an attribute, you may end up
        getting a <literal>cPickle</literal> string where a Python object is
        expected. If this is the case, you may wish to run
        <literal>cPickle.loads()</literal> on that string to get an idea of
        where things went wrong, as shown in this example:</para>

        <screen>&gt;&gt;&gt; import tables
&gt;&gt;&gt;
&gt;&gt;&gt; class MyClass(object):
...   foo = 'bar'
...
&gt;&gt;&gt; # An object of my custom class.
&gt;&gt;&gt; myObject = MyClass()
&gt;&gt;&gt;
&gt;&gt;&gt; h5f = tables.openFile('test.h5', 'w')
&gt;&gt;&gt; h5f.root._v_attrs.obj = myObject  # store the object
&gt;&gt;&gt; print h5f.root._v_attrs.obj.foo  # retrieve it
bar
&gt;&gt;&gt; h5f.close()
&gt;&gt;&gt;
&gt;&gt;&gt; # Delete class of stored object and reopen the file.
&gt;&gt;&gt; del MyClass, myObject
&gt;&gt;&gt;
&gt;&gt;&gt; h5f = tables.openFile('test.h5', 'r')
&gt;&gt;&gt; print h5f.root._v_attrs.obj.foo
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
AttributeError: 'str' object has no attribute 'foo'
&gt;&gt;&gt; # Let us inspect the object to see what is happening.
&gt;&gt;&gt; print repr(h5f.root._v_attrs.obj)
'ccopy_reg\n_reconstructor\np1\n(c__main__\nMyClass\np2\nc__builtin__\nobject\np3\nNtRp4\n.'
&gt;&gt;&gt; # Maybe unpickling the string will yield more information:
&gt;&gt;&gt; import cPickle
&gt;&gt;&gt; cPickle.loads(h5f.root._v_attrs.obj)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
AttributeError: 'module' object has no attribute 'MyClass'
&gt;&gt;&gt; # So the problem was not in the stored object,
&gt;&gt;&gt; # but in the *environment* where it was restored.
&gt;&gt;&gt; h5f.close()</screen>

        <section id="AttributeSetClassInstanceVariables">
          <title><literal>AttributeSet</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">_v_node</emphasis></glossterm>

              <glossdef>
                <para>The parent node instance.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_attrnames</emphasis></glossterm>

              <glossdef>
                <para>List with all attribute names.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_attrnamessys</emphasis></glossterm>

              <glossdef>
                <para>List with system attribute names.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_attrnamesuser</emphasis></glossterm>

              <glossdef>
                <para>List with user attribute names.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>AttributeSet</literal> methods</title>

          <para>Note that this class defines the
          <literal>__setattr__</literal>, <literal>__getattr__</literal> and
          <literal>__delattr__</literal> and they work as normally intended.
          Any scalar (string, ints or floats) attribute is supported natively
          as an attribute. However, <literal>(c)Pickle</literal> is
          automatically used so as to serialize other kind of objects (like
          lists, tuples, dicts, small NumPy/Numeric/numarray objects, ...)
          that you might want to save. If an attribute is set on a target node
          that already has a large number of attributes, a
          <literal>PerformanceWarning</literal> will be issued.</para>

          <para>With these special methods, you can access, assign or delete
          attributes on disk by just using the next constructs: <screen>leaf.attrs.myattr = "str attr"  # Set a string (native support)
leaf.attrs.myattr2 = 3          # Set an integer (native support)
leaf.attrs.myattr3 = [3,(1,2)]  # A generic object (Pickled)
attrib = leaf.attrs.myattr      # Get the attribute myattr
del leaf.attrs.myattr           # Delete the attribute myattr</screen></para>

          <section>
            <title>_f_copy(where)</title>

            <para>Copy the user attributes (as well as
            <emphasis>certain</emphasis> system attributes) to
            <emphasis>where</emphasis> object. <emphasis>where</emphasis> has
            to be a <literal>Group</literal> or <literal>Leaf</literal>
            instance.</para>
          </section>

          <section>
            <title>_f_list(attrset="user")</title>

            <para>Return a list of attribute names of the parent node.
            <emphasis>attrset</emphasis> selects the attribute set to be used.
            A <literal>user</literal> value returns only the user attributes
            and this is the default. <literal>sys</literal> returns only the
            system attributes. <literal>all</literal> returns both the system
            and user attributes.</para>
          </section>

          <section>
            <title>_f_rename(oldattrname, newattrname)</title>

            <para>Rename an attribute.</para>
          </section>
        </section>
      </section>

      <section id="declarativeClasses">
        <title>Declarative classes</title>

        <para>In this section a series of classes that are meant to
        <emphasis>declare</emphasis> datatypes that are required for primary
        PyTables (like <literal>Table</literal> or <literal>VLArray</literal>)
        objects are described.</para>

        <section id="IsDescriptionClassDescr">
          <title>The <literal>IsDescription</literal> class</title>

          <para>This class is designed to be used as an easy, yet meaningful
          way to describe the properties of <literal>Table</literal> objects
          through the definition of <emphasis>derived classes</emphasis> that
          inherit properties from it. In order to define such a class, you
          must declare it as descendant of <emphasis>IsDescription</emphasis>,
          with as many attributes as columns you want in your table. The name
          of each attribute will become the name of a column, and its value
          will hold a description of it.</para>

          <para>Ordinary columns can be described using instances of the
          <literal>Col</literal> (see <xref linkend="ColClassDescr"
          xrefstyle="select: label" />) class. Nested columns can be described
          by using classes derived from <literal>IsDescription</literal> or
          instances of it. Derived classes can be declared in place (in which
          case the column takes the name of the class) or referenced by name,
          and they can have a <literal>_v_pos</literal> special attribute
          which sets the position of the nested column among its sibling
          columns.</para>

          <para>Once you have created a description object, you can pass it to
          the <literal>Table</literal> constructor, where all the information
          it contains will be used to define the table structure. See the
          <xref linkend="secondExample" xrefstyle="select: label" /> for an
          example on how that works.</para>

          <para>See below for a complete list of the special attributes that
          can be specified to complement the <emphasis>metadata</emphasis> of
          an <literal>IsDescription</literal> class.</para>

          <section id="IsDescription.specialAttrs">
            <title><literal>IsDescription</literal> special attributes</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">_v_pos</emphasis></glossterm>

                <glossdef>
                  <para>Sets the position of a possible nested column
                  description among its sibling columns.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>
        </section>

        <section id="ColClassDescr">
          <title>The <literal>Col</literal> class and its descendants</title>

          <para><literal>Col</literal> instances are used as a means to
          declare the different properties of a non-nested column in a table.
          <literal>Col</literal> classes are descendants of their equivalent
          <literal>Atom</literal> classes (see <xref linkend="AtomClassDescr"
          xrefstyle="select: label" />), but their instances have an
          additional <literal>_v_pos</literal> attribute that can be set with
          the optional constructor parameter <literal>pos</literal>, which
          defaults to <literal>None</literal> and may take an integer.</para>

          <para>In the same fashion as <literal>Atom</literal>, you should use
          a particular <literal>Col</literal> descendant class whenever you
          know the exact type you will need when writing your code. Otherwise,
          you may use one of the <literal>Col.from_*()</literal> factory
          methods.</para>

          <section>
            <title><literal>Col</literal> instance attributes</title>

            <para>In addition to the variables that they inherit from the
            <literal>Atom</literal> class, <literal>Col</literal> instances
            have the following attributes:</para>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">_v_pos</emphasis></glossterm>

                <glossdef>
                  <para>The position of this column with regard to its column
                  siblings.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Col</literal> factory methods</title>

            <section>
              <title>from_atom(atom, pos=None)</title>

              <para>Create a <literal>Col</literal> definition from a PyTables
              <emphasis>atom</emphasis>.</para>

              <para>An optional position may be specified as the
              <emphasis>pos</emphasis> argument.</para>
            </section>

            <section>
              <title>from_dtype(dtype, dflt=None, pos=None)</title>

              <para>Create a <literal>Col</literal> definition from a NumPy
              <emphasis>dtype</emphasis>.</para>

              <para>Optional default value and position may be specified as
              the <emphasis>dflt</emphasis> and <emphasis>pos</emphasis>
              arguments, respectively. Information in the
              <emphasis>dtype</emphasis> not represented in a
              <literal>Col</literal> is ignored.</para>
            </section>

            <section>
              <title>from_kind(kind, itemsize=None, shape=1, dflt=None,
              pos=None)</title>

              <para>Create a <literal>Col</literal> definition from a PyTables
              <emphasis>kind</emphasis>.</para>

              <para>Optional item size, shape, default value and position may
              be specified as the <emphasis>itemsize</emphasis>,
              <emphasis>shape</emphasis>, <emphasis>dflt</emphasis> and
              <emphasis>pos</emphasis> arguments, respectively. Bear in mind
              that not all columns support a default item size.</para>
            </section>

            <section>
              <title>from_sctype(sctype, shape=1, dflt=None, pos=None)</title>

              <para>Create a <literal>Col</literal> definition from a NumPy
              scalar type <emphasis>sctype</emphasis>.</para>

              <para>Optional shape, default value and position may be
              specified as the <emphasis>shape</emphasis>,
              <emphasis>dflt</emphasis> and <emphasis>pos</emphasis>
              arguments, respectively. Information in the
              <emphasis>sctype</emphasis> not represented in a
              <literal>Col</literal> is ignored.</para>
            </section>

            <section>
              <title>from_type(type, shape=1, dflt=None, pos=None)</title>

              <para>Create a <literal>Col</literal> definition from a PyTables
              <emphasis>type</emphasis>.</para>

              <para>Optional shape, default value and position may be
              specified as the <emphasis>shape</emphasis>,
              <emphasis>dflt</emphasis> and <emphasis>pos</emphasis>
              arguments, respectively.</para>
            </section>
          </section>

          <section>
            <title><literal>Col</literal> constructors</title>

            <para>For each <literal><varname>TYPE</varname>Atom</literal>
            class there is a matching
            <literal><varname>TYPE</varname>Col</literal> class with the same
            constructor signature, plus an additional <literal>pos</literal>
            parameter, which defaults to <literal>None</literal>. By default,
            columns are arranged in memory following an alphanumeric order on
            column names. However, it is convenient to let the user impose a
            defined ordering. The <literal>pos</literal> parameter allows the
            user to force the desired ordering.</para>
          </section>
        </section>

        <section id="AtomClassDescr">
          <title>The <literal>Atom</literal> class and its
          descendants.</title>

          <para><literal>Atom</literal> instances define the type of atomic
          cells stored in a dataset. The meaning of
          <emphasis>atomic</emphasis> is that individual elements of a cell
          can not be extracted directly by indexing (i.e.
          <literal>__getitem__()</literal>) the dataset; e.g. if a dataset has
          shape (2, 2) and its atoms have shape (3,), to get the third element
          of the cell at (1, 0) one should use
          <literal>dataset[1,0][2]</literal> instead of
          <literal>dataset[1,0,2]</literal>.</para>

          <para>The <literal>Atom</literal> class is meant to declare the
          different properties of the <emphasis>base element</emphasis> (also
          known as <emphasis>atom</emphasis>) of <literal>CArray</literal>,
          <literal>EArray</literal> and <literal>VLArray</literal> objects,
          although they are used also so as to describe the base elements of
          <literal>Array</literal> objects as well. <literal>Atom</literal>
          instances have the property that their length is always the same.
          However, you can grow objects along the extensible dimension in the
          case of <literal>EArray</literal> or put a variable number of them
          on a <literal>VLArray</literal> row. Moreover, the atoms are not
          restricted to scalar values, and they can be fully multidimensional
          objects.</para>

          <para>A series of descendant classes are offered in order to make
          the use of these element descriptions easier. You should use a
          particular <literal>Atom</literal> descendant class whenever you
          know the exact type you will need when writing your code. Otherwise,
          you may use one of the <literal>Atom.from_*()</literal> factory
          methods.</para>

          <section>
            <title><literal>Atom</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">kind</emphasis></glossterm>

                <glossdef>
                  <para>The PyTables kind of the atom (a string). For a
                  relation of the data kinds supported by PyTables and more
                  information about them, see the <xref
                  linkend="datatypesSupported"
                  xrefstyle="select: label" />.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">type</emphasis></glossterm>

                <glossdef>
                  <para>The PyTables type of the atom (a string). For a
                  relation of the data types supported by PyTables and more
                  information about them, see the <xref
                  linkend="datatypesSupported"
                  xrefstyle="select: label" />.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the atom (a tuple, <literal>()</literal>
                  for scalar atoms). In a <literal>EArray</literal> context,
                  it is a <emphasis>tuple</emphasis> specifying the shape of
                  the object, and one (and only one) of its dimensions
                  <emphasis>must</emphasis> be 0, meaning that the
                  <literal>EArray</literal> object will be enlarged along this
                  axis.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dflt</emphasis></glossterm>

                <glossdef>
                  <para>The default value of the atom. If the user does not
                  supply a value for an element while filling a dataset, this
                  default value will be written to disk. If the user supplies
                  a scalar value for a multidimensional atom, this value is
                  automatically <emphasis>broadcast</emphasis> to all the
                  items in the atom cell. If <emphasis>dflt</emphasis> is not
                  supplied, an appropriate zero value (or
                  <emphasis>null</emphasis> string) will be chosen by default.
                  Please note that default values are kept internally as NumPy
                  objects.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">size</emphasis></glossterm>

                <glossdef>
                  <para>Total size in bytes of the atom.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">itemsize</emphasis></glossterm>

                <glossdef>
                  <para>Size in bytes of a sigle item in the atom. Specially
                  useful for <literal>string</literal> objects.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dtype</emphasis></glossterm>

                <glossdef>
                  <para>The NumPy <literal>dtype</literal> that most closely
                  matches this atom.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">recarrtype</emphasis></glossterm>

                <glossdef>
                  <para>String type to be used in
                  <literal>numpy.rec.array()</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Atoms can be compared with atoms and other objects for
            strict (in)equality without having to compare individual
            attributes:</para>

            <screen>&gt;&gt;&gt; atom1 = StringAtom(itemsize=10)  # same as ``atom2``
&gt;&gt;&gt; atom2 = Atom.from_kind('string', 10)  # same as ``atom1``
&gt;&gt;&gt; atom3 = IntAtom()
&gt;&gt;&gt; atom1 == 'foo'
False
&gt;&gt;&gt; atom1 == atom2
True
&gt;&gt;&gt; atom2 != atom1
False
&gt;&gt;&gt; atom1 == atom3
False
&gt;&gt;&gt; atom3 != atom2
True</screen>
          </section>

          <section>
            <title><literal>Atom</literal> methods</title>

            <section>
              <title>copy(**override)</title>

              <para>Get a copy of the atom, possibly overriding some
              arguments.</para>

              <para>Constructor arguments to be overridden must be passed as
              keyword arguments.</para>

              <screen>&gt;&gt;&gt; atom1 = StringAtom(itemsize=12)
&gt;&gt;&gt; atom2 = atom1.copy()
&gt;&gt;&gt; print atom1
StringAtom(itemsize=12, shape=(), dflt='')
&gt;&gt;&gt; print atom2
StringAtom(itemsize=12, shape=(), dflt='')
&gt;&gt;&gt; atom1 is atom2
False
&gt;&gt;&gt; atom3 = atom1.copy(itemsize=100, shape=(2, 2))
&gt;&gt;&gt; print atom3
StringAtom(itemsize=100, shape=(2, 2), dflt='')
&gt;&gt;&gt; atom1.copy(foobar=42)
Traceback (most recent call last):
  ...
TypeError: __init__() got an unexpected keyword argument 'foobar'</screen>
            </section>
          </section>

          <section>
            <title><literal>Atom</literal> factory methods</title>

            <section>
              <title>from_dtype(dtype, dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a NumPy
              <emphasis>dtype</emphasis>.</para>

              <para>An optional default value may be specified as the
              <emphasis>dflt</emphasis> argument. Information in the
              <emphasis>dtype</emphasis> not represented in an
              <literal>Atom</literal> is ignored.</para>

              <screen>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; Atom.from_dtype(numpy.dtype((numpy.int16, (2, 2))))
Int16Atom(shape=(2, 2), dflt=0)
&gt;&gt;&gt; Atom.from_dtype(numpy.dtype('S5'), dflt='hello')
StringAtom(itemsize=5, shape=(), dflt='hello')
&gt;&gt;&gt; Atom.from_dtype(numpy.dtype('Float64'))
Float64Atom(shape=(), dflt=0.0)</screen>
            </section>

            <section>
              <title>from_kind(kind, itemsize=None, shape=1,
              dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a PyTables
              <emphasis>kind</emphasis>.</para>

              <para>Optional item size, shape and default value may be
              specified as the <emphasis>itemsize</emphasis>,
              <emphasis>shape</emphasis> and <emphasis>dflt</emphasis>
              arguments, respectively. Bear in mind that not all atoms support
              a default item size.</para>

              <screen>&gt;&gt;&gt; Atom.from_kind('int', itemsize=2, shape=(2, 2))
Int16Atom(shape=(2, 2), dflt=0)
&gt;&gt;&gt; Atom.from_kind('int', shape=(2, 2))
Int32Atom(shape=(2, 2), dflt=0)
&gt;&gt;&gt; Atom.from_kind('string', itemsize=5, dflt='hello')
StringAtom(itemsize=5, shape=(), dflt='hello')
&gt;&gt;&gt; Atom.from_kind('string', dflt='hello')
Traceback (most recent call last):
  ...
ValueError: no default item size for kind ``string``
&gt;&gt;&gt; Atom.from_kind('Float')
Traceback (most recent call last):
  ...
ValueError: unknown kind: 'Float'</screen>

              <para>Moreover, some kinds with atypical constructor signatures
              are not supported; you need to use the proper
              constructor:</para>

              <screen>&gt;&gt;&gt; Atom.from_kind('enum')
Traceback (most recent call last):
  ...
ValueError: the ``enum`` kind is not supported...</screen>
            </section>

            <section>
              <title>from_sctype(sctype, shape=1, dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a NumPy scalar type
              <emphasis>sctype</emphasis>.</para>

              <para>Optional shape and default value may be specified as the
              <emphasis>shape</emphasis> and <emphasis>dflt</emphasis>
              arguments, respectively. Information in the
              <emphasis>sctype</emphasis> not represented in an
              <literal>Atom</literal> is ignored.</para>

              <screen>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; Atom.from_sctype(numpy.int16, shape=(2, 2))
Int16Atom(shape=(2, 2), dflt=0)
&gt;&gt;&gt; Atom.from_sctype('S5', dflt='hello')
Traceback (most recent call last):
  ...
ValueError: unknown NumPy scalar type: 'S5'
&gt;&gt;&gt; Atom.from_sctype('Float64')
Float64Atom(shape=(), dflt=0.0)</screen>
            </section>

            <section>
              <title>from_type(type, shape=1, dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a PyTables
              <emphasis>type</emphasis>.</para>

              <para>Optional shape and default value may be specified as the
              <emphasis>shape</emphasis> and <emphasis>dflt</emphasis>
              arguments, respectively.</para>

              <screen>
&gt;&gt;&gt; Atom.from_type('bool')
BoolAtom(shape=(), dflt=False)
&gt;&gt;&gt; Atom.from_type('int16', shape=(2, 2))
Int16Atom(shape=(2, 2), dflt=0)
&gt;&gt;&gt; Atom.from_type('string40', dflt='hello')
Traceback (most recent call last):
  ...
ValueError: unknown type: 'string40'
&gt;&gt;&gt; Atom.from_type('Float64')
Traceback (most recent call last):
  ...
ValueError: unknown type: 'Float64'</screen>
            </section>
          </section>

          <section>
            <title><literal>Atom</literal> constructors</title>

            <para>There are some common arguments for most
            <literal>Atom</literal>-derived constructors:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">itemsize</emphasis></glossterm>

                <glossdef>
                  <para>For types with a non-fixed size, this sets the size in
                  bytes of individual items in the atom.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>Sets the shape of the atom. An integer shape like
                  <literal>2</literal> is equivalent to the tuple
                  <literal>(2,)</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dflt</emphasis></glossterm>

                <glossdef>
                  <para>Sets the default value for the atom.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>A relation of the different constructors with their
            parameters follows.</para>

            <section>
              <title>StringAtom(itemsize, shape=1, dflt='')</title>

              <para>Defines an atom of type <literal>string</literal>.</para>

              <para>The item size is the <emphasis>maximum</emphasis> length
              in characters of strings.</para>
            </section>

            <section>
              <title>BoolAtom(shape=1, dflt=False)</title>

              <para>Defines an atom of type <literal>bool</literal>.</para>
            </section>

            <section>
              <title>IntAtom(itemsize=4, shape=1, dflt=0)</title>

              <para>Defines an atom of a signed integral type
              (<literal>int</literal> kind).</para>
            </section>

            <section>
              <title>Int8Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>int8</literal>.</para>
            </section>

            <section>
              <title>Int16Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>int16</literal>.</para>
            </section>

            <section>
              <title>Int32Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>int32</literal>.</para>
            </section>

            <section>
              <title>Int64Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>int64</literal>.</para>
            </section>

            <section>
              <title>UIntAtom(itemsize=4, shape=1, dflt=0)</title>

              <para>Defines an atom of an unsigned integral type
              (<literal>uint</literal> kind).</para>
            </section>

            <section>
              <title>UInt8Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>uint8</literal>.</para>
            </section>

            <section>
              <title>UInt16Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>uint16</literal>.</para>
            </section>

            <section>
              <title>UInt32Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>uint32</literal>.</para>
            </section>

            <section>
              <title>UInt64Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>uint64</literal>.</para>
            </section>

            <section>
              <title>Float32Atom(shape=1, dflt=0.0)</title>

              <para>Defines an atom of type <literal>float32</literal>.</para>
            </section>

            <section>
              <title>Float64Atom(shape=1, dflt=0.0)</title>

              <para>Defines an atom of type <literal>float64</literal>.</para>
            </section>

            <section>
              <title>ComplexAtom(itemsize, shape=1, dflt=0j)</title>

              <para>Defines an atom of kind <literal>complex</literal>.
              Allowed item sizes are 8 (single precision) and 16 (double
              precision). This class must be used instead of more concrete
              ones to avoid confusions with <literal>numarray</literal>-like
              precision specifications used in PyTables 1.X.</para>
            </section>

            <section>
              <title>TimeAtom(itemsize=4, shape=1, dflt=0)</title>

              <para>Defines an atom of time type (<literal>time</literal>
              kind).</para>

              <para>There are two distinct supported types of time: a 32 bit
              integer value and a 64 bit floating point value. Both of them
              reflect the number of seconds since the Unix epoch. This atom
              has the property of being stored using the HDF5 time
              datatypes.</para>
            </section>

            <section>
              <title>Time32Atom(shape=1, dflt=0)</title>

              <para>Defines an atom of type <literal>time32</literal>.</para>
            </section>

            <section>
              <title>Time64Atom(shape=1, dflt=0.0)</title>

              <para>Defines an atom of type <literal>time64</literal>.</para>
            </section>

            <section>
              <title>EnumAtom(enum, dflt, base, shape=1)</title>

              <para>Description of an atom of an enumerated type.</para>

              <para>Instances of this class describe the atom type used to
              store enumerated values. Those values belong to an enumerated
              type, defined by the first argument (<literal>enum</literal>) in
              the constructor of the atom, which accepts the same kinds of
              arguments as the <literal>Enum</literal> class (see <xref
              linkend="EnumClassDescr" xrefstyle="select: label" />).  The
              enumerated type is stored in the <literal>enum</literal>
              attribute of the atom.</para>

              <para>A default value must be specified as the second argument
              (<literal>dflt</literal>) in the constructor; it must be the
              <emphasis>name</emphasis> (a string) of one of the enumerated
              values in the enumerated type. When the atom is created, the
              corresponding concrete value is broadcast and stored in the
              <literal>dflt</literal> attribute (setting different default
              values for items in a multidimensional atom is not supported
              yet). If the name does not match any value in the enumerated
              type, a <literal>KeyError</literal> is raised.</para>

              <para>Another atom must be specified as the
              <literal>base</literal> argument in order to determine the base
              type used for storing the values of enumerated values in memory
              and disk. This <emphasis>storage atom</emphasis> is kept in the
              <literal>base</literal> attribute of the created atom. As a
              shorthand, you may specify a PyTables type instead of the
              storage atom, implying that this has a scalar shape.</para>

              <para>The storage atom should be able to represent each and
              every concrete value in the enumeration. If it is not, a
              <literal>TypeError</literal> is raised. The default value of the
              storage atom is ignored.</para>

              <para>The <literal>type</literal> attribute of enumerated atoms
              is always <literal>enum</literal>.</para>

              <para>Enumerated atoms also support comparisons with other
              objects:</para>

              <screen>&gt;&gt;&gt; enum = ['T0', 'T1', 'T2']
&gt;&gt;&gt; atom1 = EnumAtom(enum, 'T0', 'int8')  # same as ``atom2``
&gt;&gt;&gt; atom2 = EnumAtom(enum, 'T0', Int8Atom())  # same as ``atom1``
&gt;&gt;&gt; atom3 = EnumAtom(enum, 'T0', 'int16')
&gt;&gt;&gt; atom4 = Int8Atom()
&gt;&gt;&gt; atom1 == enum
False
&gt;&gt;&gt; atom1 == atom2
True
&gt;&gt;&gt; atom2 != atom1
False
&gt;&gt;&gt; atom1 == atom3
False
&gt;&gt;&gt; atom1 == atom4
False
&gt;&gt;&gt; atom4 != atom1
True</screen>

              <section>
                <title>Examples</title>

                <para>The next C <literal>enum</literal> construction:</para>

                <screen>enum myEnum {
  T0,
  T1,
  T2
};</screen>

                <para>would correspond to the following PyTables
                declaration:</para>

                <screen>&gt;&gt;&gt; myEnumAtom = EnumAtom(['T0', 'T1', 'T2'], 'T0', 'int32')</screen>

                <para>Please note the <literal>dflt</literal> argument with a
                value of <literal>'T0'</literal>. Since the concrete value
                matching <literal>T0</literal> is unknown right now (we have
                not used explicit concrete values), using the name is the only
                option left for defining a default value for the atom.</para>

                <para>The chosen representation of values for this enumerated
                atom uses unsigned 32-bit integers, which surely wastes quite
                a lot of memory. Another size could be selected by using the
                <literal>base</literal> argument (this time with a full-blown
                storage atom):</para>

                <screen>&gt;&gt;&gt; myEnumAtom = EnumAtom(['T0', 'T1', 'T2'], 'T0', UInt8Atom())</screen>

                <para>You can also define multidimensional arrays for data
                elements:</para>

                <screen>&gt;&gt;&gt; myEnumAtom = EnumAtom(
...    ['T0', 'T1', 'T2'], 'T0', base='uint32', shape=(3,2))</screen>

                <para>for 3x2 arrays of <literal>uint32</literal>.</para>
              </section>
            </section>

            <!-- You can place paragraphs and other block elements before a section, but you cannot place anything after it. See docbook reference. -->

            <section id="PseudoAtomsDescr">
              <title>Pseudo atoms</title>

              <para>Now, there come two special classes,
              <literal>ObjectAtom</literal> and
              <literal>VLStringAtom</literal>, that actually do not descend
              from <literal>Atom</literal>, but which goal is so similar that
              they should be described here. Pseudo atom instances do not
              allow multidimensional atoms, nor multiple values per
              row.</para>

              <para>They can be recognised because they also have
              <literal>kind</literal>, <literal>type</literal> and
              <literal>shape</literal> attributes, but no
              <literal>size</literal>, <literal>itemsize</literal> or
              <literal>dflt</literal> ones. Instead, they have a
              <literal>base</literal> atom which defines the elements used for
              storage.</para>

              <para><emphasis>Caveat emptor:</emphasis> You are only allowed
              to use these classes to create <literal>VLArray</literal>
              objects, not <literal>CArray</literal> and
              <literal>EArray</literal> objects.</para>

              <section>
                <title>ObjectAtom()</title>

                <para>This class is meant to fit <emphasis>any</emphasis> kind
                of object in a row of an <literal>VLArray</literal> instance
                by using <literal>cPickle</literal> behind the scenes. Due to
                the fact that you can not foresee how long will be the output
                of the <literal>cPickle</literal> serialization (i.e. the atom
                already has a <emphasis>variable</emphasis> length), you can
                only fit a representant of it per row. However, you can still
                pass several parameters to the
                <literal>VLArray.append()</literal> method as they will be
                regarded as a <emphasis>tuple</emphasis> of compound objects
                (the parameters), so that we still have only one object to be
                saved in a single row. It does not accept parameters and it
                causes the reads of rows to always return Python objects. You
                can regard <literal>ObjectAtom</literal> types as an easy way
                to save an arbitrary number of generic python objects in a
                <literal>VLArray</literal> object.</para>
              </section>

              <section>
                <title>VLStringAtom()</title>

                <para>This class describes a <emphasis>row</emphasis> of the
                <literal>VLArray</literal> class, rather than an
                <emphasis>atom</emphasis>. It differs from the
                <literal>StringAtom</literal> class in that you can only add
                one instance of it to one specific row, i.e. the
                <literal>VLArray.append()</literal> method only accepts one
                object when the base atom is of this type. Besides, it
                supports Unicode strings (contrarily to
                <literal>StringAtom</literal>) because it uses the UTF-8
                codification (this is why its <literal>atomsize()</literal>
                method returns always 1) when serializing to disk. It does not
                accept any parameter and it causes the reads of rows to always
                return Python strings. See the <xref
                linkend="VLArrayFormatDescr" xrefstyle="select: label" /> if
                you are curious on how this is implemented at the low-level.
                You can regard <literal>VLStringAtom</literal> types as an
                easy way to save generic variable length strings.</para>
              </section>
            </section>

            <!-- You can place paragraphs and other block elements before a section, but you cannot place anything after it. See docbook reference. -->

            <section>
              <title></title>

              <para>See <literal>examples/vlarray1.py</literal> and
              <literal>examples/vlarray2.py</literal> for further examples on
              <literal>VLArray</literal>s, including object serialization and
              Unicode string management.</para>
            </section>
          </section>
        </section>
      </section>

      <section id="helperClasses">
        <title>Helper classes</title>

        <para>In this section are listed classes that does not fit in any
        other section and that mainly serve for ancillary purposes.</para>

        <section id="FiltersClassDescr">
          <title>The <literal>Filters</literal> class</title>

          <para>This class is meant to serve as a container that keeps
          information about the filter properties associated with the chunked
          leaves, that is <literal>Table</literal>, <literal>CArray</literal>,
          <literal>EArray</literal> and <literal>VLArray</literal>.</para>

          <para>The public variables of <literal>Filters</literal> are listed
          below:</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">complevel</emphasis></glossterm>

              <glossdef>
                <para>The compression level (0 disables compression).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">complib</emphasis></glossterm>

              <glossdef>
                <para>The compression filter used (irrelevant when compression
                is not enabled).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">shuffle</emphasis></glossterm>

              <glossdef>
                <para>Whether the <emphasis>Shuffle</emphasis> filter is
                active or not.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">fletcher32</emphasis></glossterm>

              <glossdef>
                <para>Whether the <emphasis>Fletcher32</emphasis> filter is
                active or not.</para>
              </glossdef>
            </glossentry>
          </glosslist>

          <para>The arguments for the <literal>Filters</literal> constructor
          are described next:</para>

          <section id="FiltersInitDescr">
            <title><literal>Filters(complevel=0, complib="zlib", shuffle=True,
            fletcher32=False)</literal></title>

            <para>The constructor for the <literal>Filters</literal> class and
            its only method are described next.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">complevel</emphasis></glossterm>

                <glossdef>
                  <para>Specifies a compression level for data. The allowed
                  range is 0-9. A value of 0 (the default) disables
                  compression.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">complib</emphasis></glossterm>

                <glossdef>
                  <para>Specifies the compression library to be used. Right
                  now, 'zlib' (the default), 'lzo' and 'bzip2' are supported.
                  Specifying a compression library which is not available in
                  the system issues a <literal>FiltersWarning</literal> and
                  sets the library to the default one.</para>

                  <para>See <xref linkend="compressionIssues"
                  xrefstyle="select: label" /> for some advice on which
                  library is better suited to your needs.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">shuffle</emphasis></glossterm>

                <glossdef>
                  <para>Whether or not to use the <emphasis>Shuffle</emphasis>
                  filter in the HDF5 library. This is normally used to improve
                  the compression ratio. A false value disables shuffling and
                  a true one enables it. The default value depends on whether
                  compression is enabled or not; if compression is enabled,
                  shuffling defaults to be enabled, else shuffling is
                  disabled. Shuffling can only be used when compression is
                  enabled.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">fletcher32</emphasis></glossterm>

                <glossdef>
                  <para>Whether or not to use the
                  <emphasis>Fletcher32</emphasis> filter in the HDF5 library.
                  This is used to add a checksum on each data chunk. A false
                  value (the default) disables the checksum.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>This is a small example on using the
            <literal>Filters</literal> class: <screen>import numpy
from tables import *

fileh = openFile("test5.h5", mode="w")
atom = Float32Atom()
filters = Filters(complevel=1, complib="lzo", fletcher32=True)
arr = fileh.createEArray(fileh.root, 'earray', atom, (0,2), "A growable array",
                         filters = filters)
# Append several rows in only one call
arr.append(numpy.array([[1., 2.],
                       [2., 3.],
                       [3., 4.]], dtype=numpy.float32))

# Print information on that enlargeable array
print "Result Array:"
print repr(arr)

fileh.close()</screen>This enforces the use of the <literal>LZO</literal>
            library, a compression level of 1 and a fletcher32 checksum filter
            as well. See the output of this example: <screen>Result Array:
/earray (EArray(3L, 2), fletcher32, shuffle, lzo(1)) 'A growable array'
  type = float32
  shape = (3L, 2)
  itemsize = 4
  nrows = 3
  extdim = 0
  flavor = 'numpy'
  byteorder = 'little'</screen></para>
          </section>

          <section>
            <title><literal>copy(override)</literal></title>

            <para>Get a copy of the filters, possibly overriding some
            arguments.</para>

            <para>Constructor arguments to be overridden must be passed as
            keyword arguments.</para>

            <para>Using this method is recommended over replacing the
            attributes of an instance, since instances of this class may
            become immutable in the future.</para>

            <screen>&gt;&gt;&gt; filters1 = Filters()
&gt;&gt;&gt; filters2 = filters1.copy()
&gt;&gt;&gt; filters1 == filters2
True
&gt;&gt;&gt; filters1 is filters2
False
&gt;&gt;&gt; filters3 = filters1.copy(complevel=1)
Traceback (most recent call last):
  ...
ValueError: compression library ``None`` is not supported...
&gt;&gt;&gt; filters3 = filters1.copy(complevel=1, complib='zlib')
&gt;&gt;&gt; print filters1
Filters(complevel=0, shuffle=False, fletcher32=False)
&gt;&gt;&gt; print filters3
Filters(complevel=1, complib='zlib', shuffle=False, fletcher32=False)
&gt;&gt;&gt; filters1.copy(foobar=42)
Traceback (most recent call last):
  ...
TypeError: __init__() got an unexpected keyword argument 'foobar'</screen>
          </section>
        </section>

        <section id="IndexPropsClassDescr">
          <title>The <literal>IndexProps</literal> class</title>

          <para>You can use this class to set/unset the properties in the
          indexing process of a <literal>Table</literal> column. To use it,
          create an instance, and assign it to the special attribute
          <literal>_v_indexprops</literal> in a table description class (see
          <xref linkend="IsDescriptionClassDescr" xrefstyle="select: label"
          />) or dictionary.</para>

          <para>The public variables of <literal>IndexProps</literal> are
          listed below:</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">auto</emphasis></glossterm>

              <glossdef>
                <para>Whether an existing index should be automatically
                updated after a table append operation or reindexed after an
                index-invalidating operation.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">filters</emphasis></glossterm>

              <glossdef>
                <para>The filter settings for the different
                <literal>Table</literal> indexes.</para>
              </glossdef>
            </glossentry>
          </glosslist>

          <para>There are no <literal>IndexProps</literal> public methods with
          the exception of the constructor itself that is described
          next.</para>
        </section>

        <section id="IndexClassDescr">
          <title>The <literal>Index</literal> class</title>

          <para>This class is used to keep the indexing information for table
          columns. It is actually a descendant of the <literal>Group</literal>
          class, with some added functionality.</para>

          <para>It has no methods intended for programmer's use, but it has
          some attributes that may be interesting for him.</para>

          <section id="IndexClassInstanceVariables">
            <title><literal>Index</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">column</emphasis></glossterm>

                <glossdef>
                  <para>The column object this index belongs to.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">type</emphasis></glossterm>

                <glossdef>
                  <para>The type class for the index.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">itemsize</emphasis></glossterm>

                <glossdef>
                  <para>The size of the atomic items. Specially useful for
                  columns of <literal>CharType</literal> type.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">nelements</emphasis></glossterm>

                <glossdef>
                  <para>The total number of elements in index.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dirty</emphasis></glossterm>

                <glossdef>
                  <para>Whether the index is dirty or not. Dirty indexes are
                  out of sync with column data, so they exist but they are not
                  usable.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>The <literal>Filters</literal> (see <xref
                  linkend="FiltersClassDescr" xrefstyle="select: label" />)
                  instance for this index.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>
        </section>

        <section id="EnumClassDescr">
          <title>The <literal>Enum</literal> class</title>

          <para>Each instance of this class represents an enumerated type. The
          values of the type must be declared
          <emphasis>exhaustively</emphasis> and named with
          <emphasis>strings</emphasis>, and they might be given explicit
          concrete values, though this is not compulsory. Once the type is
          defined, it can not be modified.</para>

          <para>There are three ways of defining an enumerated type. Each one
          of them corresponds to the type of the only argument in the
          constructor of <literal>Enum</literal>:</para>

          <itemizedlist>
            <listitem>
              <para><emphasis>Sequence of names</emphasis>: each enumerated
              value is named using a string, and its order is determined by
              its position in the sequence; the concrete value is assigned
              automatically:</para>

              <screen>&gt;&gt;&gt; boolEnum = Enum(['True', 'False'])</screen>
            </listitem>

            <listitem>
              <para><emphasis>Mapping of names</emphasis>: each enumerated
              value is named by a string and given an explicit concrete value.
              All of the concrete values must be different, or a
              <literal>ValueError</literal> will be raised.</para>

              <screen>&gt;&gt;&gt; priority = Enum({'red': 20, 'orange': 10, 'green': 0})
&gt;&gt;&gt; colors = Enum({'red': 1, 'blue': 1})
Traceback (most recent call last):
  ...
ValueError: enumerated values contain duplicate concrete values: 1</screen>
            </listitem>

            <listitem>
              <para><emphasis>Enumerated type</emphasis>: in that case, a copy
              of the original enumerated type is created. Both enumerated
              types are considered equal.</para>

              <screen>&gt;&gt;&gt; prio2 = Enum(priority)
&gt;&gt;&gt; priority == prio2
True</screen>
            </listitem>
          </itemizedlist>

          <para>Please note that names starting with <literal>_</literal> are
          not allowed, since they are reserved for internal usage:</para>

          <screen>&gt;&gt;&gt; prio2 = Enum(['_xx'])
Traceback (most recent call last):
  ...
ValueError: name of enumerated value can not start with ``_``: '_xx'</screen>

          <para>The concrete value of an enumerated value is obtained by
          getting its name as an attribute of the <literal>Enum</literal>
          instance (see <literal>__getattr__()</literal>) or as an item (see
          <literal>__getitem__()</literal>). This allows comparisons between
          enumerated values and assigning them to ordinary Python
          variables:</para>

          <screen>&gt;&gt;&gt; redv = priority.red
&gt;&gt;&gt; redv == priority['red']
True
&gt;&gt;&gt; redv &gt; priority.green
True
&gt;&gt;&gt; priority.red == priority.orange
False</screen>

          <para>The name of the enumerated value corresponding to a concrete
          value can also be obtained by using the
          <literal>__call__()</literal> method of the enumerated type. In this
          way you get the symbolic name to use it later with
          <literal>__getitem__()</literal>:</para>

          <screen>&gt;&gt;&gt; priority(redv)
'red'
&gt;&gt;&gt; priority.red == priority[priority(priority.red)]
True</screen>

          <para>(If you ask, the <literal>__getitem__()</literal> method is
          not used for this purpose to avoid ambiguity in the case of using
          strings as concrete values.)</para>

          <section>
            <title>Special methods</title>

            <section>
              <title>__getitem__(name)</title>

              <para>Get the concrete value of the enumerated value with that
              <literal>name</literal>.</para>

              <para>The <literal>name</literal> of the enumerated value must
              be a string. If there is no value with that
              <literal>name</literal> in the enumeration, a
              <literal>KeyError</literal> is raised.</para>
            </section>

            <section>
              <title>__getattr__(name)</title>

              <para>Get the concrete value of the enumerated value with that
              <literal>name</literal>.</para>

              <para>The <literal>name</literal> of the enumerated value must
              be a string. If there is no value with that
              <literal>name</literal> in the enumeration, an
              <literal>AttributeError</literal> is raised.</para>
            </section>

            <section>
              <title>__contains__(name)</title>

              <para>Is there an enumerated value with that
              <literal>name</literal> in the type?</para>

              <para>If the enumerated type has an enumerated value with that
              <literal>name</literal>, <literal>True</literal> is returned.
              Otherwise, <literal>False</literal> is returned. The
              <literal>name</literal> must be a string.</para>

              <para>This method does <emphasis>not</emphasis> check for
              concrete values matching a value in an enumerated type. For
              that, please use the <literal>__call__()</literal>
              method.</para>
            </section>

            <section>
              <title>__call__(value, *default)</title>

              <para>Get the name of the enumerated value with that concrete
              <literal>value</literal>.</para>

              <para>If there is no value with that concrete value in the
              enumeration and a second argument is given as a
              <literal>default</literal>, this is returned. Else, a
              <literal>ValueError</literal> is raised.</para>

              <para>This method can be used for checking that a concrete value
              belongs to the set of concrete values in an enumerated
              type.</para>
            </section>

            <section>
              <title>__len__()</title>

              <para>Return the number of enumerated values in the enumerated
              type.</para>
            </section>

            <section>
              <title>__iter__()</title>

              <para>Iterate over the enumerated values.</para>

              <para>Enumerated values are returned as <literal>(name,
              value)</literal> pairs <emphasis>in no particular
              order</emphasis>.</para>
            </section>

            <section>
              <title>__eq__(other)</title>

              <para>Is the <literal>other</literal> enumerated type equivalent
              to this one?</para>

              <para>Two enumerated types are equivalent if they have exactly
              the same enumerated values (i.e. with the same names and
              concrete values).</para>
            </section>

            <section>
              <title>__repr__()</title>

              <para>Return the canonical string representation of the
              enumeration. The output of this method can be evaluated to give
              a new enumeration object that will compare equal to this
              one.</para>
            </section>
          </section>
        </section>
      </section>
    </chapter>

    <chapter id="optimizationTips">
      <title>Optimization tips</title>

      <epigraph>
        <attribution>Johann Karl Friedrich Gauss <citetitle>[asked how he came
        upon his theorems]</citetitle></attribution>

        <literallayout>
      ... durch planmässiges Tattonieren.
      [... through systematic, palpable experimentation.]
      </literallayout>
      </epigraph>

      <para></para>

      <para>On this chapter, you will get deeper knowledge of PyTables
      internals. PyTables has several places where the user can improve the
      performance of his application. If you are planning to deal with really
      large data, you should read carefully this section in order to learn how
      to get an important efficiency boost for your code. But if your dataset
      is small or medium size (say, up to 10 MB), you should not worry about
      that as the default parameters in PyTables are already tuned to handle
      that perfectly.</para>

      <section id="expectedRowsOptim">
        <title>Informing PyTables about expected number of rows in
        tables</title>

        <para>The underlying HDF5 library that is used by PyTables allows for
        certain datasets (<emphasis>chunked</emphasis> datasets) to take the
        data in bunches of a certain length, so-called
        <emphasis>chunks</emphasis>, to write them on disk as a whole, i.e.
        the HDF5 library treats chunks as atomic objects and disk I/O is
        always made in terms of complete chunks. This allows data filters to
        be defined by the application to perform tasks such as compression,
        encryption, checksumming, etc. on entire chunks.</para>

        <para>An in-memory B-tree is used to map chunk structures on disk. The
        more chunks that are allocated for a dataset the larger the B-tree.
        Large B-trees take memory and cause file storage overhead as well as
        more disk I/O and higher contention for the metadata cache.
        Consequently, it's important to balance between memory and I/O
        overhead (small B-trees) and time to access data (big B-trees).</para>

        <para>PyTables can determine an optimum chunk size to make B-trees
        adequate to your dataset size if you help it by providing an
        estimation of the number of rows for a table. This must be made at
        table creation time by passing this value to the
        <literal>expectedrows</literal> keyword of the
        <literal>createTable</literal> method (see <xref
        linkend="createTableDescr"
        xrefstyle="select:         label" />).</para>

        <para>When your table size is bigger than 10 MB (take this figure only
        as a reference, not strictly), by providing this guess of the number
        of rows you will be optimizing the access to your data. When the table
        size is larger than, say 100MB, you are <emphasis>strongly</emphasis>
        suggested to provide such a guess; failing to do that may cause your
        application to do very slow I/O operations and to demand
        <emphasis>huge</emphasis> amounts of memory. You have been
        warned!</para>
      </section>

      <section id="searchOptim">
        <title>Accelerating your searches</title>

        <para>If you are going to use a lot of searches like the next one:
        <screen>
row = table.row
result = [row['var2'] for row in table if row['var1'] &lt;= 20 ]</screen> (for
        future reference, we will call this the <emphasis>standard</emphasis>
        selection mode) and you want to improve the time taken to run it, keep
        reading.</para>

        <section id="inkernelSearch">
          <title>In-kernel searches</title>

          <para>PyTables provides a way to accelerate data selections relating
          to a single table, through the use of the <literal>where</literal>
          iterator (see <xref linkend="Table.where" />). We will call this
          mode of selecting data <emphasis>in-kernel</emphasis>. Let's see an
          example of <emphasis>in-kernel</emphasis> selection based on the
          <emphasis>standard</emphasis> selection mentioned above: <screen>
row = table
result = [ row['var2'] for row in table.where('var1 &lt;= 20')]</screen> This
          simple change of mode selection can account for an improvement in
          search times up to a factor of 10 (see the <xref
          linkend="searchTimes-int" xrefstyle="select: label" />).</para>

          <figure id="searchTimes-int">
            <title>Times for different selection modes over
            <literal>Int32</literal> values. Benchmark made on a machine with
            Itanium (IA64) @ 900 MHz processors with SCSI disk @ 10K
            RPM.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="searchTimes-int-itanium.svg" format="SVG"
                           scale="50" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="searchTimes-int-itanium.png" format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="searchTimes-float">
            <title>Times for different selection modes over
            <literal>Float64</literal> values. Benchmark made on a machine
            with Itanium (IA64) @ 900 MHz processors with SCSI disk @ 10K
            RPM.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="searchTimes-float-itanium.svg"
                           format="SVG" scale="50" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="searchTimes-float-itanium.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>So, where is the trick? It's easy. In the
          <emphasis>standard</emphasis> selection mode the data for column
          <literal>var1</literal> has to be carried up to Python space so as
          to evaluate the condition and decide if the <literal>var2</literal>
          value should be added to the <literal>result</literal> list. On the
          contrary, in the <emphasis>in-kernel</emphasis> mode, the
          <emphasis>condition</emphasis> is passed to the PyTables kernel
          (hence the name), written in C, and evaluated there at C speed (with
          some help of the NumPy package), so that the only values that are
          brought to the Python space are the references for
          <literal>rows</literal> that fulfilled the condition.</para>

          <para>You can mix the <emphasis>in-kernel</emphasis> and
          <emphasis>standard</emphasis> selection modes for evaluating
          arbitrarily complex conditions making use of external functions.
          Look at this example: <screen>
row = table
result = [ row['var2']
           for row in table.where('(var3 == "foo") &amp; (var1 &lt;= 20)')
           if your_function(row['var2']) ]</screen> Here, we use an
          <emphasis>in-kernel</emphasis> selection to choose rows according to
          the values of the <literal>var3</literal> and
          <literal>var1</literal> fields. Then, we apply a
          <emphasis>standard</emphasis> selection to complete the query. Of
          course, when you mix the <emphasis>in-kernel</emphasis> and
          <emphasis>standard</emphasis> selection modes you should pass the
          most restrictive condition to the <emphasis>in-kernel</emphasis>
          part, i.e. to the <literal>where</literal> iterator. In situations
          where it is not clear which is the most restrictive condition, you
          might want to experiment a bit in order to find the best
          combination.</para>

          <para>However, since in-kernel condition strings allow rich
          expressions with multiple columns, arithmetic operations and some
          functions, it is unlikely that you will be forced to use external
          standard selections in conditions of small to medium complexity. See
          <xref linkend="conditionSyntax" xrefstyle="select: label" /> for
          more information on in-kernel condition syntax.</para>
        </section>

        <section id="indexedSearches">
          <title>Indexed searches</title>

          <para>When you need more speed than <emphasis>in-kernel</emphasis>
          selections can offer you, PyTables offers a third selection method,
          the so-called <emphasis>indexed</emphasis> mode. In this mode, you
          have to decide which column(s) you are going to do your selections
          on, and index them. Indexing is just a kind of sort operation, so
          that next searches along a column will look at the sorted
          information using a <emphasis>binary search</emphasis> which is much
          faster than a <emphasis>sequential search</emphasis>.</para>

          <para>You can index the columns you choose by calling the
          <literal>Column.createIndex()</literal> method (see <xref
          linkend="createIndexColumnDescr" />) on an already created table.
          For example: <screen>
indexrows = table.cols.var1.createIndex()
indexrows = table.cols.var2.createIndex()
indexrows = table.cols.var3.createIndex()</screen> will create indexes for all
          <literal>var1</literal>, <literal>var2</literal> and
          <literal>var3</literal> columns.</para>

          <para>After you have indexed a column, PyTables will try to use the
          index in your queries. For the moment, a one-piece, non-empty range
          involving constants and an indexed column (all possibly ANDed with
          other conditions) can be detected and the index will then be used.
          It is recommended that you place comparisons involving indexed
          colums the least deep in the condition as possible to maximise the
          chances of actually using indexes.</para>

          <para>Example conditions where an index can be used: <itemizedlist>
              <listitem>
                <para><literal>var1 &gt;= "foo"</literal> (var1 is
                used)</para>
              </listitem>

              <listitem>
                <para><literal>var1 &gt;= mystr</literal> (var1 is
                used)</para>
              </listitem>

              <listitem>
                <para><literal>(var1 &gt;= "foo") &amp; (var3 &gt;
                10)</literal> (var1 is used)</para>
              </listitem>

              <listitem>
                <para><literal>(var1 &gt;= "foo") &amp; (var4 &gt;
                0.0)</literal> (var1 is used)</para>
              </listitem>

              <listitem>
                <para><literal>("bar" &lt;= var1) &amp; (var1 &lt;
                "foo")</literal> (var1 is used)</para>
              </listitem>

              <listitem>
                <para><literal>(("bar" &lt;= var1) &amp; (var1 &lt; "foo"))
                &amp; (var4 &gt; 0.0)</literal> (var1 is used)</para>
              </listitem>
            </itemizedlist></para>

          <para>Example conditions where an index can <emphasis>not</emphasis>
          be used: <itemizedlist>
              <listitem>
                <para><literal>var4 &gt; 0.0</literal> (var4 is not
                indexed)</para>
              </listitem>

              <listitem>
                <para><literal>var1 != 0.0</literal> (range has two
                pieces)</para>
              </listitem>

              <listitem>
                <para><literal>(var1 &gt;= "foo") | (var3 &gt; 10)</literal>
                (conditions are ORed)</para>
              </listitem>
            </itemizedlist></para>

          <para>Remember you can use the
          <literal>willQueryUseIndexing()</literal> method (see <xref
          linkend="Table.willQueryUseIndexing" xrefstyle="select: label" />)
          to check whether a particular query will used indexing or
          not.</para>

          <para>You can see in figures <xref linkend="searchTimes-int"
          xrefstyle="select: labelnumber" /> and <xref
          linkend="searchTimes-float"
          xrefstyle="select:           labelnumber" /> that indexing can
          accelerate quite a lot your data selections in tables. For
          moderately large tables (&gt; one million rows), you can get
          speedups in the order of 100x with regard to
          <emphasis>in-kernel</emphasis> selections, and in the order of 1000x
          with regard to <emphasis>standard</emphasis> selections.</para>

          <para>One important aspect of indexing in PyTables is that it has
          been implemented with the goal of being capable to manage
          effectively very large tables. In <xref linkend="indexTimes"
          xrefstyle="select: label" />, you can see that the times to index
          columns in tables always grow <emphasis>linearly</emphasis>. In
          particular, the time to index a couple of columns with 1 billion of
          rows each is 40 min. (roughly 20 min. each), which is a quite
          reasonable figure. This is because PyTables has chosen an algorithm
          that does a <emphasis>partial</emphasis> sort of the columns in
          order to ensure that the indexing time grows
          <emphasis>linearly</emphasis>. On the contrary, most of relational
          databases try to do a <emphasis>complete</emphasis> sort of columns,
          and this makes the time to index grow much faster with the number of
          rows.</para>

          <para>The fact that relational databases use a complete sorting
          algorithm for indexes means that their index would be more effective
          (but not by a large extent) for searching purposes than the PyTables
          approach. However, for relatively large tables (&gt; 10 millions of
          rows) the time required for completing such a sort can be so large,
          that indexing is not normally worth the effort. In other words,
          PyTables indexing scales much better than relational databases. So
          don't worry if you have extremely large columns to index: PyTables
          is designed to cope with that perfectly.</para>

          <figure id="indexTimes">
            <title>Times for indexing a couple of columns of data type
            <literal>Int32</literal> and <literal>Float64</literal>. Benchmark
            made on a machine with Itanium (IA64) @ 900 MHz processors with
            SCSI disk @ 10K RPM.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="indexTimes-itanium.svg"
                           format="SVG" scale="50" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="indexTimes-itanium.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section id="compressionIssues">
        <title>Compression issues</title>

        <para>One of the beauties of PyTables is that it supports compression
        on tables and arrays<footnote>
            <para>More precisely, it is supported in
            <literal>CArray</literal>, <literal>EArray</literal> and
            <literal>VLArray</literal> objects, but not in
            <literal>Array</literal> objects.</para>
          </footnote>, although it is not used by default. Compression of big
        amounts of data might be a bit controversial feature, because
        compression has a legend of being a very big consumer of CPU time
        resources. However, if you are willing to check if compression can
        help not only by reducing your dataset file size but
        <emphasis>also</emphasis> by improving I/O efficiency, specially when
        dealing with very large datasets, keep reading.</para>

        <para>There is a common scenario where users need to save duplicated
        data in some record fields, while the others have varying values. In a
        relational database approach such redundant data can normally be moved
        to other tables and a relationship between the rows on the separate
        tables can be created. But that takes analysis and implementation
        time, and makes the underlying libraries more complex and
        slower.</para>

        <para>PyTables transparent compression allows the users to not worry
        about finding which is their optimum strategy for data tables, but
        rather use less, not directly related, tables with a larger number of
        columns while still not cluttering the database too much with
        duplicated data (compression is responsible to avoid that). As a side
        effect, data selections can be made more easily because you have more
        fields available in a single table, and they can be referred in the
        same loop. This process may normally end in a simpler, yet powerful
        manner to process your data (although you should still be careful
        about in which kind of scenarios the use of compression is convenient
        or not).</para>

        <para>The compression library used by default is the
        <emphasis>Zlib</emphasis> (see <biblioref linkend="zlibRef" />). Since
        HDF5 <emphasis>requires</emphasis> it, you can safely use it and
        expect that your HDF5 files will be readable on any other platform
        that has HDF5 libraries installed. Zlib provides good compression
        ratio, although somewhat slow, and reasonably fast decompression.
        Because of that, it is a good candidate to be used for compressing you
        data.</para>

        <para>However, in some situations it is critical to have
        <emphasis>very good</emphasis> decompression speed (at the expense of
        lower compression ratios or more CPU wasted on compression, as we will
        see soon). In others, the emphasis is put in achieving the maximum
        compression ratios, no matter which reading speed will result. This is
        why support for two additional compressors has been added to PyTables:
        LZO (see <biblioref linkend="lzoRef" />) and bzip2 (see <biblioref
        linkend="bzip2Ref" />). Following the author of LZO (and checked by
        the author of this section, as you will see soon), LZO offers pretty
        fast compression (though a small compression ratio) and extremely fast
        decompression. In fact, LZO is so fast when compressing/decompressing
        that it may well happen (that depends on your data, of course) that
        writing or reading a compressed dataset is sometimes faster than if it
        is not compressed at all (specially when dealing with extremely large
        datasets). This fact is very important, specially if you have to deal
        with very large amounts of data. Regarding bzip2, it has a reputation
        of achieving excellent compression ratios, but at the price of
        spending much more CPU time, which results in very low
        compression/decompression speeds.</para>

        <para>Be aware that the LZO and bzip2 support in PyTables is not
        standard on HDF5, so if you are going to use your PyTables files in
        other contexts different from PyTables you will not be able to read
        them. Still, see the <xref linkend="ptrepackDescr"
        xrefstyle="select: label" /> (where the <literal>ptrepack</literal>
        utility is described) to find a way to free your files from LZO or
        bzip2 dependencies, so that you can use these compressors locally with
        the warranty that you can replace them with Zlib (or even remove
        compression completely) if you want to use these files with other HDF5
        tools or platforms afterwards.</para>

        <para>In order to allow you to grasp what amount of compression can be
        achieved, and how this affects performance, a series of experiments
        has been carried out. All the results presented in this section (and
        in the next one) have been obtained with synthetic data and using
        PyTables 1.3. Also, the tests have been conducted on a IBM OpenPower
        720 (e-series) with a PowerPC G5 at 1.65 GHz and a hard disk spinning
        at 15K RPM. As your data and platform may be totally different for
        your case, take this just as a guide because your mileage will
        probably vary. Finally, and to be able to play with tables with a
        number of rows as large as possible, the record size has been chosen
        to be small (16 bytes). Here is its definition:</para>

        <screen>
class Bench(IsDescription):
    var1 = StringCol(length=4)
    var2 = IntCol()
    var3 = FloatCol()
        </screen>

        <para>With this setup, you can look at the compression ratios that can
        be achieved in <xref linkend="comprTblComparison"
        xrefstyle="select: label" />. As you can see, LZO is the compressor
        that performs worse in this sense, but, curiosly enough, there is not
        much difference between Zlib and bzip2.</para>

        <figure id="comprTblComparison">
          <title>Comparison between different compression libraries.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="compressed-recordsize.svg"
                         format="SVG" scale="55" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="compressed-recordsize.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>Also, PyTables lets you select different compression levels for
        Zlib and bzip2, although you may get a bit disappointed by the small
        improvement that show these compressors when dealing with a
        combination of numbers and strings as in our example. As a reference,
        see plot <xref linkend="comprZlibComparison"
        xrefstyle="select: labelnumber" /> for a comparison of the compression
        achieved by selecting different levels of Zlib. Very oddly, the best
        compression ratio corresponds to level 1 (!). It's difficult to
        explain that, but this lesson will serve to reaffirm that there is no
        replacement for experiments with your own data. In general, it is
        recommended to select the <emphasis>lowest</emphasis> level of
        compression in order to achieve best performance and decent (if not
        the best!) compression ratio. See later for more figures on this
        regard.</para>

        <figure id="comprZlibComparison">
          <title>Comparison between different compression levels of
          Zlib.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-recordsize-zlib.svg" format="SVG"
                         scale="55" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-recordsize-zlib.png" format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>Have also a look at <xref linkend="comprWriteComparison"
        xrefstyle="select: label" />. It shows how the speed of writing rows
        evolves as the size (the row number) of the table grows. Even though
        in these graphs the size of one single row is 16 bytes, you can most
        probably extrapolate these figures to other row sizes.</para>

        <figure id="comprWriteComparison">
          <title>Writing tables with several compressors.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="compressed-writing.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="compressed-writing.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>In <xref linkend="comprReadNoCacheComparison"
        xrefstyle="select: label" /> you can see how compression affects the
        reading performance. In fact, what you see in the plot is an
        <emphasis>in-kernel selection</emphasis> speed, but provided that this
        operation is very fast (see <xref linkend="inkernelSearch"
        xrefstyle="select: label" />), we can accept it as an actual read
        test. Compared with the reference line without compression, the
        general trend here is that LZO does not affect too much the reading
        performance (and in some points it is actually better), Zlib makes
        speed to drop to a half, while bzip2 is performing very slow (up to 8x
        slower).</para>

        <para>Also, in the same <xref linkend="comprReadNoCacheComparison"
        xrefstyle="select: label" /> you can notice some strange peaks in the
        speed that we might be tempted to attribute to libraries on which
        PyTables relies (HDF5, compressors...), or to PyTables itself.
        However, <xref linkend="comprReadCacheComparison"
        xrefstyle="select: label" /> reveals that, if we put the file in the
        filesystem cache (by reading it several times before, for example),
        the evolution of the performance is much smoother. So, the most
        probable explanation would be that such a peaks are a consequence of
        the underlying OS filesystem, rather than a flaw in PyTables (or any
        other library behind it). Another consequence that can be derived from
        the above plot is that LZO decompression performance is much better
        than Zlib, allowing an improvement in overal speed of more than 2x,
        and perhaps more important, the read performance for really large
        datasets (i.e. when they do not fit in the OS filesystem cache) can be
        actually <emphasis>better</emphasis> than not using compression at
        all. Finally, one can see that reading performance is very badly
        affected when bzip2 is used (it is 10x slower than LZO and 4x than
        Zlib), but this is not too strange anyway.</para>

        <figure id="comprReadNoCacheComparison">
          <title>Selecting values in tables with several compressors. The file
          is not in the OS cache.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-select-nocache.svg" format="SVG"
                         scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-select-nocache.png" format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="comprReadCacheComparison">
          <title>Selecting values in tables with several compressors. The file
          is in the OS cache.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="compressed-select-cache.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="compressed-select-cache.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>So, generally speaking and looking at the experiments above, you
        can expect that LZO will be the fastest in both compressing and
        decompressing, but the one that achieves the worse compression ratio
        (although that may be just OK for many situations, specially when used
        with the <xref linkend="ShufflingOptim" xrefstyle="select: label" />).
        bzip2 is the slowest, by large, in both compressing and decompressing,
        and besides, it does not achieve any better compression ratio than
        Zlib. Zlib represents a balance between them: it's somewhat slow
        compressing (2x) and decompressing (3x) than LZO, but it normally
        achieves fairly good compression ratios.</para>

        <para>Finally, by looking at the plots <xref
        linkend="comprWriteZlibComparison"
        xrefstyle="select:         labelnumber" />, <xref
        linkend="comprReadZlibComparison" xrefstyle="select: labelnumber" />,
        and the aforementioned <xref linkend="comprZlibComparison"
        xrefstyle="select: labelnumber" /> you can see why the recommended
        compression level to use for all compression libraries is 1. This is
        the lowest level of compression, but if you take the approach
        suggested above, the redundant data is to be found normally in the
        same row, making redundancy locality very high so that a small level
        of compression should be enough to achieve a good compression ratio on
        your data tables, saving CPU cycles for doing other things.
        Nonetheless, in some situations you may want to check for your own how
        the different compression levels affect your application.</para>

        <para>You can select the compression library and level by setting the
        <literal>complib</literal> and <literal>complevel</literal> keywords
        in the <literal>Filters</literal> class (see <xref
        linkend="FiltersClassDescr" xrefstyle="select: label" />). A
        compression level of 0 will completely disable compression (the
        default), 1 is the less CPU time demanding level, while 9 is the
        maximum level and most CPU intensive. Finally, have in mind that LZO
        is not accepting a compression level right now, so, when using LZO, 0
        means that compression is not active, and any other value means that
        LZO is active.</para>

        <para>So, in conclusion, if your ultimate goal is writing and reading
        as fast as possible, choose LZO. If you want to reduce as much as
        possible your data, while retaining acceptable read speed, choose
        Zlib. Finally, if portability is important for you, Zlib is your best
        bet. So, when you want to use bzip2? Well, looking at the results, it
        is difficult to recommend its use in general, but you may want to
        experiment with it in those cases where you know that it is well
        suited for your data pattern (for example, for dealing with repetitive
        string datasets).</para>

        <figure id="comprWriteZlibComparison">
          <title>Writing in tables with different levels of
          compression.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="compressed-writing-zlib.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="compressed-writing-zlib.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="comprReadZlibComparison">
          <title>Selecting values in tables with different levels of
          compression. The file is in the OS cache.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-select-cache-zlib.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-select-cache-zlib.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section id="ShufflingOptim">
        <title>Shuffling (or how to make the compression process more
        effective)</title>

        <para>The HDF5 library provides an interesting filter that can
        leverage the results of your favorite compressor. Its name is
        <emphasis>shuffle</emphasis>, and because it can greatly benefit
        compression and it does not take many CPU resources (see below for a
        justification), it is active by <emphasis>default</emphasis> in
        PyTables whenever compression is activated (independently of the
        chosen compressor). It is of course deactivated when compression is
        off (which is the default, as you already should know). Of course, you
        can deactivate it if you want, but this is not recommended.</para>

        <para>So, how exactly works this mysterious filter? From the HDF5
        reference manual: <quote>The shuffle filter de-interlaces a block of
        data by reordering the bytes. All the bytes from one consistent byte
        position of each data element are placed together in one block; all
        bytes from a second consistent byte position of each data element are
        placed together a second block; etc. For example, given three data
        elements of a 4-byte datatype stored as 012301230123, shuffling will
        re-order data as 000111222333. This can be a valuable step in an
        effective compression algorithm because the bytes in each byte
        position are often closely related to each other and putting them
        together can increase the compression ratio.</quote></para>

        <para>In <xref linkend="comprShuffleComparison"
        xrefstyle="select: label" /> you can see a benchmark that shows how
        the <emphasis>shuffle</emphasis> filter can help the different
        libraries in compressing data. In this experiment, shuffle has made
        LZO to compress almost 3x more (!), while Zlib and bzip2 are seeing
        improvements of 2x. Once again, the data for this experiment is
        synthetic, and <emphasis>shuffle</emphasis> seems to do a great work
        with it, but in general, the results will vary in each case<footnote>
            <para>Some users reported that the typical improvement with real
            data is between a factor 1.5x and 2.5x over the already compressed
            datasets.</para>
          </footnote>.</para>

        <figure id="comprShuffleComparison">
          <title>Comparison between different compression libraries with and
          without the <emphasis>shuffle</emphasis> filter.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-recordsize-shuffle.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-recordsize-shuffle.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>At any rate, the most remarkable fact about the
        <emphasis>shuffle</emphasis> filter is the relatively high level of
        compression that compressor filters can achieve when used in
        combination with it. A curious thing to note is that the Bzip2
        compression rate does not seem very much improved (less than a 40%),
        and what is more striking, Bzip2+shuffle does compress quite
        <emphasis>less</emphasis> than Zlib+shuffle or LZO+shuffle
        combinations, which is kind of unexpected. The thing that seems clear
        is that Bzip2 is not very good at compressing patterns that result of
        shuffle application. As always, you may want to experiment with your
        own data before widely applying the Bzip2+shuffle combination in order
        to avoid surprises.</para>

        <para>Now, how does shuffling affect performance? Well, if you look at
        plots <xref linkend="comprWriteShuffleComparison"
        xrefstyle="select: labelnumber" />, <xref
        linkend="comprReadNoCacheShuffleComparison"
        xrefstyle="select:         labelnumber" /> and <xref
        linkend="comprReadCacheShuffleComparison"
        xrefstyle="select: labelnumber" />, you will get a somewhat unexpected
        (but pleasant) surprise. Roughly, <emphasis>shuffle</emphasis> makes
        the writing process (shuffling+compressing) faster (aproximately a 15%
        for LZO, 30% for Bzip2 and a 80% for Zlib), which is an interesting
        result by itself. But perhaps more exciting is the fact that the
        reading process (unshuffling+decompressing) is also accelerated by a
        similar extent (a 20% for LZO, 60% for Zlib and a 75% for Bzip2,
        roughly).</para>

        <figure id="comprWriteShuffleComparison">
          <title>Writing with different compression libraries with and without
          the <emphasis>shuffle</emphasis> filter.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-writing-shuffle.svg" format="SVG"
                         scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-writing-shuffle.png" format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="comprReadNoCacheShuffleComparison">
          <title>Reading with different compression libraries with the
          <emphasis>shuffle</emphasis> filter. The file is not in OS
          cache.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-select-nocache-shuffle-only.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-select-nocache-shuffle-only.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="comprReadCacheShuffleComparison">
          <title>Reading with different compression libraries with and without
          the <emphasis>shuffle</emphasis> filter. The file is in OS
          cache.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="compressed-select-cache-shuffle.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="compressed-select-cache-shuffle.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>You may wonder why introducing another filter in the write/read
        pipelines does effectively accelerate the throughput. Well, maybe data
        elements are more similar or related column-wise than row-wise, i.e.
        contiguous elements in the same column are more alike, so shuffling
        makes the job of the compressor easier (faster) and more effective
        (greater ratios). As a side effect, compressed chunks do fit better in
        the CPU cache (at least, the chunks are smaller!) so that the process
        of unshuffle/decompress can make a better use of the cache (i.e.
        reducing the number of CPU cache faults).</para>

        <para>So, given the potential gains (faster writing and reading, but
        specially much improved compression level), it is a good thing to have
        such a filter enabled by default in the battle for discovering
        redundancy when you want to compress your data, just as PyTables
        does.</para>
      </section>

      <section>
        <title>Using Psyco</title>

        <para>Psyco (see <biblioref linkend="psycoRef" />) is a kind of
        specialized compiler for Python that typically accelerates Python
        applications with no change in source code. You can think of Psyco as
        a kind of just-in-time (JIT) compiler, a little bit like Java's, that
        emits machine code on the fly instead of interpreting your Python
        program step by step. The result is that your unmodified Python
        programs run faster.</para>

        <para>Psyco is very easy to install and use, so in most scenarios it
        is worth to give it a try. However, it only runs on Intel 386
        architectures, so if you are using other architectures, you are out of
        luck (at least until Psyco will support yours).</para>

        <para>As an example, imagine that you have a small script that reads
        and selects data over a series of datasets, like this:</para>

        <screen>
def readFile(filename):
"Select data from all the tables in filename"

fileh = openFile(filename, mode = "r")
result = []
for table in fileh("/", 'Table'):
result = [ p['var3'] for p in table if p['var2'] &lt;= 20 ]

fileh.close()
return result

if __name__=="__main__":
print readFile("myfile.h5")
        </screen>

        <para>In order to accelerate this piece of code, you can rewrite your
        main program to look like:</para>

        <screen>
if __name__=="__main__":
import psyco
psyco.bind(readFile)
print readFile("myfile.h5")
        </screen>

        <para>That's all!. From now on, each time that you execute your Python
        script, Psyco will deploy its sophisticated algorithms so as to
        accelerate your calculations.</para>

        <para>You can see in the graphs <xref linkend="psycoWriteComparison"
        xrefstyle="select: labelnumber" /> and <xref
        linkend="psycoReadComparison"
        xrefstyle="select:         labelnumber" /> how much I/O speed
        improvement you can get by using Psyco. By looking at this figures you
        can get an idea if these improvements are of your interest or not. In
        general, if you are not going to use compression you will take
        advantage of Psyco if your tables are medium sized (from a thousand to
        a million rows), and this advantage will disappear progressively when
        the number of rows grows well over one million. However if you use
        compression, you will probably see improvements even beyond this limit
        (see <xref linkend="compressionIssues" xrefstyle="select: label" />).
        As always, there is no substitute for experimentation with your own
        dataset.</para>

        <figure id="psycoWriteComparison">
          <title>Writing tables with/without Psyco.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="write-medium-psyco-nopsyco-comparison.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="write-medium-psyco-nopsyco-comparison.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="psycoReadComparison">
          <title>Reading tables with/without Psyco.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="read-medium-psyco-nopsyco-comparison.svg"
                         format="SVG" scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="read-medium-psyco-nopsyco-comparison.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Getting the most from the node LRU cache</title>

        <para>Starting from PyTables 1.2 on, it has been introduced a new LRU
        cache that prevents from loading all the nodes of the <emphasis>object
        tree</emphasis> in memory. This cache is responsible of loading just
        up to a certain amount of nodes and discard the least recent used ones
        when there is a need to load new ones. This represents a big advantage
        over the old schema, specially in terms of memory usage (as there is
        no need to load <emphasis>every</emphasis> node in memory), but it
        also adds very convenient optimizations for working interactively
        like, for example, speeding-up the opening times of files with lots of
        nodes, allowing to open almost any kind of file in typically less than
        one tenth of second (compare this with the more than 10 seconds for
        files with more than 10000 nodes in PyTables pre-1.2 era). See
        <biblioref linkend="NewObjectTreeCacheRef" /> for more info on the
        advantages (and also drawbacks) of this approach.</para>

        <para>One thing that deserves some discussion is the election of the
        parameter that sets the maximum amount of nodes to be held in memory
        at any time. As PyTables is meant to be deployed in machines that have
        potentially low memory, the default for it is quite conservative (you
        can look at its actual value in the <literal>NODE_CACHE_SIZE</literal>
        parameter in module <literal>tables/constants.py</literal>). However,
        if you usually have to deal with files that have much more nodes than
        the maximum default, and you have a lot of free memory in your system,
        then you may want to experiment which is the appropriate value of
        <literal>NODE_CACHE_SIZE</literal> that fits better your needs.</para>

        <para>As an example, look at the next code:</para>

        <screen>
def browse_tables(filename):
fileh = openFile(filename,'a')
group = fileh.root.newgroup
for j in range(10):
for tt in fileh.walkNodes(group, "Table"):
      title = tt.attrs.TITLE
      for row in tt:
pass
fileh.close()
        </screen>

        <para>We will be running the code above against a couple of files
        having a <literal>/newgroup</literal> containing 100 tables and 1000
        tables respectively. We will run this small benchmark for different
        values of the LRU cache size, namely 256 and 1024. You can see the
        results in <xref linkend="LRUTblComparison"
        xrefstyle="select: label" />.</para>

        <table align="center" id="LRUTblComparison">
          <title>Retrieving speed and memory consumption dependency of the
          number of nodes in LRU cache.</title>

          <tgroup cols="10">
            <colspec align="left" colname="c1" />

            <colspec align="left" colname="c2" />

            <colspec align="right" colname="c3" />

            <colspec align="right" colname="c4" />

            <colspec align="right" colname="c5" />

            <colspec align="right" colname="c6" />

            <colspec align="right" colname="c7" />

            <colspec align="right" colname="c8" />

            <colspec align="right" colname="c9" />

            <colspec align="right" colname="c10" />

            <thead>
              <row>
                <entry align="left" nameend="c2" namest="c1"></entry>

                <entry align="center" nameend="c6" namest="c3">100
                nodes</entry>

                <entry align="center" nameend="c10" namest="c7">1000
                nodes</entry>
              </row>

              <row>
                <entry align="left" nameend="c2" namest="c1"></entry>

                <entry align="center" nameend="c4" namest="c3">Memory
                (MB)</entry>

                <entry align="center" nameend="c6" namest="c5">Time
                (ms)</entry>

                <entry align="center" nameend="c8" namest="c7">Memory
                (MB)</entry>

                <entry align="center" nameend="c10" namest="c9">Time
                (ms)</entry>
              </row>

              <row>
                <entry align="left">Node is coming from...</entry>

                <entry align="left">Cache size</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry align="left">From disk</entry>

                <entry align="left"></entry>

                <entry align="right">14</entry>

                <entry align="right">14</entry>

                <entry align="right">1.24</entry>

                <entry align="right">1.24</entry>

                <entry align="right">51</entry>

                <entry align="right">66</entry>

                <entry align="right">1.33</entry>

                <entry align="right">1.31</entry>
              </row>

              <row>
                <entry align="left">From cache</entry>

                <entry align="left"></entry>

                <entry align="right">14</entry>

                <entry align="right">14</entry>

                <entry align="right">0.53</entry>

                <entry align="right">0.52</entry>

                <entry align="right">65</entry>

                <entry align="right">73</entry>

                <entry align="right">1.35</entry>

                <entry align="right">0.68</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>From the data in <xref linkend="LRUTblComparison"
        xrefstyle="select: label" />, one can see that, when the number of
        objects that you are dealing with does fit in cache, you will get
        better access times to them. Also, incrementing the node cache size
        does effectively consumes more memory <emphasis>only</emphasis> if the
        total nodes exceeds the slots in cache; otherwise the memory
        consumption remains the same. It is also worth noting that
        incrementing the node cache size in the case you want to fit all your
        nodes in cache, it does not take much more memory than keeping too
        conservative. On another hand, it might happen that the speed-up that
        you can achieve by allocating more slots in your cache maybe is not
        worth the amount of memory used.</para>

        <para>Anyway, if you feel that this issue is important for you, setup
        your own experiments and proceed fine-tuning the
        <literal>NODE_CACHE_SIZE</literal> parameter.</para>
      </section>

      <section id="rootUEPOptim">
        <title>Selecting an User Entry Point (UEP) in your tree</title>

        <para>If you have a <emphasis>huge</emphasis> tree in your data file
        with many nodes on it, creating the object tree would take long time.
        Many times, however, you are interested only in access to a part of
        the complete tree, so you won't strictly need PyTables to build the
        entire object tree in-memory, but only the
        <emphasis>interesting</emphasis> part.</para>

        <para>This is where the <literal>rootUEP</literal> parameter of
        <literal>openFile</literal> function (see <xref
        linkend="openFileDescr" xrefstyle="select: label" />) can be
        helpful. Imagine that you have a file called
        <literal>"test.h5"</literal> with the associated tree that you can see
        in <xref linkend="rootUEPfig1" xrefstyle="select: label" />, and you
        are interested only in the section marked in red. You can avoid the
        build of all the object tree by saying to <literal>openFile</literal>
        that your root will be the <literal>/Group2/Group3</literal> group.
        That is:</para>

        <screen>
fileh = openFile("test.h5", rootUEP="/Group2/Group3")
        </screen>

        <para>As a result, the actual object tree built will be like the one
        that can be seen in <xref linkend="rootUEPfig2"
        xrefstyle="select: label" />.</para>

        <para>Of course this has been a simple example and the use of the
        <literal>rootUEP</literal> parameter was not very necessary. But when
        you have <emphasis>thousands</emphasis> of nodes on a tree, you will
        certainly appreciate the <literal>rootUEP</literal> parameter.</para>

        <figure id="rootUEPfig1">
          <title>Complete tree in file <literal>test.h5</literal>, and subtree
          of interest for the user.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="rootUEP1.svg" format="SVG"
                         scale="60" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="rootUEP1.png" format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="rootUEPfig2">
          <title>Resulting object tree derived from the use of the
          <literal>rootUEP</literal> parameter.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="rootUEP2.svg" format="SVG"
                         scale="70" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="rootUEP2.png" format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Compacting your PyTables files</title>

        <para>Let's suppose that you have a file on which you have made a lot
        of row deletions on one or more tables, or deleted many leaves or even
        entire subtrees. These operations might leave
        <emphasis>holes</emphasis> (i.e. space that is not used anymore) in
        your files, that may potentially affect not only the size of the files
        but, more importantly, the performance of I/O. This is because when
        you delete a lot of rows on a table, the space is not automatically
        recovered on-the-flight. In addition, if you add many more rows to a
        table than specified in the <literal>expectedrows</literal> keyword in
        creation time this may affect performance as well, as explained in
        <xref linkend="expectedRowsOptim" xrefstyle="select: label" />.</para>

        <para>In order to cope with these issues, you should be aware that a
        handy PyTables utility called <literal>ptrepack</literal> can be very
        useful, not only to compact your already existing
        <emphasis>leaky</emphasis> files, but also to adjust some internal
        parameters (both in memory and in file) in order to create adequate
        buffer sizes and chunk sizes for optimum I/O speed. Please check the
        <xref linkend="ptrepackDescr" xrefstyle="select: label" /> for a brief
        tutorial on its use.</para>

        <para>Another thing that you might want to use
        <literal>ptrepack</literal> for is changing the compression filters or
        compression levels on your existing data for different goals, like
        checking how this can affect both final size and I/O performance, or
        getting rid of the optional compressors like <literal>LZO</literal> or
        <literal>bzip2</literal> in your existing files in case you want to
        use them with generic HDF5 tools that do not have support for these
        filters.</para>
      </section>
    </chapter>
  </part>

  <part label="II">
    <title>Complementary modules</title>

    <chapter id="filenode">
      <title>filenode - simulating a filesystem with PyTables</title>

      <para></para>

      <section>
        <title>What is <literal>filenode</literal>?</title>

        <para><literal>filenode</literal> is a module which enables you to
        create a PyTables database of nodes which can be used like regular
        opened files in Python. In other words, you can store a file in a
        PyTables database, and read and write it as you would do with any
        other file in Python. Used in conjunction with PyTables hierarchical
        database organization, you can have your database turned into an open,
        extensible, efficient, high capacity, portable and metadata-rich
        filesystem for data exchange with other systems (including backup
        purposes).</para>

        <para>Between the main features of <literal>filenode</literal>, one
        can list:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis>Open:</emphasis> Since it relies on PyTables,
            which in turn, sits over HDF5 (see <biblioref
            linkend="HDFWhatIs" />), a standard hierarchical data format from
            NCSA.</para>
          </listitem>

          <listitem>
            <para><emphasis>Extensible:</emphasis> You can define new types of
            nodes, and their instances will be safely preserved (as are normal
            groups, leafs and attributes) by PyTables applications having no
            knowledge of their types. Moreover, the set of possible attributes
            for a node is not fixed, so you can define your own node
            attributes.</para>
          </listitem>

          <listitem>
            <para><emphasis>Efficient:</emphasis> Thanks to PyTables' proven
            extreme efficiency on handling huge amounts of data.
            <literal>filenode</literal> can make use of PyTables' on-the-fly
            compression and decompression of data.</para>
          </listitem>

          <listitem>
            <para><emphasis>High capacity:</emphasis> Since PyTables and HDF5
            are designed for massive data storage (they use 64-bit addressing
            even where the platform does not support it natively).</para>
          </listitem>

          <listitem>
            <para><emphasis>Portable:</emphasis> Since the HDF5 format has an
            architecture-neutral design, and the HDF5 libraries and PyTables
            are known to run under a variety of platforms. Besides that, a
            PyTables database fits into a single file, which poses no trouble
            for transportation.</para>
          </listitem>

          <listitem>
            <para><emphasis>Metadata-rich:</emphasis> Since PyTables can store
            arbitrary key-value pairs (even Python objects!) for every
            database node. Metadata may include authorship, keywords, MIME
            types and encodings, ownership information, access control lists
            (ACL), decoding functions and anything you can imagine!</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Finding a <literal>filenode</literal> node</title>

        <para><literal>filenode</literal> nodes can be recognized because they
        have a <literal>NODE_TYPE</literal> system attribute with a
        <literal>'file'</literal> value. It is recommended that you use the
        <literal>getNodeAttr()</literal> method (see <xref
        linkend="File.getNodeAttr" />) of <literal>tables.File</literal> class
        to get the <literal>NODE_TYPE</literal> attribute independently of the
        nature (group or leaf) of the node, so you do not need to care
        about.</para>
      </section>

      <section>
        <title><literal>filenode</literal> - simulating files inside
        PyTables</title>

        <para>The <literal>filenode</literal> module is part of the
        <literal>nodes</literal> sub-package of PyTables. The recommended way
        to import the module is:</para>

        <screen>
&gt;&gt;&gt; from tables.nodes import filenode
        </screen>

        <para>However, <literal>filenode</literal> exports very few symbols,
        so you can import <literal>*</literal> for interactive usage. In fact,
        you will most probably only use the <literal>NodeType</literal>
        constant and the <literal>newNode()</literal> and
        <literal>openNode()</literal> calls.</para>

        <para>The <literal>NodeType</literal> constant contains the value that
        the <literal>NODE_TYPE</literal> system attribute of a node file is
        expected to contain (<literal>'file'</literal>, as we have seen).
        Although this is not expected to change, you should use
        <literal>filenode.NodeType</literal> instead of the literal
        <literal>'file'</literal> when possible.</para>

        <para><literal>newNode()</literal> and <literal>openNode()</literal>
        are the equivalent to the Python <literal>file()</literal> call (alias
        <literal>open()</literal>) for ordinary files. Their arguments differ
        from that of <literal>file()</literal>, but this is the only point
        where you will note the difference between working with a node file
        and working with an ordinary file.</para>

        <para>For this little tutorial, we will assume that we have a PyTables
        database opened for writing. Also, if you are somewhat lazy at typing
        sentences, the code that we are going to explain is included in the
        <literal>examples/filenodes1.py</literal> file.</para>

        <para>You can create a brand new file with these sentences:</para>

        <screen>
&gt;&gt;&gt; import tables
&gt;&gt;&gt; h5file = tables.openFile('fnode.h5', 'w')
        </screen>

        <section>
          <title>Creating a new file node</title>

          <para>Creation of a new file node is achieved with the
          <literal>newNode()</literal> call. You must tell it in which
          PyTables file you want to create it, where in the PyTables hierarchy
          you want to create the node and which will be its name. The PyTables
          file is the first argument to <literal>newNode()</literal>; it will
          be also called the <literal>'host PyTables file'</literal>. The
          other two arguments must be given as keyword arguments
          <literal>where</literal> and <literal>name</literal>, respectively.
          As a result of the call, a brand new appendable and readable file
          node object is returned.</para>

          <para>So let us create a new node file in the previously opened
          <literal>h5file</literal> PyTables file, named
          <literal>'fnode_test'</literal> and placed right under the root of
          the database hierarchy. This is that command:</para>

          <screen>
&gt;&gt;&gt; fnode = filenode.newNode(h5file, where='/', name='fnode_test')
          </screen>

          <para>That is basically all you need to create a file node. Simple,
          isn't it? From that point on, you can use <literal>fnode</literal>
          as any opened Python file (i.e. you can write data, read data, lines
          of text and so on).</para>

          <para><literal>newNode()</literal> accepts some more keyword
          arguments. You can give a title to your file with the
          <literal>title</literal> argument. You can use PyTables' compression
          features with the <literal>filters</literal> argument. If you know
          beforehand the size that your file will have, you can give its final
          file size in bytes to the <literal>expectedsize</literal> argument
          so that the PyTables library would be able to optimize the data
          access.</para>

          <para><literal>newNode()</literal> creates a PyTables node where it
          is told to. To prove it, we will try to get the
          <literal>NODE_TYPE</literal> attribute from the newly created
          node.</para>

          <screen>
&gt;&gt;&gt; print h5file.getNodeAttr('/fnode_test', 'NODE_TYPE')
file
          </screen>
        </section>

        <section>
          <title>Using a file node</title>

          <para>As stated above, you can use the new node file as any other
          opened file. Let us try to write some text in and read it.</para>

          <screen>
&gt;&gt;&gt; print &gt;&gt; fnode, "This is a test text line."
&gt;&gt;&gt; print &gt;&gt; fnode, "And this is another one."
&gt;&gt;&gt; print &gt;&gt; fnode
&gt;&gt;&gt; fnode.write("Of course, file methods can also be used.")
&gt;&gt;&gt;
&gt;&gt;&gt; fnode.seek(0)  # Go back to the beginning of file.
&gt;&gt;&gt;
&gt;&gt;&gt; for line in fnode:
...   print repr(line)
'This is a test text line.\n'
'And this is another one.\n'
'\n'
'Of course, file methods can also be used.'
          </screen>

          <para>This was run on a Unix system, so newlines are expressed as
          <literal>'\n'</literal>. In fact, you can override the line
          separator for a file by setting its <literal>lineSeparator</literal>
          property to any string you want.</para>

          <para>While using a file node, you should take care of closing it
          <emphasis>before</emphasis> you close the PyTables host file.
          Because of the way PyTables works, your data it will not be at a
          risk, but every operation you execute after closing the host file
          will fail with a <literal>ValueError</literal>. To close a file
          node, simply delete it or call its <literal>close()</literal>
          method.</para>

          <screen>
&gt;&gt;&gt; fnode.close()
&gt;&gt;&gt; print fnode.closed
True
          </screen>
        </section>

        <section>
          <title>Opening an existing file node</title>

          <para>If you have a file node that you created using
          <literal>newNode()</literal>, you can open it later by calling
          <literal>openNode()</literal>. Its arguments are similar to that of
          <literal>file()</literal> or <literal>open()</literal>: the first
          argument is the PyTables node that you want to open (i.e. a node
          with a <literal>NODE_TYPE</literal> attribute having a
          <literal>'file'</literal> value), and the second argument is a mode
          string indicating how to open the file. Contrary to
          <literal>file()</literal>, <literal>openNode()</literal> can not be
          used to create a new file node.</para>

          <para>File nodes can be opened in read-only mode
          (<literal>'r'</literal>) or in read-and-append mode
          (<literal>'a+'</literal>). Reading from a file node is allowed in
          both modes, but appending is only allowed in the second one. Just
          like Python files do, writing data to an appendable file places it
          after the file pointer if it is on or beyond the end of the file, or
          otherwise after the existing data. Let us see an example:</para>

          <screen>
&gt;&gt;&gt; node = h5file.root.fnode_test
&gt;&gt;&gt; fnode = filenode.openNode(node, 'a+')
&gt;&gt;&gt; print repr(fnode.readline())
'This is a test text line.\n'
&gt;&gt;&gt; print fnode.tell()
26
&gt;&gt;&gt; print &gt;&gt; fnode, "This is a new line."
&gt;&gt;&gt; print repr(fnode.readline())
''
          </screen>

          <para>Of course, the data append process places the pointer at the
          end of the file, so the last <literal>readline()</literal> call hit
          <literal>EOF</literal>. Let us seek to the beginning of the file to
          see the whole contents of our file.</para>

          <screen>
&gt;&gt;&gt; fnode.seek(0)
&gt;&gt;&gt; for line in fnode:
...   print repr(line)
'This is a test text line.\n'
'And this is another one.\n'
'\n'
'Of course, file methods can also be used.This is a new line.\n'
          </screen>

          <para>As you can check, the last string we wrote was correctly
          appended at the end of the file, instead of overwriting the second
          line, where the file pointer was positioned by the time of the
          appending.</para>
        </section>

        <section>
          <title>Adding metadata to a file node</title>

          <para>You can associate arbitrary metadata to any open node file,
          regardless of its mode, as long as the host PyTables file is
          writable. Of course, you could use the
          <literal>setNodeAttr()</literal> method of
          <literal>tables.File</literal> to do it directly on the proper node,
          but <literal>filenode</literal> offers a much more comfortable way
          to do it. <literal>filenode</literal> objects have an
          <literal>attrs</literal> property which gives you direct access to
          their corresponding <literal>AttributeSet</literal> object.</para>

          <para>For instance, let us see how to associate MIME type metadata
          to our file node:</para>

          <screen>
&gt;&gt;&gt; fnode.attrs.content_type = 'text/plain; charset=us-ascii'
          </screen>

          <para>As simple as A-B-C. You can put nearly anything in an
          attribute, which opens the way to authorship, keywords, permissions
          and more. Moreover, there is not a fixed list of attributes.
          However, you should avoid names in all caps or starting with
          <literal>'_'</literal>, since PyTables and
          <literal>filenode</literal> may use them internally. Some valid
          examples:</para>

          <screen>
&gt;&gt;&gt; fnode.attrs.author = "Ivan Vilata i Balaguer"
&gt;&gt;&gt; fnode.attrs.creation_date = '2004-10-20T13:25:25+0200'
&gt;&gt;&gt; fnode.attrs.keywords_en = ["FileNode", "test", "metadata"]
&gt;&gt;&gt; fnode.attrs.keywords_ca = ["FileNode", "prova", "metadades"]
&gt;&gt;&gt; fnode.attrs.owner = 'ivan'
&gt;&gt;&gt; fnode.attrs.acl = {'ivan': 'rw', '@users': 'r'}
          </screen>

          <para>You can check that these attributes get stored by running the
          <literal>ptdump</literal> command on the host PyTables file:</para>

          <screen>
$ ptdump -a fnode.h5:/fnode_test
/fnode_test (EArray(113,)) ''
/fnode_test.attrs (AttributeSet), 14 attributes:
[CLASS := 'EARRAY',
EXTDIM := 0,
FLAVOR := 'numpy',
NODE_TYPE := 'file',
NODE_TYPE_VERSION := 2,
TITLE := '',
VERSION := '1.2',
acl := {'ivan': 'rw', '@users': 'r'},
author := 'Ivan Vilata i Balaguer',
content_type := 'text/plain; charset=us-ascii',
creation_date := '2004-10-20T13:25:25+0200',
keywords_ca := ['FileNode', 'prova', 'metadades'],
keywords_en := ['FileNode', 'test', 'metadata'],
owner := 'ivan']
          </screen>

          <para>Note that <literal>filenode</literal> makes no assumptions
          about the meaning of your metadata, so its handling is entirely left
          to your needs and imagination.</para>
        </section>
      </section>

      <section>
        <title>Complementary notes</title>

        <para>You can use file nodes and PyTables groups to mimic a filesystem
        with files and directories. Since you can store nearly anything you
        want as file metadata, this enables you to use a PyTables file as a
        portable compressed backup, even between radically different
        platforms. Take this with a grain of salt, since node files are
        restricted in their naming (only valid Python identifiers are valid);
        however, remember that you can use node titles and metadata to
        overcome this limitation. Also, you may need to devise some strategy
        to represent special files such as devices, sockets and such (not
        necessarily using <literal>filenode</literal>).</para>

        <para>We are eager to hear your opinion about
        <literal>filenode</literal> and its potential uses. Suggestions to
        improve <literal>filenode</literal> and create other node types are
        also welcome. Do not hesitate to contact us!</para>
      </section>

      <section>
        <title>Current limitations</title>

        <para><literal>filenode</literal> is still a young piece of software,
        so it lacks some functionality. This is a list of known current
        limitations:</para>

        <orderedlist>
          <!-- This limitation no longer applies F. Altet 2006-10-25 -->

          <!--           <listitem><para>Node file names are constrained to PyTables node -->

          <!--           names (i.e.  most valid Python identifiers). For the moment, if you -->

          <!--           want arbitrary names you will have to use a translation map (see -->

          <!--           <xref xrefstyle="select: label" linkend="openFileDescr"/>) or -->

          <!--           the node title. The same restriction applies to attribute -->

          <!--           names.</para></listitem> -->

          <listitem>
            <para>Node files can only be opened for read-only or read and
            append mode. This should be enhanced in the future.</para>
          </listitem>

          <!-- Near future? -->

          <listitem>
            <para>There is no universal newline support yet. This is likely to
            be implemented in a near future.</para>
          </listitem>

          <listitem>
            <para>Sparse files (files with lots of zeros) are not treated
            specially; if you want them to take less space, you should be
            better off using compression.</para>
          </listitem>
        </orderedlist>

        <para>These limitations still make <literal>filenode</literal>
        entirely adequate to work with most binary and text files. Of course,
        suggestions and patches are welcome.</para>
      </section>

      <section>
        <title><literal>filenode</literal> module reference</title>

        <section>
          <title>Global constants</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">NodeType</emphasis></glossterm>

              <glossdef>
                <para>Value for <literal>NODE_TYPE</literal> node system
                attribute.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">NodeTypeVersions</emphasis></glossterm>

              <glossdef>
                <para>Supported values for
                <literal>NODE_TYPE_VERSION</literal> node system
                attribute.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title>Global functions</title>

          <section>
            <title>newNode(h5file, where, name, title="", filters=None,
            expectedsize=1000)</title>

            <para>Creates a new file node object in the specified PyTables
            file object. Additional named arguments <literal>where</literal>
            and <literal>name</literal> must be passed to specify where the
            file node is to be created. Other named arguments such as
            <literal>title</literal> and <literal>filters</literal> may also
            be passed. The special named argument
            <literal>expectedsize</literal>, indicating an estimate of the
            file size in bytes, may also be passed. It returns the file node
            object.</para>
          </section>

          <section>
            <title>openNode(node, mode = 'r')</title>

            <para>Opens an existing file node. Returns a file node object from
            the existing specified PyTables node. If mode is not specified or
            it is <literal>'r'</literal>, the file can only be read, and the
            pointer is positioned at the beginning of the file. If mode is
            <literal>'a+'</literal>, the file can be read and appended, and
            the pointer is positioned at the end of the file.</para>
          </section>
        </section>

        <section>
          <title>The <literal>FileNode</literal> abstract class</title>

          <para>This is the ancestor of <literal>ROFileNode</literal> and
          <literal>RAFileNode</literal> (see below). Instances of these
          classes are returned when <literal>newNode()</literal> or
          <literal>openNode()</literal> are called. It represents a new file
          node associated with a PyTables node, providing a standard Python
          file interface to it.</para>

          <para>This abstract class provides only an implementation of the
          reading methods needed to implement a file-like object over a
          PyTables node. The attribute set of the node becomes available via
          the <literal>attrs</literal> property. You can add attributes there,
          but try to avoid attribute names in all caps or starting with
          <literal>'_'</literal>, since they may clash with internal
          attributes.</para>

          <para>The node used as storage is also made available via the
          read-only attribute <literal>node</literal>. Please do not tamper
          with this object unless unavoidably, since you may break the
          operation of the file node object.</para>

          <para>The <literal>lineSeparator</literal> property contains the
          string used as a line separator, and defaults to
          <literal>os.linesep</literal>. It can be set to any reasonably-sized
          string you want.</para>

          <para>The constructor sets the <literal>closed</literal>,
          <literal>softspace</literal> and <literal>_lineSeparator</literal>
          attributes to their initial values, as well as the
          <literal>node</literal> attribute to <literal>None</literal>.
          Sub-classes should set the <literal>node</literal>,
          <literal>mode</literal> and <literal>offset</literal>
          attributes.</para>

          <para>Version 1 implements the file storage as a
          <literal>UInt8</literal> uni-dimensional
          <literal>EArray</literal>.</para>

          <section>
            <title><literal>FileNode</literal> methods</title>

            <section>
              <title><emphasis
              role="bold">getLineSeparator()</emphasis></title>

              <para>Returns the line separator string.</para>
            </section>

            <section>
              <title><emphasis
              role="bold">setLineSeparator()</emphasis></title>

              <para>Sets the line separator string.</para>
            </section>

            <section>
              <title><emphasis role="bold">getAttrs()</emphasis></title>

              <para>Returns the attribute set of the file node.</para>
            </section>

            <section>
              <title><emphasis role="bold">close()</emphasis></title>

              <para>Flushes the file and closes it. The
              <literal>node</literal> attribute becomes
              <literal>None</literal> and the <literal>attrs</literal>
              property becomes no longer available.</para>
            </section>

            <section>
              <title><emphasis role="bold">next()</emphasis></title>

              <para>Returns the next line of text. Raises
              <literal>StopIteration</literal> when lines are exhausted. See
              <literal>file.next.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">read(size=None)</emphasis></title>

              <para>Reads at most <literal>size</literal> bytes. See
              <literal>file.read.__doc__</literal> for more information</para>
            </section>

            <section>
              <title><emphasis
              role="bold">readline(size=-1)</emphasis></title>

              <para>Reads the next text line. See
              <literal>file.readline.__doc__</literal> for more
              information</para>
            </section>

            <section>
              <title><emphasis
              role="bold">readlines(sizehint=-1)</emphasis></title>

              <para>Reads the text lines. See
              <literal>file.readlines.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">seek(offset,
              whence=0)</emphasis></title>

              <para>Moves to a new file position. See
              <literal>file.seek.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">tell()</emphasis></title>

              <para>Gets the current file position. See
              <literal>file.tell.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">xreadlines()</emphasis></title>

              <para>For backward compatibility. See
              <literal>file.xreadlines.__doc__</literal> for more
              information.</para>
            </section>
          </section>
        </section>

        <section>
          <title>The <literal>ROFileNode</literal> class</title>

          <para>Instances of this class are returned when
          <literal>openNode()</literal> is called in read-only mode
          (<literal>'r'</literal>). This is a descendant of
          <literal>FileNode</literal> class, so it inherits all its methods.
          Moreover, it does not define any other useful method, just some
          protections against users intents to write on file.</para>
        </section>

        <section>
          <title>The <literal>RAFileNode</literal> class</title>

          <para>Instances of this class are returned when either
          <literal>newNode()</literal> is called or when
          <literal>openNode()</literal> is called in append mode
          (<literal>'a+'</literal>). This is a descendant of
          <literal>FileNode</literal> class, so it inherits all its methods.
          It provides additional methods that allow to write on file
          nodes.</para>

          <section>
            <title>flush()</title>

            <para>Flushes the file node. See
            <literal>file.flush.__doc__</literal> for more information.</para>
          </section>

          <section>
            <title>truncate(size=None)</title>

            <para>Truncates the file node to at most <literal>size</literal>
            bytes. Currently, this method only makes sense to grow the file
            node, since data can not be rewritten nor deleted. See
            <literal>file.truncate.__doc__</literal> for more
            information.</para>
          </section>

          <section>
            <title>write(string)</title>

            <para>Writes the string to the file. Writing an empty string does
            nothing, but requires the file to be open. See
            <literal>file.write.__doc__</literal> for more information.</para>
          </section>

          <section>
            <title>writelines(sequence)</title>

            <para>Writes the sequence of strings to the file. See
            <literal>file.writelines.__doc__</literal> for more
            information.</para>
          </section>
        </section>
      </section>
    </chapter>

    <chapter>
      <title>netcdf3 - a PyTables NetCDF3 emulation API</title>

      <section>
        <title>What is <literal>netcdf3</literal>?</title>

        <para>The netCDF format is a popular format for binary files. It is
        portable between machines and self-describing, i.e. it contains the
        information necessary to interpret its contents. A free library
        provides convenient access to these files (see <biblioref
        linkend="NetCDFRef" />). A very nice python interface to that library
        is available in the <literal>Scientific Python NetCDF</literal> module
        (see <biblioref linkend="scientificpythonRef" />). Although it is
        somewhat less efficient and flexible than HDF5, netCDF is geared for
        storing gridded data and is quite easy to use. It has become a de
        facto standard for gridded data, especially in meteorology and
        oceanography. The next version of netCDF (netCDF 4) will actually be a
        software layer on top of HDF5 (see <biblioref
        linkend="NetCDF4Ref" />). The <literal>tables.netcdf3</literal>
        package does not create HDF5 files that are compatible with netCDF 4
        (although this is a long-term goal).</para>
      </section>

      <section>
        <title>Using the <literal>tables.netcdf3</literal> package</title>

        <para>The package <literal>tables.netcdf3</literal> emulates the
        <literal>Scientific.IO.NetCDF</literal> API using PyTables. It
        presents the data in the form of objects that behave very much like
        arrays. A <literal>tables.netcdf3</literal> file contains any number
        of dimensions and variables, both of which have unique names. Each
        variable has a shape defined by a set of dimensions, and optionally
        attributes whose values can be numbers, number sequences, or strings.
        One dimension of a file can be defined as
        <emphasis>unlimited</emphasis>, meaning that the file can grow along
        that direction. In the sections that follow, a step-by-step tutorial
        shows how to create and modify a <literal>tables.netcdf3</literal>
        file. All of the code snippets presented here are included in
        <literal>examples/netCDF_example.py</literal>. The
        <literal>tables.netcdf3</literal> package is designed to be used as a
        drop-in replacement for <literal>Scientific.IO.NetCDF</literal>, with
        only minor modifications to existing code. The differences between
        <literal>tables.netcdf3</literal> and
        <literal>Scientific.IO.NetCDF</literal> are summarized in the last
        section of this chapter.</para>

        <section>
          <title>Creating/Opening/Closing a <literal>tables.netcdf3</literal>
          file</title>

          <para>To create a <literal>tables.netcdf3</literal> file from
          python, you simply call the <literal>NetCDFFile</literal>
          constructor. This is also the method used to open an existing
          <literal>tables.netcdf3</literal> file. The object returned is an
          instance of the <literal>NetCDFFile</literal> class and all future
          access must be done through this object. If the file is open for
          write access (<literal>'w'</literal> or <literal>'a'</literal>), you
          may write any type of new data including new dimensions, variables
          and attributes. The optional <literal>history</literal> keyword
          argument can be used to set the <literal>history</literal>
          <literal>NetCDFFile</literal> global file attribute. Closing the
          <literal>tables.netcdf3</literal> file is accomplished via the
          <literal>close</literal> method of <literal>NetCDFFile</literal>
          object.</para>

          <para>Here's an example:</para>

          <screen>
&gt;&gt;&gt; import tables.netcdf3 as NetCDF
&gt;&gt;&gt; import time
&gt;&gt;&gt; history = 'Created ' + time.ctime(time.time())
&gt;&gt;&gt; file = NetCDF.NetCDFFile('test.h5', 'w', history=history)
&gt;&gt;&gt; file.close()
          </screen>
        </section>

        <section>
          <title>Dimensions in a <literal>tables.netcdf3</literal>
          file</title>

          <para>NetCDF defines the sizes of all variables in terms of
          dimensions, so before any variables can be created the dimensions
          they use must be created first. A dimension is created using the
          <literal>createDimension</literal> method of the
          <literal>NetCDFFile</literal> object. A Python string is used to set
          the name of the dimension, and an integer value is used to set the
          size. To create an <emphasis>unlimited</emphasis> dimension (a
          dimension that can be appended to), the size value is set to
          <literal>None</literal>.</para>

          <screen>
&gt;&gt;&gt; import tables.netcdf3 as NetCDF
&gt;&gt;&gt; file = NetCDF.NetCDFFile('test.h5', 'a')
&gt;&gt;&gt; file.NetCDFFile.createDimension('level', 12)
&gt;&gt;&gt; file.NetCDFFile.createDimension('time', None)
&gt;&gt;&gt; file.NetCDFFile.createDimension('lat', 90)
          </screen>

          <para>All of the dimension names and their associated sizes are
          stored in a Python dictionary.</para>

          <screen>
&gt;&gt;&gt; print file.dimensions
{'lat': 90, 'time': None, 'level': 12}
          </screen>
        </section>

        <section>
          <title>Variables in a <literal>tables.netcdf3</literal> file</title>

          <para>Most of the data in a <literal>tables.netcdf3</literal> file
          is stored in a netCDF variable (except for global attributes). To
          create a netCDF variable, use the <literal>createVariable</literal>
          method of the <literal>NetCDFFile</literal> object. The
          <literal>createVariable</literal> method has three mandatory
          arguments, the variable name (a Python string), the variable
          datatype described by a single character Numeric typecode string
          which can be one of <literal>f</literal> (Float32),
          <literal>d</literal> (Float64), <literal>i</literal> (Int32),
          <literal>l</literal> (Int32), <literal>s</literal> (Int16),
          <literal>c</literal> (CharType - length 1), <literal>F</literal>
          (Complex32), <literal>D</literal> (Complex64) or
          <literal>1</literal> (Int8), and a tuple containing the variable's
          dimension names (defined previously with
          <literal>createDimension</literal>). The dimensions themselves are
          usually defined as variables, called coordinate variables. The
          <literal>createVariable</literal> method returns an instance of the
          <literal>NetCDFVariable</literal> class whose methods can be used
          later to access and set variable data and attributes.</para>

          <screen>
&gt;&gt;&gt; times = file.createVariable('time','d',('time',))
&gt;&gt;&gt; levels = file.createVariable('level','i',('level',))
&gt;&gt;&gt; latitudes = file.createVariable('latitude','f',('lat',))
&gt;&gt;&gt; temp = file.createVariable('temp','f',('time','level','lat',))
&gt;&gt;&gt; pressure = file.createVariable('pressure','i',('level','lat',))
          </screen>

          <para>All of the variables in the file are stored in a Python
          dictionary, in the same way as the dimensions:</para>

          <screen>
&gt;&gt;&gt; print file.variables
{'latitude': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f350&gt;,
'pressure': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f508&gt;,
'level': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f0d0&gt;,
'temp': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f3a0&gt;,
'time': &lt;tables.netcdf3.NetCDFVariable instance at 0x2564c88&gt;}
          </screen>
        </section>

        <section>
          <title>Attributes in a <literal>tables.netcdf3</literal>
          file</title>

          <para>There are two types of attributes in a
          <literal>tables.netcdf3</literal> file, global (or file) and
          variable. Global attributes provide information about the dataset,
          or file, as a whole. Variable attributes provide information about
          one of the variables in the file. Global attributes are set by
          assigning values to <literal>NetCDFFile</literal> instance
          variables. Variable attributes are set by assigning values to
          <literal>NetCDFVariable</literal> instance variables.</para>

          <para>Attributes can be strings, numbers or sequences. Returning to
          our example,</para>

          <screen>
&gt;&gt;&gt; file.description = 'bogus example to illustrate the use of tables.netcdf3'
&gt;&gt;&gt; file.source = 'PyTables Users Guide'
&gt;&gt;&gt; latitudes.units = 'degrees north'
&gt;&gt;&gt; pressure.units = 'hPa'
&gt;&gt;&gt; temp.units = 'K'
&gt;&gt;&gt; times.units = 'days since January 1, 2005'
&gt;&gt;&gt; times.scale_factor = 1</screen>

          <para>The <literal>ncattrs</literal> method of the
          <literal>NetCDFFile</literal> object can be used to retrieve the
          names of all the global attributes. This method is provided as a
          convenience, since using the built-in <literal> dir</literal> Python
          function will return a bunch of private methods and attributes that
          cannot (or should not) be modified by the user. Similarly, the
          <literal>ncattrs</literal> method of a
          <literal>NetCDFVariable</literal> object returns all of the netCDF
          variable attribute names. These functions can be used to easily
          print all of the attributes currently defined, like this</para>

          <screen>
&gt;&gt;&gt; for name in file.ncattrs():
&gt;&gt;&gt;     print 'Global attr', name, '=', getattr(file,name)
Global attr description = bogus example to illustrate the use of tables.netcdf3
Global attr history = Created Mon Nov  7 10:30:56 2005
Global attr source = PyTables Users Guide
          </screen>

          <para>Note that the <literal>ncattrs</literal> function is not part
          of the <literal>Scientific.IO.NetCDF</literal> interface.</para>
        </section>

        <section>
          <title>Writing data to and retrieving data from a
          <literal>tables.netcdf3</literal> variable</title>

          <para>Now that you have a netCDF variable object, how do you put
          data into it? If the variable has no <emphasis>unlimited</emphasis>
          dimension, you just treat it like a Numeric array object and assign
          data to a slice.</para>

          <screen>
&gt;&gt;&gt; import numpy
&gt;&gt;&gt; levels[:] = numpy.arange(12)+1
&gt;&gt;&gt; latitudes[:] = numpy.arange(-89,90,2)
&gt;&gt;&gt; for lev in levels[:]:
&gt;&gt;&gt;     pressure[:,:] = 1000.-100.*lev
&gt;&gt;&gt; print 'levels = ',levels[:]
levels =  [ 1  2  3  4  5  6  7  8  9 10 11 12]
&gt;&gt;&gt; print 'latitudes =\n',latitudes[:]
latitudes =
[-89. -87. -85. -83. -81. -79. -77. -75. -73. -71. -69. -67. -65. -63.
-61. -59. -57. -55. -53. -51. -49. -47. -45. -43. -41. -39. -37. -35.
-33. -31. -29. -27. -25. -23. -21. -19. -17. -15. -13. -11.  -9.  -7.
-5.  -3.  -1.   1.   3.   5.   7.   9.  11.  13.  15.  17.  19.  21.
23.  25.  27.  29.  31.  33.  35.  37.  39.  41.  43.  45.  47.  49.
51.  53.  55.  57.  59.  61.  63.  65.  67.  69.  71.  73.  75.  77.
79.  81.  83.  85.  87.  89.]
          </screen>

          <para>Note that retrieving data from the netCDF variable object
          works just like a Numeric array too. If the netCDF variable has an
          <emphasis>unlimited</emphasis> dimension, and there is not yet an
          entry for the data along that dimension, the
          <literal>append</literal> method must be used.</para>

          <screen>
&gt;&gt;&gt; for n in range(10):
&gt;&gt;&gt;     times.append(n)
&gt;&gt;&gt; print 'times = ',times[:]
times =  [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]
          </screen>

          <para>The data you append must have either the same number of
          dimensions as the <literal>NetCDFVariable</literal>, or one less.
          The shape of the data you append must be the same as the
          <literal>NetCDFVariable</literal> for all of the dimensions except
          the <emphasis>unlimited</emphasis> dimension. The length of the data
          long the <emphasis>unlimited</emphasis> dimension controls how may
          entries along the <emphasis>unlimited</emphasis> dimension are
          appended. If the data you append has one fewer number of dimensions
          than the <literal>NetCDFVariable</literal>, it is assumed that you
          are appending one entry along the <emphasis>unlimited</emphasis>
          dimension. For example, if the <literal>NetCDFVariable</literal> has
          shape <literal>(10,50,100)</literal> (where the dimension length of
          length <literal>10</literal> is the <emphasis>unlimited</emphasis>
          dimension), and you append an array of shape
          <literal>(50,100)</literal>, the <literal>NetCDFVariable</literal>
          will subsequently have a shape of <literal>(11,50,100)</literal>. If
          you append an array with shape <literal>(5,50,100)</literal>, the
          <literal>NetCDFVariable</literal> will have a new shape of
          <literal>(15,50,100)</literal>. Appending an array whose last two
          dimensions do not have a shape <literal>(50,100)</literal> will
          raise an exception. This <literal>append</literal> method does not
          exist in the <literal>Scientific.IO.NetCDF</literal> interface,
          instead entries are appended along the
          <emphasis>unlimited</emphasis> dimension one at a time by assigning
          to a slice. This is the biggest difference between the
          <literal>tables.netcdf3</literal> and
          <literal>Scientific.IO.NetCDF</literal> interfaces.</para>

          <para>Once data has been appended to any variable with an
          <emphasis>unlimited</emphasis> dimension, the
          <literal>sync</literal> method can be used to synchronize the sizes
          of all the other variables with an <emphasis>unlimited</emphasis>
          dimension. This is done by filling in missing values (given by the
          default netCDF <literal>_FillValue</literal>, which is intended to
          indicate that the data was never defined). The
          <literal>sync</literal> method is automatically invoked with a
          <literal>NetCDFFile</literal> object is closed. Once the
          <literal>sync</literal> method has been invoked, the filled-in
          values can be assigned real data with slices.</para>

          <screen>
&gt;&gt;&gt; print 'temp.shape before sync = ',temp.shape
temp.shape before sync =  (0, 12, 90)
&gt;&gt;&gt; file.sync()
&gt;&gt;&gt; print 'temp.shape after sync = ',temp.shape
temp.shape after sync =  (10L, 12, 90)
&gt;&gt;&gt; from numarray import random_array
&gt;&gt;&gt; for n in range(10):
&gt;&gt;&gt;     temp[n] = 10.*random_array.random(pressure.shape)
&gt;&gt;&gt;     print 'time, min/max temp, temp[n,0,0] = ',\
             times[n],min(temp[n].flat),max(temp[n].flat),temp[n,0,0]
time, min/max temp, temp[n,0,0] = 0.0 0.0122650898993 9.99259281158 6.13053750992
time, min/max temp, temp[n,0,0] = 1.0 0.00115821603686 9.9915933609 6.68516159058
time, min/max temp, temp[n,0,0] = 2.0 0.0152112031356 9.98737239838 3.60537290573
time, min/max temp, temp[n,0,0] = 3.0 0.0112022599205 9.99535560608 6.24249696732
time, min/max temp, temp[n,0,0] = 4.0 0.00519315246493 9.99831295013 0.225010097027
time, min/max temp, temp[n,0,0] = 5.0 0.00978941563517 9.9843454361 4.56814193726
time, min/max temp, temp[n,0,0] = 6.0 0.0159023851156 9.99160385132 6.36837291718
time, min/max temp, temp[n,0,0] = 7.0 0.0019518379122 9.99939727783 1.42762875557
time, min/max temp, temp[n,0,0] = 8.0 0.00390585977584 9.9909954071 2.79601073265
time, min/max temp, temp[n,0,0] = 9.0 0.0106026884168 9.99195957184 8.18835449219
          </screen>

          <para>Note that appending data along an
          <emphasis>unlimited</emphasis> dimension always increases the length
          of the variable along that dimension. Assigning data to a variable
          with an <emphasis>unlimited</emphasis> dimension with a slice
          operation does not change its shape. Finally, before closing the
          file we can get a summary of its contents simply by printing the
          <literal>NetCDFFile</literal> object. This produces output very
          similar to running 'ncdump -h' on a netCDF file.</para>

          <screen>
&gt;&gt;&gt; print file
test.h5 {
dimensions:
  lat = 90 ;
  time = UNLIMITED ; // (10 currently)
  level = 12 ;
variables:
  float latitude('lat',) ;
      latitude:units = 'degrees north' ;
  int pressure('level', 'lat') ;
      pressure:units = 'hPa' ;
  int level('level',) ;
  float temp('time', 'level', 'lat') ;
      temp:units = 'K' ;
  double time('time',) ;
      time:scale_factor = 1 ;
      time:units = 'days since January 1, 2005' ;
// global attributes:
      :description = 'bogus example to illustrate the use of tables.netcdf3' ;
      :history = 'Created Wed Nov  9 12:29:13 2005' ;
      :source = 'PyTables Users Guilde' ;
}
          </screen>
        </section>

        <section>
          <title>Efficient compression of <literal>tables.netcdf3</literal>
          variables</title>

          <para>Data stored in <literal>NetCDFVariable</literal> objects is
          compressed on disk by default. The parameters for the default
          compression are determined from a <literal>Filters</literal> class
          instance (see section <xref linkend="FiltersClassDescr"
          xrefstyle="select: label" />) with <literal>complevel=6,
          complib='zlib' and shuffle=True</literal>. To change the default
          compression, simply pass a <literal>Filters</literal> instance to
          <literal>createVariable</literal> with the
          <literal>filters</literal> keyword. If your data only has a certain
          number of digits of precision (say for example, it is temperature
          data that was measured with a precision of <literal>0.1</literal>
          degrees), you can dramatically improve compression by quantizing (or
          truncating) the data using the
          <literal>least_significant_digit</literal> keyword argument to
          <literal>createVariable</literal>. The <emphasis>least significant
          digit</emphasis> is the power of ten of the smallest decimal place
          in the data that is a reliable value. For example if the data has a
          precision of <literal>0.1</literal>, then setting
          <literal>least_significant_digit=1</literal> will cause data the
          data to be quantized using
          <literal>numpy.around(scale*data)/scale</literal>, where
          <literal>scale = 2**bits</literal>, and bits is determined so that a
          precision of <literal>0.1</literal> is retained (in this case
          <literal>bits=4</literal>).</para>

          <para>In our example, try replacing the line</para>

          <screen>
&gt;&gt;&gt; temp = file.createVariable('temp','f',('time','level','lat',))
          </screen>

          <para>with</para>

          <screen>
&gt;&gt;&gt; temp = file.createVariable('temp','f',('time','level','lat',),
                             least_significant_digit=1)
</screen>

          <para>and see how much smaller the resulting file is.</para>

          <para>The <literal>least_significant_digit</literal> keyword
          argument is not allowed in <literal>Scientific.IO.NetCDF</literal>,
          since netCDF version 3 does not support compression. The flexible,
          fast and efficient compression available in HDF5 is the main reason
          I wrote the <literal>tables.netcdf3</literal> package - my netCDF
          files were just getting too big.</para>

          <para>The <literal>createVariable</literal> method has one other
          keyword argument not found in
          <literal>Scientific.IO.NetCDF</literal> -
          <literal>expectedsize</literal>. The <literal>expectedsize</literal>
          keyword can be used to set the expected number of entries along the
          <emphasis>unlimited</emphasis> dimension (default 10000). If you
          expect that your data with have an order of magnitude more or less
          than 10000 entries along the <emphasis>unlimited</emphasis>
          dimension, you may consider setting this keyword to improve
          efficiency (see <xref linkend="expectedRowsOptim" xrefstyle="select:
          label" /> for details).</para>
        </section>
      </section>

      <section>
        <title><literal>tables.netcdf3</literal> package reference</title>

        <section>
          <title>Global constants</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">_fillvalue_dict</emphasis></glossterm>

              <glossdef>
                <para>Dictionary whose keys are
                <literal>NetCDFVariable</literal> single character typecodes
                and whose values are the netCDF _FillValue for that
                typecode.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">ScientificIONetCDF_imported</emphasis></glossterm>

              <glossdef>
                <para><literal>True</literal> if
                <literal>Scientific.IO.NetCDF</literal> is installed and can
                be imported.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title>The <literal>NetCDFFile</literal> class</title>

          <para><emphasis>NetCDFFile(filename, mode='r',
          history=None)</emphasis></para>

          <para>Opens an existing <literal>tables.netcdf3</literal> file (mode
          = <literal>'r'</literal> or <literal>'a'</literal>) or creates a new
          one (mode = <literal>'w'</literal>). The <literal>history</literal>
          keyword can be used to set the <literal>NetCDFFile.history</literal>
          global attribute (if mode = <literal>'a'</literal> or
          <literal>'w'</literal>).</para>

          <para>A <literal>NetCDFFile</literal> object has two standard
          attributes: <literal>dimensions</literal> and
          <literal>variables</literal>. The values of both are dictionaries,
          mapping dimension names to their associated lengths and variable
          names to variables. All other attributes correspond to global
          attributes defined in a netCDF file. Global file attributes are
          created by assigning to an attribute of the
          <literal>NetCDFFile</literal> object.</para>

          <section>
            <title><literal>NetCDFFile</literal> methods</title>

            <section>
              <title>close()</title>

              <para>Closes the file (after invoking the
              <literal>sync</literal> method).</para>
            </section>

            <section>
              <title>sync()</title>

              <para>Synchronizes the size of variables along the
              <emphasis>unlimited</emphasis> dimension, by filling in data
              with default netCDF _FillValue. Returns the length of the
              <emphasis>unlimited</emphasis> dimension. Invoked automatically
              when the <literal>NetCDFFile</literal> object is closed.</para>
            </section>

            <section>
              <title>ncattrs()</title>

              <para>Returns a list with the names of all currently defined
              netCDF global file attributes.</para>
            </section>

            <section>
              <title>createDimension(name, length)</title>

              <para>Creates a netCDF dimension with a name given by the Python
              string <literal>name</literal> and a size given by the integer
              <literal>size</literal>. If <literal>size = None</literal>, the
              dimension is <emphasis>unlimited</emphasis> (i.e. it can grow
              dynamically). There can be only one
              <emphasis>unlimited</emphasis> dimension in a file.</para>
            </section>

            <section>
              <title>createVariable(name, type, dimensions,
              least_significant_digit= None, expectedsize=10000,
              filters=None)</title>

              <para>Creates a new variable with the given <literal>name, type,
              and dimensions</literal>. The type is a one-letter Numeric
              typecode string which can be one of <literal>f</literal>
              (Float32), <literal>d</literal> (Float64), <literal>i</literal>
              (Int32), <literal>l</literal> (Int32), <literal>s</literal>
              (Int16), <literal>c</literal> (CharType - length 1),
              <literal>F</literal> (Complex32), <literal>D</literal>
              (Complex64) or <literal>1</literal> (Int8); the predefined type
              constants from Numeric can also be used. The
              <literal>F</literal> and <literal>D</literal> types are not
              supported in netCDF or Scientific.IO.NetCDF, if they are used in
              a <literal>tables.netcdf3</literal> file, that file cannot be
              converted to a true netCDF file nor can it be shared over the
              internet with OPeNDAP. Dimensions must be a tuple containing
              dimension names (strings) that have been defined previously by
              <literal>createDimensions</literal>. The
              <literal>least_significant_digit</literal> is the power of ten
              of the smallest decimal place in the variable's data that is a
              reliable value. If this keyword is specified, the variable's
              data truncated to this precision to improve compression. The
              <literal>expectedsize</literal> keyword can be used to set the
              expected number of entries along the
              <emphasis>unlimited</emphasis> dimension (default 10000). If you
              expect that your data with have an order of magnitude more or
              less than 10000 entries along the <emphasis>unlimited</emphasis>
              dimension, you may consider setting this keyword to improve
              efficiency (see <xref linkend="expectedRowsOptim"
              xrefstyle="select: label" /> for details). The
              <literal>filters</literal> keyword is a PyTables
              <literal>Filters</literal> instance that describes how to store
              the data on disk. The default corresponds to
              <literal>complevel=6</literal>,
              <literal>complib='zlib'</literal>,
              <literal>shuffle=True</literal> and
              <literal>fletcher32=False</literal>.</para>
            </section>

            <section>
              <title>nctoh5(filename, unpackshort=True, filters=None)</title>

              <para>Imports the data in a netCDF version 3 file
              (<literal>filename</literal>) into a
              <literal>NetCDFFile</literal> object using
              <literal>Scientific.IO.NetCDF</literal>
              (<literal>ScientificIONetCDF_imported</literal> must be
              <literal>True</literal>). If
              <literal>unpackshort=True</literal>, data packed as short
              integers (type <literal>s</literal>) in the netCDF file will be
              unpacked to type <literal>f</literal> using the
              <literal>scale_factor</literal> and
              <literal>add_offset</literal> netCDF variable attributes. The
              <literal>filters</literal> keyword can be set to a PyTables
              <literal>Filters</literal> instance to change the default
              parameters used to compress the data in the
              <literal>tables.netcdf3</literal> file. The default corresponds
              to <literal>complevel=6</literal>,
              <literal>complib='zlib'</literal>,
              <literal>shuffle=True</literal> and
              <literal>fletcher32=False</literal>.</para>
            </section>

            <section>
              <title>h5tonc(filename, packshort=False, scale_factor=None,
              add_offset=None)</title>

              <para>Exports the data in a <literal>tables.netcdf3</literal>
              file defined by the <literal>NetCDFFile</literal> instance into
              a netCDF version 3 file using
              <literal>Scientific.IO.NetCDF</literal>
              (<literal>ScientificIONetCDF_imported</literal> must be
              <literal>True</literal>). If
              <literal>packshort=True&gt;</literal> the dictionaries
              <literal>scale_factor</literal> and
              <literal>add_offset</literal> are used to pack data of type
              <literal>f</literal> as short integers (of type
              <literal>s</literal>) in the netCDF file. Since netCDF version 3
              does not provide automatic compression, packing as short
              integers is a commonly used way of saving disk space (see this
              <ulink
              url="http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml">page</ulink>
              for more details). The keys of these dictionaries are the
              variable names to pack, the values are the scale_factors and
              offsets to use in the packing. The data are packed so that the
              original Float32 values can be reconstructed by multiplying the
              <literal>scale_factor</literal> and adding
              <literal>add_offset</literal>. The resulting netCDF file will
              have the <literal>scale_factor</literal> and
              <literal>add_offset</literal> variable attributes set
              appropriately.</para>
            </section>
          </section>
        </section>

        <section>
          <title>The <literal>NetCDFVariable</literal> class</title>

          <para>The <literal>NetCDFVariable</literal> constructor is not
          called explicitly, rather an <literal>NetCDFVarible</literal>
          instance is returned by an invocation of
          <literal>NetCDFFile.createVariable</literal>.
          <literal>NetCDFVariable</literal> objects behave like arrays, and
          have the standard attributes of arrays (such as
          <literal>shape</literal>). Data can be assigned or extracted from
          <literal>NetCDFVariable</literal> objects via slices.</para>

          <section>
            <title><literal>NetCDFVariable</literal> methods</title>

            <section>
              <title>typecode()</title>

              <para>Returns a single character typecode describing the type of
              the variable, one of <literal>f</literal> (Float32),
              <literal>d</literal> (Float64), <literal>i</literal> (Int32),
              <literal>l</literal> (Int32), <literal>s</literal> (Int16),
              <literal>c</literal> (CharType - length 1), <literal>F</literal>
              (Complex32), <literal>D</literal> (Complex64) or
              <literal>1</literal> (Int8).</para>
            </section>

            <section>
              <title>append(data)</title>

              <para>Append data to a variable along its
              <emphasis>unlimited</emphasis> dimension. The data you append
              must have either the same number of dimensions as the
              <literal>NetCDFVariable</literal>, or one less. The shape of the
              data you append must be the same as the
              <literal>NetCDFVariable</literal> for all of the dimensions
              except the <emphasis>unlimited</emphasis> dimension. The length
              of the data long the <emphasis>unlimited</emphasis> dimension
              controls how may entries along the
              <emphasis>unlimited</emphasis> dimension are appended. If the
              data you append has one fewer number of dimensions than the
              <literal>NetCDFVariable</literal>, it is assumed that you are
              appending one entry along the <emphasis>unlimited</emphasis>
              dimension. For variables without an
              <emphasis>unlimited</emphasis> dimension, data can simply be
              assigned to a slice without using the <literal>append</literal>
              method.</para>
            </section>

            <section>
              <title>ncattrs()</title>

              <para>Returns a list with all the names of the currently defined
              netCDF variable attributes.</para>
            </section>

            <section>
              <title>assignValue(data)</title>

              <para>Provided for compatiblity with
              <literal>Scientific.IO.NetCDF</literal>. Assigns data to the
              variable. If the variable has an <emphasis>unlimited</emphasis>
              dimension, it is equivalent to <literal>append(data)</literal>.
              If the variable has no <emphasis>unlimited</emphasis> dimension,
              it is equivalent to assigning data to the variable with the
              slice <literal>[:]</literal>.</para>
            </section>

            <section>
              <title>getValue()</title>

              <para>Provided for compatiblity with
              <literal>Scientific.IO.NetCDF</literal>. Returns all the data in
              the variable. Equivalent to extracting the slice
              <literal>[:]</literal> from the variable.</para>
            </section>
          </section>
        </section>
      </section>

      <section>
        <title>Converting between true netCDF files and
        <literal>tables.netcdf3</literal> files</title>

        <para>If <literal>Scientific.IO.NetCDF</literal> is installed,
        <literal>tables.netcdf3</literal> provides facilities for converting
        between true netCDF version 3 files and
        <literal>tables.netcdf3</literal> hdf5 files via the
        <literal>NetCDFFile.h5tonc()</literal> and
        <literal>NetCDFFile.nctoh5()</literal> class methods. Also, the
        <literal>nctoh5</literal> command-line utility (see <xref
        linkend="nctoh5Descr" xrefstyle="select: label" />) uses the
        <literal>NetCDFFile.nctoh5()</literal> class method.</para>

        <para>As an example, look how to convert a
        <literal>tables.netcdf3</literal> hdf5 file to a true netCDF version 3
        file (named <literal>test.nc</literal>)</para>

        <screen>
&gt;&gt;&gt; scale_factor = {'temp': 1.75e-4}
&gt;&gt;&gt; add_offset = {'temp': 5.}
&gt;&gt;&gt; file.h5tonc('test.nc',packshort=True, \
               scale_factor=scale_factor,add_offset=add_offset)
packing temp as short integers ...
&gt;&gt;&gt; file.close()
        </screen>

        <para>The dictionaries <literal>scale_factor</literal> and
        <literal>add_offset</literal> are used to optionally pack the data as
        short integers in the netCDF file. Since netCDF version 3 does not
        provide automatic compression, packing as short integers is a commonly
        used way of saving disk space (see this <ulink
        url="http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml">page</ulink>
        for more details). The keys of these dictionaries are the variable
        names to pack, the values are the scale_factors and offsets to use in
        the packing. The resulting netCDF file will have the
        <literal>scale_factor</literal> and <literal>add_offset</literal>
        variable attributes set appropriately.</para>

        <para>To convert the netCDF file back to a
        <literal>tables.netcdf3</literal> hdf5 file:</para>

        <screen>
&gt;&gt;&gt; history = 'Convert from netCDF ' + time.ctime(time.time())
&gt;&gt;&gt; file = NetCDF.NetCDFFile('test2.h5', 'w', history=history)
&gt;&gt;&gt; nobjects, nbytes = file.nctoh5('test.nc',unpackshort=True)
&gt;&gt;&gt; print nobjects,' objects converted from netCDF, totaling',nbytes,'bytes'
5  objects converted from netCDF, totaling 48008 bytes
&gt;&gt;&gt; temp = file.variables['temp']
&gt;&gt;&gt; times = file.variables['time']
&gt;&gt;&gt; print 'temp.shape after h5 --&gt; netCDF --&gt; h5 conversion = ',temp.shape
temp.shape after h5 --&gt; netCDF --&gt; h5 conversion =  (10L, 12, 90)
&gt;&gt;&gt; for n in range(10):
&gt;&gt;&gt;     print 'time, min/max temp, temp[n,0,0] = ',\
             times[n],min(temp[n].flat),max(temp[n].flat),temp[n,0,0]
time, min/max temp, temp[n,0,0] = 0.0 0.0123250000179 9.99257469177 6.13049983978
time, min/max temp, temp[n,0,0] = 1.0 0.00130000000354 9.99152469635 6.68507480621
time, min/max temp, temp[n,0,0] = 2.0 0.0153000000864 9.98732471466 3.60542488098
time, min/max temp, temp[n,0,0] = 3.0 0.0112749999389 9.99520015717 6.2423248291
time, min/max temp, temp[n,0,0] = 4.0 0.00532499980181 9.99817466736 0.225124999881
time, min/max temp, temp[n,0,0] = 5.0 0.00987500045449 9.98417472839 4.56827497482
time, min/max temp, temp[n,0,0] = 6.0 0.01600000076 9.99152469635 6.36832523346
time, min/max temp, temp[n,0,0] = 7.0 0.00200000009499 9.99922466278 1.42772495747
time, min/max temp, temp[n,0,0] = 8.0 0.00392499985173 9.9908246994 2.79605007172
time, min/max temp, temp[n,0,0] = 9.0 0.0107500003651 9.99187469482 8.18832492828
&gt;&gt;&gt; file.close()
        </screen>

        <para>Setting <literal>unpackshort=True</literal> tells
        <literal>nctoh5</literal> to unpack all of the variables which have
        the <literal>scale_factor</literal> and <literal>add_offset</literal>
        attributes back to floating point arrays. Note that
        <literal>tables.netcdf3</literal> files have some features not
        supported in netCDF (such as Complex data types and the ability to
        make any dimension <emphasis>unlimited</emphasis>).
        <literal>tables.netcdf3</literal> files which utilize these features
        cannot be converted to netCDF using
        <literal>NetCDFFile.h5tonc</literal>.</para>
      </section>

      <section>
        <title><literal>tables.netcdf3</literal> file structure</title>

        <para>A <literal>tables.netcdf3</literal> file consists of array
        objects (either <literal> EArrays</literal> or
        <literal>CArrays</literal>) located in the root group of a pytables
        hdf5 file. Each of the array objects must have a
        <literal>dimensions</literal> attribute, consisting of a tuple of
        dimension names (the length of this tuple should be the same as the
        rank of the array object). Any array objects with one of the supported
        datatypes in a pytables file that conforms to this simple structure
        can be read with the <literal>tables.netcdf3</literal> package.</para>
      </section>

      <section>
        <title>Sharing data in <literal>tables.netcdf3</literal> files over
        the internet with OPeNDAP</title>

        <para><literal>tables.netcdf3</literal> datasets can be shared over
        the internet with the OPeNDAP protocol (<ulink
        url="http://opendap.org">http://opendap.org</ulink>), via the python
        opendap module (<ulink
        url="http://opendap.oceanografia.org">http://opendap.oceanografia.org</ulink>).
        A plugin for the python opendap server is included with the pytables
        distribution (<literal>contrib/h5_dap_plugin.py</literal>). Simply
        copy that file into the <literal>plugins</literal> directory of the
        opendap python module source distribution, run <literal>python
        setup.py install</literal>, point the opendap server to the directory
        containing your <literal>tables.netcdf3</literal> files, and away you
        go. Any OPeNDAP aware client (such as Matlab or IDL) will now be able
        to access your data over http as if it were a local disk file. The
        only restriction is that your <literal>tables.netcdf3</literal> files
        must have the extension <literal>.h5</literal> or
        <literal>.hdf5</literal>. Unfortunately,
        <literal>tables.netcdf3</literal> itself cannot act as an OPeNDAP
        client, although there is a client included in the opendap python
        module, and <literal>Scientific.IO.NetCDF</literal> can act as an
        OPeNDAP client if it is linked with the OPeNDAP netCDF client library.
        Either of these python modules can be used to remotely acess
        <literal>tables.netcdf3</literal> datasets with OPeNDAP.</para>
      </section>

      <section>
        <title>Differences between the <literal>Scientific.IO.NetCDF</literal>
        API and the <literal>tables.netcdf3</literal> API</title>

        <orderedlist>
          <listitem>
            <para><literal>tables.netcdf3</literal> data is stored in an HDF5
            file instead of a netCDF file.</para>
          </listitem>

          <listitem>
            <para>Although each variable can have only one
            <emphasis>unlimited</emphasis> dimension in a
            <literal>tables.netcdf3</literal> file, it need not be the first
            as in a true NetCDF file. Complex data types <literal>F</literal>
            (Complex32) and <literal>D</literal> (Complex64) are supported in
            <literal>tables.netcdf3</literal>, but are not supported in netCDF
            (or <literal>Scientific.IO.NetCDF</literal>). Files with variables
            that have these datatypes, or an <emphasis>unlimited</emphasis>
            dimension other than the first, cannot be converted to netCDF
            using <literal>h5tonc</literal>.</para>
          </listitem>

          <listitem>
            <para>Variables in a <literal>tables.netcdf3</literal> file are
            compressed on disk by default using HDF5 zlib compression with the
            <emphasis>shuffle</emphasis> filter. If the
            <emphasis>least_significant_digit</emphasis> keyword is used when
            a variable is created with the <literal>createVariable
            method</literal>, data will be truncated (quantized) before being
            written to the file. This can significantly improve compression.
            For example, if <literal>least_significant_digit=1</literal>, data
            will be quantized using
            <literal>numpy.around(scale*data)/scale</literal>, where
            <literal>scale = 2**bits</literal>, and bits is determined so that
            a precision of 0.1 is retained (in this case
            <literal>bits=4</literal>). From <ulink
            url="http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml">http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml</ulink>:
            <quote>least_significant_digit -- power of ten of the smallest
            decimal place in unpacked data that is a reliable value.</quote>
            Automatic data compression is not available in netCDF version 3,
            and hence is not available in the
            <literal>Scientific.IO.NetCDF</literal> module.</para>
          </listitem>

          <listitem>
            <para>In <literal>tables.netcdf3</literal>, data must be appended
            to a variable with an <emphasis>unlimited</emphasis> dimension
            using the <literal>append</literal> method of the
            <literal>netCDF</literal> variable object. In
            <literal>Scientific.IO.NetCDF</literal>, data can be added along
            an <emphasis>unlimited</emphasis> dimension by assigning it to a
            slice (there is no append method). The <literal>sync</literal>
            method of a <literal>tables.netcdf3 NetCDFVariable</literal>
            object synchronizes the size of all variables with an
            <emphasis>unlimited</emphasis> dimension by filling in data using
            the default netCDF <literal>_FillValue</literal>. The
            <literal>sync</literal> method is automatically invoked with a
            <literal>NetCDFFile</literal> object is closed. In
            <literal>Scientific.IO.NetCDF</literal>, the
            <literal>sync()</literal> method flushes the data to disk.</para>
          </listitem>

          <listitem>
            <para>The <literal>tables.netcdf3 createVariable()</literal>
            method has three extra optional keyword arguments not found in the
            <literal>Scientific.IO.NetCDF</literal> interface,
            <emphasis>least_significant_digit</emphasis> (see item (2) above),
            <emphasis>expectedsize</emphasis> and
            <emphasis>filters</emphasis>. The
            <emphasis>expectedsize</emphasis> keyword applies only to
            variables with an <emphasis>unlimited</emphasis> dimension, and is
            an estimate of the number of entries that will be added along that
            dimension (default 1000). This estimate is used to optimize HDF5
            file access and memory usage. The <emphasis>filters</emphasis>
            keyword is a PyTables filters instance that describes how to store
            the data on disk. The default corresponds to
            <literal>complevel=6</literal>, <literal>complib='zlib'</literal>,
            <literal>shuffle=True</literal> and
            <literal>fletcher32=False</literal>.</para>
          </listitem>

          <listitem>
            <para><literal>tables.netcdf3</literal> data can be saved to a
            true netCDF file using the <literal>NetCDFFile</literal> class
            method <literal>h5tonc</literal> (if
            <literal>Scientific.IO.NetCDF</literal> is installed). The
            <emphasis>unlimited</emphasis> dimension must be the first (for
            all variables in the file) in order to use the
            <literal>h5tonc</literal> method. Data can also be imported from a
            true netCDF file and saved in an HDF5
            <literal>tables.netcdf3</literal> file using the
            <literal>nctoh5</literal> class method.</para>
          </listitem>

          <listitem>
            <para>In <literal>tables.netcdf3</literal> a list of attributes
            corresponding to global netCDF attributes defined in the file can
            be obtained with the <literal>NetCDFFile ncattrs </literal>method.
            Similarly, netCDF variable attributes can be obtained with the
            <literal>NetCDFVariable</literal> <literal>ncattrs</literal>
            method. These functions are not available in the
            <literal>Scientific.IO.NetCDF</literal> API.</para>
          </listitem>

          <listitem>
            <para>You should not define <literal>tables.netcdf3</literal>
            global or variable attributes that start with
            <literal>_NetCDF_</literal>. Those names are reserved for internal
            use.</para>
          </listitem>

          <listitem>
            <para>Output similar to 'ncdump -h' can be obtained by simply
            printing a <literal>tables.netcdf3</literal>
            <literal>NetCDFFile</literal> instance.</para>
          </listitem>
        </orderedlist>
      </section>
    </chapter>
  </part>

  <part label="III">
    <title>Appendixes</title>

    <appendix id="datatypesSupported">
      <title>Supported data types in PyTables</title>

      <para>All PyTables datasets can handle the complete set of data types
      supported by the NumPy (see <biblioref linkend="NumPy" />),
      <literal>numarray</literal> (see <biblioref linkend="Numarray" />) and
      Numeric (see <biblioref linkend="Numeric" />) packages in Python. The
      data types for table fields can be set via instances of the
      <literal>Col</literal> class and its descendants (see <xref
      linkend="ColClassDescr" xrefstyle="select: label" />), while the data
      type of array elements can be set through the use of the
      <literal>Atom</literal> class and its descendants (see <xref
      linkend="AtomClassDescr" xrefstyle="select: label" />).</para>

      <para>PyTables uses ordinary strings to represent its
      <emphasis>types</emphasis>, with most of them matching the names of
      NumPy scalar types. Usually, a PyTables type consists of two parts: a
      <emphasis>kind</emphasis> and a <emphasis>precision</emphasis> in bits.
      The precision may be omitted in types with just one supported precision
      (like <literal>bool</literal>) or with a non-fixed size (like
      <literal>string</literal>).</para>

      <para>There are eight kinds of types supported by PyTables:
      <itemizedlist>
          <listitem>
            <para><literal>bool</literal>: Boolean (true/false) types.
            Supported precisions: 8 (default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>int</literal>: Signed integer types. Supported
            precisions: 8, 16, 32 (default) and 64 bits.</para>
          </listitem>

          <listitem>
            <para><literal>uint</literal>: Unsigned integer types. Supported
            precisions: 8, 16, 32 (default) and 64 bits.</para>
          </listitem>

          <listitem>
            <para><literal>float</literal>: Floating point types. Supported
            precisions: 32 and 64 (default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>complex</literal>: Complex number types. Supported
            precisions: 64 (32+32) and 128 (64+64, default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>string</literal>: Raw string types. Supported
            precisions: 8-bit positive multiples.</para>
          </listitem>

          <listitem>
            <para><literal>time</literal>: Data/time types. Supported
            precisions: 32 and 64 (default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>enum</literal>: Enumerated types. Precision depends
            on base type.</para>
          </listitem>
        </itemizedlist></para>

      <para>The <literal>time</literal> and <literal>enum</literal> kinds are
      a little bit special, since they represent HDF5 types which have no
      direct Python counterpart, though atoms of these kinds have a
      more-or-less equivalent NumPy data type.</para>

      <para>There are two types of <literal>time</literal>: 4-byte signed
      integer (<literal>time32</literal>) and 8-byte double precision floating
      point (<literal>time64</literal>). Both of them reflect the number of
      seconds since the Unix epoch, i.e. Jan 1 00:00:00 UTC 1970. They are
      stored in memory as NumPy's <literal>int32</literal> and
      <literal>float64</literal>, respectively, and in the HDF5 file using the
      <literal>H5T_TIME</literal> class. Integer times are stored on disk as
      such, while floating point times are split into two signed integer
      values representing seconds and microseconds (beware: smaller decimals
      will be lost!).</para>

      <para>PyTables also supports HDF5 <literal>H5T_ENUM</literal>
      <emphasis>enumerations</emphasis> (restricted sets of unique name and
      unique value pairs). The NumPy representation of an enumerated value (an
      <literal>Enum</literal>, see <xref linkend="EnumClassDescr"
      xrefstyle="select: label" />) depends on the concrete <emphasis>base
      type</emphasis> used to store the enumeration in the HDF5
      file. Currently, only scalar integer values (both signed and unsigned)
      are supported in enumerations. This restriction may be lifted when HDF5
      supports other kinds on enumerated values.</para>

      <para>Here you have a quick reference to the complete set of supported
      data types:</para>

      <table align="center" id="datatypesSupportedTable">
        <title>Data types supported for array elements and tables columns in
        PyTables.</title>

        <tgroup cols="5">
          <colspec align="left" colname="c1" />

          <colspec align="left" colname="c2" />

          <colspec align="left" colname="c3" />

          <colspec align="center" colname="c4" />

          <colspec align="left" colname="c5" />

          <thead>
            <row>
              <entry align="left">Type Code</entry>

              <entry align="left">Description</entry>

              <entry align="left">C Type</entry>

              <entry align="center">Size (in bytes)</entry>

              <entry align="left">Python Counterpart</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry align="left">bool</entry>

              <entry align="left">boolean</entry>

              <entry align="left">unsigned char</entry>

              <entry align="center">1</entry>

              <entry align="left">bool</entry>
            </row>

            <row>
              <entry align="left">int8</entry>

              <entry align="left">8-bit integer</entry>

              <entry align="left">signed char</entry>

              <entry align="center">1</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">uint8</entry>

              <entry align="left">8-bit unsigned integer</entry>

              <entry align="left">unsigned char</entry>

              <entry align="center">1</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">int16</entry>

              <entry align="left">16-bit integer</entry>

              <entry align="left">short</entry>

              <entry align="center">2</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">uint16</entry>

              <entry align="left">16-bit unsigned integer</entry>

              <entry align="left">unsigned short</entry>

              <entry align="center">2</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">int32</entry>

              <entry align="left">integer</entry>

              <entry align="left">int</entry>

              <entry align="center">4</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">uint32</entry>

              <entry align="left">unsigned integer</entry>

              <entry align="left">unsigned int</entry>

              <entry align="center">4</entry>

              <entry align="left">long</entry>
            </row>

            <row>
              <entry align="left">int64</entry>

              <entry align="left">64-bit integer</entry>

              <entry align="left">long long</entry>

              <entry align="center">8</entry>

              <entry align="left">long</entry>
            </row>

            <row>
              <entry align="left">uint64</entry>

              <entry align="left">unsigned 64-bit integer</entry>

              <entry align="left">unsigned long long</entry>

              <entry align="center">8</entry>

              <entry align="left">long</entry>
            </row>

            <row>
              <entry align="left">float32</entry>

              <entry align="left">single-precision float</entry>

              <entry align="left">float</entry>

              <entry align="center">4</entry>

              <entry align="left">float</entry>
            </row>

            <row>
              <entry align="left">float64</entry>

              <entry align="left">double-precision float</entry>

              <entry align="left">double</entry>

              <entry align="center">8</entry>

              <entry align="left">float</entry>
            </row>

            <row>
              <entry align="left">complex64</entry>

              <entry align="left">single-precision complex</entry>

              <entry align="left">struct {float r, i;}</entry>

              <entry align="center">8</entry>

              <entry align="left">complex</entry>
            </row>

            <row>
              <entry align="left">complex128</entry>

              <entry align="left">double-precision complex</entry>

              <entry align="left">struct {double r, i;}</entry>

              <entry align="center">16</entry>

              <entry align="left">complex</entry>
            </row>

            <row>
              <entry align="left">String</entry>

              <entry align="left">arbitrary length string</entry>

              <entry align="left">char[]</entry>

              <entry align="center">*</entry>

              <entry align="left">str</entry>
            </row>

            <row>
              <entry align="left">time32</entry>

              <entry align="left">integer time</entry>

              <entry align="left">POSIX's time_t</entry>

              <entry align="center">4</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">time64</entry>

              <entry align="left">floating point time</entry>

              <entry align="left">POSIX's struct timeval</entry>

              <entry align="center">8</entry>

              <entry align="left">float</entry>
            </row>

            <row>
              <entry align="left">enum</entry>

              <entry align="left">enumerated value</entry>

              <entry align="left">enum</entry>

              <entry align="center">-</entry>

              <entry align="left">-</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </appendix>

    <appendix id="conditionSyntax">
      <title>Condition syntax</title>

      <para>Conditions in PyTables are used in methods related with in-kernel
      and indexed searches such as <literal>Table.where()</literal> (see <xref
      linkend="Table.where" />) or <literal>Table.readWhere()</literal> (see
      <xref linkend="Table.readWhere" />). They are interpreted using a
      customized version of <ulink
      url="http://scipy.org/SciPyPackages/NumExpr">Numexpr</ulink>, a powerful
      package for C-speed computation of array operations, originally written
      by David Cooke.</para>

      <para>A condition on a table is just a <emphasis>string</emphasis>
      containing a Python expression involving <emphasis>at least one
      column</emphasis>, and maybe some constants and external variables, all
      combined with algebraic operators and functions. The result of a valid
      condition is always a <emphasis>boolean array</emphasis> of the same
      length as the table, where the <emphasis>i</emphasis>-th element is true
      if the value of the expression on the <emphasis>i</emphasis>-th row of
      the table evaluates to true <footnote>
          <para>That is the reason why multidimensional fields in a table are
          not supported in conditions, since the truth value of each resulting
          multidimensional boolean value is not obvious.</para>
        </footnote>. Usually, a method using a condition will only consider
      the rows where the boolean result is true.</para>

      <para>For instance, the condition <literal>'sqrt(x*x + y*y) &lt;
      1'</literal> applied on a table with <literal>x</literal> and
      <literal>y</literal> columns consisting of floating point numbers
      results in a boolean array where the <emphasis>i</emphasis>-th element
      is true if (unsurprisingly) the value of the square root of the sum of
      squares of <literal>x</literal> and <literal>y</literal> is less than 1.
      The <literal>sqrt()</literal> function works element-wise, the 1
      constant is adequately broadcast to an array of ones of the length of
      the table for evaluation, and the <emphasis>less than</emphasis>
      operator makes the result a valid boolean array. A condition like
      <literal>'mycolumn'</literal> alone will not usually be valid, unless
      <literal>mycolumn</literal> is itself a column of scalar, boolean
      values.</para>

      <para>In the previous conditions, <literal>mycolumn</literal>,
      <literal>x</literal> and <literal>y</literal> are examples of
      <emphasis>variables</emphasis> which are associated with columns.
      Methods supporting conditions do usually provide their own ways of
      binding variable names to columns and other values. You can read the
      documentation of <literal>Table.where()</literal> (see <xref
      linkend="Table.where" />) for more information on that. Also, please
      note that the names <literal>None</literal>, <literal>True</literal> and
      <literal>False</literal>, besides the names of functions (see below)
      <emphasis>can not be overridden</emphasis>, but you can always define
      other new names for the objects you intend to use.</para>

      <para>Values in a condition may have the following types: <itemizedlist>
          <listitem>
            <para>8-bit boolean (<literal>bool</literal>).</para>
          </listitem>

          <listitem>
            <para>32-bit signed integer (<literal>int</literal>).</para>
          </listitem>

          <listitem>
            <para>64-bit signed integer (<literal>long</literal>).</para>
          </listitem>

          <listitem>
            <para>64-bit, double-precision floating point number
            (<literal>float</literal>).</para>
          </listitem>

          <listitem>
            <para>2x64-bit, double-precision complex number
            (<literal>complex</literal>).</para>
          </listitem>

          <listitem>
            <para>Raw string of bytes (<literal>str</literal>).</para>
          </listitem>
        </itemizedlist> The types in PyTables conditions are somewhat stricter
      than those of Python. For instance, the <emphasis>only</emphasis> valid
      constants for booleans are <literal>True</literal> and
      <literal>False</literal>, and they are <emphasis>never</emphasis>
      automatically cast to integers. The type strengthening also affects the
      availability of operators and functions. Beyond that, the usual type
      inference rules apply.</para>

      <para>Conditions support the set of operators listed below:
      <itemizedlist>
          <listitem>
            <para>Logical operators: &amp;, |, ~.</para>
          </listitem>

          <listitem>
            <para>Comparison operators: &lt;, &lt;=, ==, !=, &gt;=, &gt;.</para>
          </listitem>

          <listitem>
            <para>Unary arithmetic operators: -.</para>
          </listitem>

          <listitem>
            <para>Binary arithmetic operators: +, -, *, /, **, %.</para>
          </listitem>
        </itemizedlist> Types do not support all operators. Boolean values
      only support logical and strict (in)equality comparison operators, while
      strings only support comparisons, numbers do not work with logical
      operators, and complex comparisons can only check for strict
      (in)equality. Unsupported operations (including invalid castings) raise
      <literal>NotImplementedError</literal> exceptions.</para>

      <para>You may have noticed the special meaning of the usually bitwise
      operators <literal>&amp;</literal>, <literal>|</literal> and
      <literal>~</literal>. Because of the way Python handles the
      short-circuiting of logical operators and the truth values of their
      operands, conditions must use the bitwise operator equivalents instead.
      This is not difficult to remember, but you must be careful because
      bitwise operators have a <emphasis>higher precedence</emphasis> than
      logical operators. For instance, <literal>'a and b == c'</literal>
      (<emphasis><literal>a</literal> is true AND <literal>b</literal> is
      equal to <literal>c</literal></emphasis>) is <emphasis>not</emphasis>
      equivalent to <literal>'a &amp; b == c'</literal>
      (<emphasis><literal>a</literal> AND <literal>b</literal> is equal to
      <literal>c</literal>)</emphasis>. The safest way to avoid confusions is
      to <emphasis>use parentheses</emphasis> around logical operators, like
      this: <literal>'a &amp; (b == c)'</literal>. Another effect of
      short-circuiting is that expressions like <literal>'0 &lt; x &lt;
      1'</literal> will <emphasis>not</emphasis> work as expected; you should
      use <literal>'(0 &lt; x) &amp; (x &lt; 1)'</literal> <footnote>
          <para>All of this may be solved if Python supported overloadable
          boolean operators (see PEP 335) or some kind of non-shortcircuiting
          boolean operators (like C's <literal>&amp;&amp;</literal>,
          <literal>||</literal> and <literal>!</literal>).</para>
        </footnote></para>

      <para>You can also use the following functions in conditions:
      <itemizedlist>
          <listitem>
            <para><literal>where(bool, number1, number2): number</literal> —
            <literal>number1</literal> if the <literal>bool</literal>
            condition is true, <literal>number2</literal> otherwise.</para>
          </listitem>

          <listitem>
            <para><literal>{sin,cos,tan}(float|complex):
            float|complex</literal> — trigonometric sinus, cosinus or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>{arcsin,arccos,arctan}(float|complex):
            float|complex</literal> — trigonometric inverse sinus, cosinus or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>arctan2(float1, float2): float</literal> —
            trigonometric inverse tangent of
            <literal>float1/float2</literal>.</para>
          </listitem>

          <listitem>
            <para><literal>{sinh,cosh,tanh}(float|complex):
            float|complex</literal> — hyperbolic sinus, cosinus or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>sqrt(float|complex): float|complex</literal> —
            square root.</para>
          </listitem>

          <listitem>
            <para><literal>{real,imag}(complex): float</literal> — real or
            imaginary part of complex.</para>
          </listitem>

          <listitem>
            <para><literal>complex(float, float): complex</literal> — complex
            from real and imaginary parts.</para>
          </listitem>

          <listitem>
            <para><literal>pow(complex, complex): complex</literal> — complex
            exponentiation.</para>
          </listitem>
        </itemizedlist></para>
    </appendix>

    <appendix id="NestedRecArrayClassDescr">
      <title>Using nested record arrays</title>

      <section>
        <title>Introduction</title>

        <para>Nested record arrays are a generalization of the record array
        concept. Basically, a nested record array is a record array that
        supports nested datatypes. It means that columns can contain not only
        regular datatypes but also nested datatypes.</para>

        <para>Each nested record array is a <literal>NestedRecArray</literal>
        object in the <literal>tables.nra</literal> package. Nested record
        arrays are intended to be as compatible as possible with ordinary
        record arrays (in fact the <literal>NestedRecArray</literal> class
        inherits from <literal>RecArray</literal>). As a consequence, the user
        can deal with nested record arrays nearly in the same way that he does
        with ordinary record arrays.</para>

        <para>The easiest way to create a nested record array is to use the
        <literal>array()</literal> function in the
        <literal>tables.nra</literal> package. The only difference between
        this function and its non-nested capable analogous is that now, we
        <emphasis>must</emphasis> provide an structure for the buffer being
        stored. For instance:</para>

        <screen>&gt;&gt;&gt; from tables.nra import array
&gt;&gt;&gt; nra1 = array(
...     [(1, (0.5, 1.0), ('a1', 1j)), (2, (0, 0), ('a2', 1+.1j))],
...     formats=['Int64', '(2,)Float32', ['a2', 'Complex64']])</screen>

        <para>will create a two rows nested record array with two regular
        fields (columns), and one nested field with two sub-fields.</para>

        <para>The field structure of the nested record array is specified by
        the keyword argument <literal>formats</literal>. This argument only
        supports sequences of strings and other sequences. Each string defines
        the shape and type of a non-nested field. Each sequence contains the
        formats of the sub-fields of a nested field. Optionally, we can also
        pass an additional <literal>names</literal> keyword argument
        containing the names of fields and sub-fields:</para>

        <screen>&gt;&gt;&gt; nra2 = array(
...     [(1, (0.5, 1.0), ('a1', 1j)), (2, (0, 0), ('a2', 1+.1j))],
...     names=['id', 'pos', ('info', ['name', 'value'])],
...     formats=['Int64', '(2,)Float32', ['a2', 'Complex64']])</screen>

        <para>The names argument only supports lists of strings and 2-tuples.
        Each string defines the name of a non-nested field. Each 2-tuple
        contains the name of a nested field and a list describing the names of
        its sub-fields. If the <literal>names</literal> argument is not passed
        then all fields are automatically named (<literal>c1</literal>,
        <literal>c2</literal> etc. on each nested field) so, in our first
        example, the fields will be named as <literal>['c1', 'c2', ('c3',
        ['c1', 'c2'])]</literal>.</para>

        <para>Another way to specify the nested record array structure is to
        use the <literal>descr</literal> keyword argument:</para>

        <screen>&gt;&gt;&gt; nra3 = array(
...     [(1, (0.5, 1.0), ('a1', 1j)), (2, (0, 0), ('a2', 1+.1j))],
...     descr=[('id', 'Int64'), ('pos', '(2,)Float32'),
...            ('info', [('name', 'a2'), ('value', 'Complex64')])])
&gt;&gt;&gt;
&gt;&gt;&gt; nra3
array(
[(1L, array([ 0.5,  1. ], type=Float32), ('a1', 1j)),
(2L, array([ 0.,  0.], type=Float32), ('a2', (1+0.10000000000000001j)))],
descr=[('id', 'Int64'), ('pos', '(2,)Float32'), ('info', [('name', 'a2'),
('value', 'Complex64')])],
shape=2)
&gt;&gt;&gt;</screen>

        <para>The <literal>descr</literal> argument is a list of 2-tuples,
        each of them describing a field. The first value in a tuple is the
        name of the field, while the second one is a description of its
        structure. If the second value is a string, it defines the format
        (shape and type) of a non-nested field. Else, it is a list of 2-tuples
        describing the sub-fields of a nested field.</para>

        <para>As you can see, the <literal>descr</literal> list is a mix of
        the <literal>names</literal> and <literal>formats</literal> arguments.
        In fact, this argument is intended to replace
        <literal>formats</literal> and <literal>names</literal>, so they
        cannot be used at the same time.</para>

        <para>Of course the structure of all three keyword arguments must
        match that of the elements (rows) in the <literal>buffer</literal>
        being stored.</para>

        <para>Sometimes it is convenient to create nested arrays by processing
        a set of columns. In these cases the function
        <literal>fromarrays</literal> comes handy. This function works in a
        very similar way to the array function, but the passed buffer is a
        list of columns. For instance:</para>

        <screen>&gt;&gt;&gt; from tables.nra import fromarrays
&gt;&gt;&gt; nra = fromarrays([[1, 2], [4, 5]], descr=[('x', 'f8'),('y', 'f4')])
&gt;&gt;&gt;
&gt;&gt;&gt; nra
array(
[(1.0, 4.0),
(2.0, 5.0)],
descr=[('x', 'f8'), ('y', 'f4')],
shape=2)</screen>

        <para>Columns can be passed as nested arrays, what makes really
        straightforward to combine different nested arrays to get a new one,
        as you can see in the following examples:</para>

        <screen>&gt;&gt;&gt; nra1 = fromarrays([nra, [7, 8]], descr=[('2D', [('x', 'f8'), ('y', 'f4')]),
&gt;&gt;&gt; ... ('z', 'f4')])
&gt;&gt;&gt;
&gt;&gt;&gt; nra1
array(
[((1.0, 4.0), 7.0),
((2.0, 5.0), 8.0)],
descr=[('2D', [('x', 'f8'), ('y', 'f4')]), ('z', 'f4')],
shape=2)
&gt;&gt;&gt;
&gt;&gt;&gt; nra2 = fromarrays([nra1.field('2D/x'), nra1.field('z')], descr=[('x', 'f8'),
('z', 'f4')])
&gt;&gt;&gt;
&gt;&gt;&gt; nra2
array(
[(1.0, 7.0),
(2.0, 8.0)],
descr=[('x', 'f8'), ('z', 'f4')],
shape=2)</screen>

        <para>Finally it's worth to mention a small group of utility functions
        in the <literal>tables.nra.nestedrecords</literal> module,
        <literal>makeFormats</literal>, <literal>makeNames</literal> and
        <literal>makeDescr</literal>, that can be useful to obtain the
        structure specification to be used with the <literal>array</literal>
        and <literal>fromarrays</literal> functions. Given a description list,
        <literal>makeFormats</literal> gets the corresponding
        <literal>formats</literal> list. In the same way
        <literal>makeNames</literal> gets the <literal>names</literal> list.
        On the other hand the <literal>descr</literal> list can be obtained
        from <literal>formats</literal> and names lists using the
        <literal>makeDescr</literal> function. For example:</para>

        <screen>&gt;&gt;&gt; from tables.nra.nestedrecords import makeDescr, makeFormats, makeNames
&gt;&gt;&gt; descr =[('2D', [('x', 'f8'), ('y', 'f4')]),('z', 'f4')]
&gt;&gt;&gt;
&gt;&gt;&gt; formats = makeFormats(descr)
&gt;&gt;&gt; formats
[['f8', 'f4'], 'f4']
&gt;&gt;&gt; names = makeNames(descr)
&gt;&gt;&gt; names
[('2D', ['x', 'y']), 'z']
&gt;&gt;&gt; d1 = makeDescr(formats, names)
&gt;&gt;&gt; d1
[('2D', [('x', 'f8'), ('y', 'f4')]), ('z', 'f4')]
&gt;&gt;&gt; # If no names are passed then they are automatically generated
&gt;&gt;&gt; d2 = makeDescr(formats)
&gt;&gt;&gt; d2
[('c1', [('c1', 'f8'), ('c2', 'f4')]),('c2', 'f4')]</screen>
      </section>

      <section>
        <title><literal>NestedRecArray</literal> methods</title>

        <para>To access the fields in the nested record array use the
        <literal>field()</literal> method:</para>

        <screen>&gt;&gt;&gt; print nra2.field('id')
[1, 2]
&gt;&gt;&gt;</screen>

        <para>The <literal>field()</literal> method accepts also names of
        sub-fields. It will consist of several field name components separated
        by the string <literal>'/'</literal> <footnote>
            <para>This way of specifying the names of sub-fields is
            <emphasis>very</emphasis> specific to the implementation of
            <literal>numarray</literal> nested arrays of PyTables.
            Particularly, if you are using NumPy arrays, keep in mind that
            sub-fields in such arrays must be accessed one at a time, like
            this: <literal>numpy_array['info']['name']</literal>, and not like
            this: <literal>numpy_array['info/name']</literal>.</para>
          </footnote>, for instance:</para>

        <screen>&gt;&gt;&gt; print nra2.field('info/name')
['a1', 'a2']
&gt;&gt;&gt;</screen>

        <para>Finally, the top level fields of the nested recarray can be
        accessed passing an integer argument to the <literal>field()</literal>
        method:</para>

        <screen>&gt;&gt;&gt; print nra2.field(1)
[[ 0.5 1. ] [ 0.  0. ]]
&gt;&gt;&gt;</screen>

        <para>An alternative to the <literal>field()</literal> method is the
        use of the <literal>fields</literal> attribute. It is intended mainly
        for interactive usage in the Python console. For example:</para>

        <screen>&gt;&gt;&gt; nra2.fields.id
[1, 2]
&gt;&gt;&gt; nra2.fields.info.fields.name
['a1', 'a2']
&gt;&gt;&gt;</screen>

        <para>Rows of nested recarrays can be read using the typical index
        syntax. The rows are retrieved as <literal>NestedRecord</literal>
        objects:</para>

        <screen>&gt;&gt;&gt; print nra2[0]
(1L, array([ 0.5,  1. ], type=Float32), ('a1', 1j))
&gt;&gt;&gt;
&gt;&gt;&gt; nra2[0].__class__
&lt;class tables.nra.nestedrecords.NestedRecord at 0x413cbb9c&gt;</screen>

        <para>Slicing is also supported in the usual way:</para>

        <screen>&gt;&gt;&gt; print nra2[0:2]
NestedRecArray[
(1L, array([ 0.5,  1. ], type=Float32), ('a1', 1j)),
(2L, array([ 0.,  0.], type=Float32), ('a2', (1+0.10000000000000001j)))
]
&gt;&gt;&gt;</screen>

        <para>Another useful method is <literal>asRecArray()</literal>. It
        converts a nested array to a non-nested equivalent array.</para>

        <para>This method creates a new vanilla <literal>RecArray</literal>
        instance equivalent to this one by flattening its fields. Only
        bottom-level fields included in the array. Sub-fields are named by
        pre-pending the names of their parent fields up to the top-level
        fields, using <literal>'/'</literal> as a separator. The data area of
        the array is copied into the new one. For example, calling
        <literal>nra3.asRecArray()</literal> would return the same array as
        calling:</para>

        <screen>&gt;&gt;&gt; ra = numarray.records.array(
...     [(1, (0.5, 1.0), 'a1', 1j), (2, (0, 0), 'a2', 1+.1j)],
...     names=['id', 'pos', 'info/name', 'info/value'],
...     formats=['Int64', '(2,)Float32', 'a2', 'Complex64'])</screen>

        <para>Note that the shape of multidimensional fields is kept.</para>
      </section>

      <section>
        <title><literal>NestedRecord</literal> objects</title>

        <para>Each element of the nested record array is a
        <literal>NestedRecord</literal>, i.e. a <literal>Record</literal> with
        support for nested datatypes. As said before, we can do indexing as
        usual:</para>

        <screen>&gt;&gt;&gt; print nra1[0]
(1, (0.5, 1.0), ('a1', 1j))
&gt;&gt;&gt;</screen>

        <para>Using <literal>NestedRecord</literal> objects is quite similar
        to using <literal>Record</literal> objects. To get the data of a field
        we use the <literal>field()</literal> method. As an argument to this
        method we pass a field name. Sub-field names can be passed in the way
        described for <literal>NestedRecArray.field()</literal>. The
        <literal>fields</literal> attribute is also present and works as it
        does in <literal>NestedRecArray</literal>.</para>

        <para>Field data can be set with the <literal>setField()</literal>
        method. It takes two arguments, the field name and its value.
        Sub-field names can be passed as usual. Finally, the
        <literal>asRecord()</literal> method converts a nested record into a
        non-nested equivalent record.</para>
      </section>
    </appendix>

    <appendix id="PTutilities">
      <title>Utilities</title>

      <para>PyTables comes with a couple of utilities that make the life
      easier to the user. One is called <literal>ptdump</literal> and lets you
      see the contents of a PyTables file (or generic HDF5 file, if
      supported). The other one is named <literal>ptrepack</literal> that
      allows to (recursively) copy sub-hierarchies of objects present in a
      file into another one, changing, if desired, some of the filters applied
      to the leaves during the copy process.</para>

      <para>Normally, these utilities will be installed somewhere in your PATH
      during the process of installation of the PyTables package, so that you
      can invoke them from any place in your file system after the
      installation has successfully finished.</para>

      <section id="ptdumpDescr">
        <title>ptdump</title>

        <para>As has been said before, <literal>ptdump</literal> utility
        allows you look into the contents of your PyTables files. It lets you
        see not only the data but also the metadata (that is, the
        <emphasis>structure</emphasis> and additional information in the form
        of <emphasis>attributes</emphasis>).</para>

        <section>
          <title>Usage</title>

          <para>For instructions on how to use it, just pass the
          <literal>-h</literal> flag to the command: <screen>$ ptdump -h</screen>
          to see the message usage: <screen>usage: ptdump [-R start,stop,step] [-a] [-h] [-d] [-v] file[:nodepath]
      -R RANGE -- Select a RANGE of rows in the form "start,stop,step"
      -a -- Show attributes in nodes (only useful when -v or -d are active)
      -c -- Show info of columns in tables (only useful when -v or -d are active)
      -i -- Show info of indexed columns (only useful when -v or -d are active)
      -d -- Dump data information on leaves
      -h -- Print help on usage
      -v -- Dump more metainformation on nodes</screen></para>
        </section>

        <section>
          <title>A small tutorial on <literal>ptdump</literal></title>

          <para>Let's suppose that we want to know only the
          <emphasis>structure</emphasis> of a file. In order to do that, just
          don't pass any flag, just the file as parameter: <screen>$ ptdump vlarray1.h5
vlarray1.h5 (File) ''
Last modif.: 'Mon Jan  8 16:21:25 2007'
Object Tree: 
/ (RootGroup) ''
/vlarray1 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of ints'
/vlarray2 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of strings'</screen>
          we can see that the file contains just a leaf object called
          <literal>vlarray1</literal>, that is an instance of
          <literal>VLArray</literal>, has 4 rows, and two filters has been
          used in order to create it: <literal>shuffle</literal> and
          <literal>zlib</literal> (with a compression level of 1).</para>

          <para>Let's say we want more meta-information. Just add the
          <literal>-v</literal> (verbose) flag: <screen>$ ptdump -v vlarray1.h5
/ (RootGroup) ''
/vlarray1 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of ints'
  atom = Int32Atom(shape=(), dflt=0)
  byteorder = 'little'
  nrows = 3
  flavor = 'numeric'
/vlarray2 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of strings'
  atom = StringAtom(itemsize=2, shape=(), dflt='')
  byteorder = 'irrelevant'
  nrows = 3
  flavor = 'python'</screen> so we can see more info about the atoms that are
          the components of the <literal>vlarray1</literal> dataset, i.e. they
          are scalars of type <literal>Int32</literal> and with
          <literal>Numeric</literal> <emphasis>flavor</emphasis>.</para>

          <para>If we want information about the attributes on the nodes, we
          must add the <literal>-a</literal> flag: <screen>$ ptdump -va vlarray1.h5
/ (RootGroup) ''
  /._v_attrs (AttributeSet), 5 attributes:
   [CLASS := 'GROUP',
    PYTABLES_FORMAT_VERSION := '2.0',
    TITLE := '',
    VERSION := '1.0']
/vlarray1 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of ints'
  atom = Int32Atom(shape=(), dflt=0)
  byteorder = 'little'
  nrows = 3
  flavor = 'numeric'
  /vlarray1._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'VLARRAY',
    FLAVOR := 'numeric',
    TITLE := 'ragged array of ints',
    VERSION := '1.2']
/vlarray2 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of strings'
  atom = StringAtom(itemsize=2, shape=(), dflt='')
  byteorder = 'irrelevant'
  nrows = 3
  flavor = 'python'
  /vlarray2._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'VLARRAY',
    FLAVOR := 'python',
    TITLE := 'ragged array of strings',
    VERSION := '1.2']</screen></para>

          <para>Let's have a look at the real data: <screen>$ ptdump -d vlarray1.h5
/ (RootGroup) ''
/vlarray1 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of ints'
  Data dump:
[0] [5 6]
[1] [5 6 7]
[2] [5 6 9 8]
/vlarray2 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of strings'
  Data dump:
[0] ['5', '66']
[1] ['5', '6', '77']
[2] ['5', '6', '9', '88']</screen> we see here a data dump of the 4 rows in
          <literal>vlarray1</literal> object, in the form of a list. Because
          the object is a VLA, we see a different number of integers on each
          row.</para>

          <para>Say that we are interested only on a specific <emphasis>row
          range</emphasis> of the <literal>/vlarray1</literal> object:
          <screen>ptdump -R2,3 -d vlarray1.h5:/vlarray1
/vlarray1 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of ints'
  Data dump:
[2] [5 6 9 8]</screen> Here, we have specified the range of rows between 2 and
          4 (the upper limit excluded, as usual in Python). See how we have
          selected only the <literal>/vlarray1</literal> object for doing the
          dump (<literal>vlarray1.h5:/vlarray1</literal>).</para>

          <para>Finally, you can mix several information at once: <screen>$ ptdump -R2,3 -vad vlarray1.h5:/vlarray1
/vlarray1 (VLArray(3L,), shuffle, zlib(1)) 'ragged array of ints'
  atom = Int32Atom(shape=(), dflt=0)
  byteorder = 'little'
  nrows = 3
  flavor = 'numeric'
  /vlarray1._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'VLARRAY',
    FLAVOR := 'numeric',
    TITLE := 'ragged array of ints',
    VERSION := '1.2']
  Data dump:
[2] [5 6 9 8]</screen></para>
        </section>
      </section>

      <section id="ptrepackDescr">
        <title>ptrepack</title>

        <para>This utility is a very powerful one and lets you copy any leaf,
        group or complete subtree into another file. During the copy process
        you are allowed to change the filter properties if you want so. Also,
        in the case of duplicated pathnames, you can decide if you want to
        overwrite already existing nodes on the destination file. Generally
        speaking, <literal>ptrepack</literal> can be useful in may situations,
        like replicating a subtree in another file, change the filters in
        objects and see how affect this to the compression degree or I/O
        performance, consolidating specific data in repositories or even
        <emphasis>importing</emphasis> generic HDF5 files and create true
        PyTables counterparts.</para>

        <section>
          <title>Usage</title>

          <para>For instructions on how to use it, just pass the
          <literal>-h</literal> flag to the command: <screen>$ ptrepack -h</screen>
          to see the message usage: <screen>usage: ptrepack [-h] [-v] [-o] 
  [-R start,stop,step] [--non-recursive]
  [--dest-title=title] [--dont-copyuser-attrs] [--overwrite-nodes]
  [--complevel=(0-9)] [--complib=lib] [--shuffle=(0|1)]
  [--fletcher32=(0|1)] [--keep-source-filters] [--upgrade-flavors]
  [--dont-regenerate-old-indexes]
  sourcefile:sourcegroup destfile:destgroup
     -h -- Print usage message.
     -v -- Show more information.
     -o -- Overwite destination file.
     -R RANGE -- Select a RANGE of rows (in the form "start,stop,step")
         during the copy of *all* the leaves.
     --non-recursive -- Do not do a recursive copy. Default is to do it.
     --dest-title=title -- Title for the new file (if not specified,
         the source is copied).
     --dont-copy-userattrs -- Do not copy the user attrs (default is to do it)
     --overwrite-nodes -- Overwrite destination nodes if they exist. Default is
         to not overwrite them.
     --complevel=(0-9) -- Set a compression level (0 for no compression, which
         is the default).
     --complib=lib -- Set the compression library to be used during the copy.
         lib can be set to "zlib", "lzo" or "bzip2". Defaults to "zlib".
     --shuffle=(0|1) -- Activate or not the shuffling filter (default is active
         if complevel&gt;0).
     --fletcher32=(0|1) -- Whether to activate or not the fletcher32 filter
         (not active by default).
     --keep-source-filters -- Use the original filters in source files. The
         default is not doing that if any of --complevel, --complib, --shuffle
         or --fletcher32 option is specified.
     --upgrade-flavors -- When repacking PyTables 1.x files, the flavor of
         leaves will be unset. With this, such a leaves will be serialized
         as objects with the internal flavor ('numpy' for 2.x series).
     --dont-regenerate-old-indexes -- Disable regenerating old indexes. The
         default is to regenerate old indexes as they are found.
          </screen></para>
        </section>

        <section>
          <title>A small tutorial on <literal>ptrepack</literal></title>

          <para>Imagine that we have ended the tutorial 1 (see the output of
          <literal>examples/tutorial1-1.py</literal>), and we want to copy our
          reduced data (i.e. those datasets that hangs from the
          <literal>/column</literal> group) to another file. First, let's
          remember the content of the
          <literal>examples/tutorial1.h5</literal>: <screen>$ ptdump tutorial1.h5
tutorial1.h5 (File) 'Test file'
Last modif.: 'Mon Jan  8 16:30:30 2007'
Object Tree: 
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10L,)) 'Readout example'</screen> Now, copy the
          <literal>/columns</literal> to other non-existing file. That's easy:
          <screen>$ ptrepack tutorial1.h5:/columns reduced.h5</screen> That's
          all. Let's see the contents of the newly created
          <literal>reduced.h5</literal> file: <screen>$ ptdump reduced.h5
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:31:31 2007'
Object Tree: 
/ (RootGroup) ''
/name (Array(3L,)) 'Name column selection'
/pressure (Array(3L,)) 'Pressure column selection'</screen> so, you have
          copied the children of <literal>/columns</literal> group into the
          <emphasis>root</emphasis> of the <literal>reduced.h5</literal>
          file.</para>

          <para>Now, you suddenly realized that what you intended to do was to
          copy all the hierarchy, the group <literal>/columns</literal> itself
          included. You can do that by just specifying the destination group:
          <screen>$ ptrepack tutorial1.h5:/columns reduced.h5:/columns
$ ptdump reduced.h5
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:32:25 2007'
Object Tree: 
/ (RootGroup) ''
/name (Array(3L,)) 'Name column selection'
/pressure (Array(3L,)) 'Pressure column selection'
/columns (Group) ''
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'</screen> OK. Much
          better. But you want to get rid of the existing nodes on the new
          file. You can achieve this by adding the -o flag: <screen>$ ptrepack -o tutorial1.h5:/columns reduced.h5:/columns
$ ptdump reduced.h5
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:33:08 2007'
Object Tree: 
/ (RootGroup) ''
/columns (Group) ''
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'</screen> where you
          can see how the old contents of the <literal>reduced.h5</literal>
          file has been overwritten.</para>

          <para>You can copy just one single node in the repacking operation
          and change its name in destination: <screen>$ ptrepack tutorial1.h5:/detector/readout reduced.h5:/rawdata
$ ptdump reduced.h5
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:33:59 2007'
Object Tree: 
/ (RootGroup) ''
/rawdata (Table(10L,)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'</screen> where the
          <literal>/detector/readout</literal> has been copied to
          <literal>/rawdata</literal> in destination.</para>

          <para>We can change the filter properties as well: <screen>$ ptrepack --complevel=1 tutorial1.h5:/detector/readout reduced.h5:/rawdata
Problems doing the copy from 'tutorial1.h5:/detector/readout' to 'reduced.h5:/rawdata'
The error was --&gt; tables.exceptions.NodeError: destination group ``/`` already has a node named ``rawdata``; you may want to use the ``overwrite`` argument
The destination file looks like:
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:33:59 2007'
Object Tree: 
/ (RootGroup) ''
/rawdata (Table(10L,)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'

Traceback (most recent call last):
  File "utils/ptrepack", line 3, in ?
    main()
  File ".../tables/scripts/ptrepack.py", line 349, in main
    stats = stats, start = start, stop = stop, step = step)
  File ".../tables/scripts/ptrepack.py", line 107, in copyLeaf
    raise RuntimeError, "Please check that the node names are not
    duplicated in destination, and if so, add the --overwrite-nodes flag
    if desired."
RuntimeError: Please check that the node names are not duplicated in
destination, and if so, add the --overwrite-nodes flag if desired.</screen>
          Ooops! We ran into problems: we forgot that the
          <literal>/rawdata</literal> pathname already existed in destination
          file. Let's add the <literal>--overwrite-nodes</literal>, as the
          verbose error suggested: <screen>$ ptrepack --overwrite-nodes --complevel=1 tutorial1.h5:/detector/readout
reduced.h5:/rawdata
$ ptdump reduced.h5
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:36:54 2007'
Object Tree: 
/ (RootGroup) ''
/rawdata (Table(10L,), shuffle, zlib(1)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'</screen> you can
          check how the filter properties has been changed for the
          <literal>/rawdata</literal> table. Check as the other nodes still
          exists.</para>

          <para>Finally, let's copy a <emphasis>slice</emphasis> of the
          <literal>readout</literal> table in origin to destination, under a
          new group called <literal>/slices</literal> and with the name, for
          example, <literal>aslice</literal>: <screen>$ ptrepack -R1,8,3 tutorial1.h5:/detector/readout reduced.h5:/slices/aslice
$ ptdump reduced.h5
reduced.h5 (File) ''
Last modif.: 'Mon Jan  8 16:38:13 2007'
Object Tree: 
/ (RootGroup) ''
/rawdata (Table(10L,), shuffle, zlib(1)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3L,)) 'Name column selection'
/columns/pressure (Array(3L,)) 'Pressure column selection'
/slices (Group) ''
/slices/aslice (Table(3L,)) 'Readout example'</screen> note how only 3 rows of
          the original <literal>readout</literal> table has been copied to the
          new <literal>aslice</literal> destination. Note as well how the
          previously inexistent <literal>slices</literal> group has been
          created in the same operation.</para>
        </section>
      </section>

      <section id="nctoh5Descr">
        <title>nctoh5</title>

        <para>This tool is able to convert a file in <ulink
        url="http://www.unidata.ucar.edu/packages/netcdf/"><literal>NetCDF</literal></ulink>
        format to a PyTables file (and hence, to a HDF5 file). However, for
        this to work, you will need the NetCDF interface for Python that comes
        with the excellent <literal>Scientific Python</literal> (see
        <biblioref linkend="scientificpythonRef" />) package. This script was
        initially contributed by Jeff Whitaker. It has been updated to support
        selectable filters from the command line and some other small
        improvements.</para>

        <para>If you want other file formats to be converted to PyTables, have
        a look at the <literal>SciPy</literal> (see <biblioref
        linkend="scipyRef" />) project (subpackage <literal>io</literal>), and
        look for different methods to import them into
        <literal>NumPy/Numeric/numarray</literal> objects. Following the
        <literal>SciPy</literal> documentation, you can read, among other
        formats, ASCII files (<literal>read_array</literal>), binary files in
        C or Fortran (<literal>fopen</literal>) and <literal>MATLAB</literal>
        (version 4, 5 or 6) files (<literal>loadmat</literal>). Once you have
        the content of your files as <literal>NumPy/Numeric/numarray</literal>
        objects, you can save them as regular <literal>(E)Arrays</literal> in
        PyTables files. Remember, if you end with a nice conversor, do not
        forget to contribute it back to the community. Thanks!</para>

        <section>
          <title>Usage</title>

          <para>For instructions on how to use it, just pass the
          <literal>-h</literal> flag to the command: <screen>$ nctoh5 -h</screen>
          to see the message usage: <screen>usage: nctoh5 [-h] [-v] [-o] [--complevel=(0-9)] [--complib=lib]
 [--shuffle=(0|1)] [--fletcher32=(0|1)] [--unpackshort=(0|1)]
 [--quantize=(0|1)] netcdffilename hdf5filename
 -h -- Print usage message.
 -v -- Show more information.
 -o -- Overwite destination file.
 --complevel=(0-9) -- Set a compression level (0 for no compression, which
     is the default).
 --complib=lib -- Set the compression library to be used during the copy.
     lib can be set to "zlib", "lzo" or "bzip2". Defaults to "zlib".
 --shuffle=(0|1) -- Activate or not the shuffling filter (default is active
     if complevel&gt;0).
 --fletcher32=(0|1) -- Whether to activate or not the fletcher32 filter (not
     active by default).
 --unpackshort=(0|1) -- unpack short integer variables to float variables
     using scale_factor and add_offset netCDF variable attributes
     (not active by default).
 --quantize=(0|1) -- quantize data to improve compression using
     least_significant_digit netCDF variable attribute (not active by default).
     See http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml
     for further explanation of what this attribute means.</screen> If you
          have followed the small tutorial on the <literal>ptrepack</literal>
          utility (see <xref linkend="ptrepackDescr"
          xrefstyle="select: label" />), you should easily realize what
          most of the different flags would mean.</para>
        </section>
      </section>
    </appendix>

    <appendix id="PyTablesInternalFormat">
      <title>PyTables File Format</title>

      <para>PyTables has a powerful capability to deal with native HDF5 files
      created with another tools. However, there are situations were you may
      want to create truly native PyTables files with those tools while
      retaining fully compatibility with PyTables format. That is perfectly
      possible, and in this appendix is presented the format that you should
      endow to your own-generated files in order to get a fully PyTables
      compatible file.</para>

      <para>We are going to describe the <emphasis>2.0 version of PyTables
      file format</emphasis> (introduced in PyTables version 2.0). As time
      goes by, some changes might be introduced (and documented here) in order
      to cope with new necessities. However, the changes will be carefully
      pondered so as to ensure backward compatibility whenever is
      possible.</para>

      <para>A PyTables file is composed with arbitrarily large amounts of HDF5
      groups (<literal>Groups</literal> in PyTables naming scheme) and
      datasets (<literal>Leaves</literal> in PyTables naming scheme). For
      groups, the only requirements are that they must have some
      <emphasis>system attributes</emphasis> available. By convention, system
      attributes in PyTables are written in upper case, and user attributes in
      lower case but this is not enforced by the software. In the case of
      datasets, besides the mandatory system attributes, some conditions are
      further needed in their storage layout, as well as in the datatypes used
      in there, as we will see shortly.</para>

      <para>As a final remark, you can use any filter as you want to create a
      PyTables file, provided that the filter is a standard one in HDF5, like
      <emphasis>zlib</emphasis>, <emphasis>shuffle</emphasis> or
      <emphasis>szip</emphasis> (although the last one can not be used from
      within PyTables to create a new file, datasets compressed with szip can
      be read, because it is the HDF5 library which do the decompression
      transparently).</para>

      <section>
        <title>Mandatory attributes for a <literal>File</literal></title>

        <para>The <literal>File</literal> object is, in fact, an special HDF5
        <emphasis>group</emphasis> structure that is <emphasis>root</emphasis>
        for the rest of the objects on the object tree. The next attributes
        are mandatory for the HDF5 <emphasis>root group</emphasis> structure
        in PyTables files:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

            <glossdef>
              <para>This attribute should always be set to
              <literal>'GROUP'</literal> for group structures.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis
            role="bold">PYTABLES_FORMAT_VERSION</emphasis></glossterm>

            <glossdef>
              <para>It represents the internal format version, and currently
              should be set to the <literal>'2.0'</literal> string.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

            <glossdef>
              <para>A string where the user can put some description on what
              is this group used for.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">VERSION</emphasis></glossterm>

            <glossdef>
              <para>Should contains the string
              <literal>'1.0'</literal>.</para>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>

      <section>
        <title>Mandatory attributes for a <literal>Group</literal></title>

        <para>The next attributes are mandatory for <emphasis>group</emphasis>
        structures:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

            <glossdef>
              <para>This attribute should always be set to
              <literal>'GROUP'</literal> for group structures.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

            <glossdef>
              <para>A string where the user can put some description on what
              is this group used for.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">VERSION</emphasis></glossterm>

            <glossdef>
              <para>Should contains the string
              <literal>'1.0'</literal>.</para>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>

      <section>
        <title>Optional attributes for a <literal>Group</literal></title>

        <para>The next attributes are optional for <emphasis>group</emphasis>
        structures:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">FILTERS</emphasis></glossterm>

            <glossdef>
              <para>When present, this attribute contains the filter
              properties (a <literal>Filters</literal> instance, see section
              <xref linkend="FiltersClassDescr" xrefstyle="select: label" />)
              that may be inherited by leaves or groups created immediately
              under this group. This is a packed 64-bit integer structure,
              where</para>

              <itemizedlist>
                <listitem>
                  <para><emphasis role="bold">byte 0</emphasis> (the
                  least-significant byte) is the compression level
                  (<literal>complevel</literal>).</para>
                </listitem>

                <listitem>
                  <para><emphasis role="bold">byte 1</emphasis> is the
                  compression library used (<literal>complib</literal>): 0
                  when irrelevant, 1 for Zlib, 2 for LZO and 3 for
                  Bzip2.</para>
                </listitem>

                <listitem>
                  <para><emphasis role="bold">byte 2</emphasis> indicates
                  which parameterless filters are enabled
                  (<literal>shuffle</literal> and
                  <literal>fletcher32</literal>): bit 0 is for
                  <emphasis>Shuffle</emphasis> while bit 1 is for
                  <emphasis>Fletcher32</emphasis>.</para>
                </listitem>

                <listitem>
                  <para>other bytes are reserved for future use.</para>
                </listitem>
              </itemizedlist>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>

      <section>
        <title>Mandatory attributes, storage layout and supported data types
        for <literal>Leaves</literal></title>

        <para>This depends on the kind of <literal>Leaf</literal>. The format
        for each type follows.</para>

        <section id="TableFormatDescr">
          <title><literal>Table</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>table</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'TABLE'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'2.6'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">FIELD_X_NAME</emphasis></glossterm>

                <glossdef>
                  <para>It contains the names of the different fields. The
                  <literal>X</literal> means the number of the field,
                  zero-based (beware, order do matter). You should add as many
                  attributes of this kind as fields you have in your
                  records.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">FIELD_X_FILL</emphasis></glossterm>

                <glossdef>
                  <para>It contains the default values of the different
                  fields. All the datatypes are suported natively, except for
                  complex types that are currently serialized using Pickle.
                  The <literal>X</literal> means the number of the field,
                  zero-based (beware, order do matter). You should add as many
                  attributes of this kind as fields you have in your records.
                  These fields are meant for saving the default values
                  persistently and their existence is optional.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">NROWS</emphasis></glossterm>

                <glossdef>
                  <para>This should contain the number of
                  <emphasis>compound</emphasis> data type entries in the
                  dataset. It must be an <emphasis>int</emphasis> data
                  type.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>A <literal>Table</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>1-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The datatype of the elements (rows) of
            <literal>Table</literal> must be the H5T_COMPOUND
            <emphasis>compound</emphasis> data type, and each of these
            compound components must be built with only the next HDF5 data
            types <emphasis>classes</emphasis>:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_BITFIELD</emphasis></glossterm>

                <glossdef>
                  <para>This class is used to represent the
                  <literal>Bool</literal> type. Such a type must be build
                  using a H5T_NATIVE_B8 datatype, followed by a HDF5
                  <literal>H5Tset_precision</literal> call to set its
                  precision to be just 1 bit.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_INTEGER</emphasis></glossterm>

                <glossdef>
                  <para>This includes the next data types:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_SCHAR</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>signed
                        char</emphasis> C type, but it is effectively used to
                        represent an <literal>Int8</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_UCHAR</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        char</emphasis> C type, but it is effectively used to
                        represent an <literal>UInt8</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_SHORT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>short</emphasis> C
                        type, and it is effectively used to represent an
                        <literal>Int16</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_USHORT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        short</emphasis> C type, and it is effectively used to
                        represent an <literal>UInt16</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_INT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>int</emphasis> C
                        type, and it is effectively used to represent an
                        <literal>Int32</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_UINT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        int</emphasis> C type, and it is effectively used to
                        represent an <literal>UInt32</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_LONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>long</emphasis> C
                        type, and it is effectively used to represent an
                        <literal>Int32</literal> or an
                        <literal>Int64</literal>, depending on whether you are
                        running a 32-bit or 64-bit architecture.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_ULONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        long</emphasis> C type, and it is effectively used to
                        represent an <literal>UInt32</literal> or an
                        <literal>UInt64</literal>, depending on whether you
                        are running a 32-bit or 64-bit architecture.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_LLONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>long long</emphasis>
                        C type (<literal>__int64</literal>, if you are using a
                        Windows system) and it is effectively used to
                        represent an <literal>Int64</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_ULLONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned long
                        long</emphasis> C type (beware: this type does not
                        have a correspondence on Windows systems) and it is
                        effectively used to represent an
                        <literal>UInt64</literal> type.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_FLOAT</emphasis></glossterm>

                <glossdef>
                  <para>This includes the next datatypes:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_FLOAT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>float</emphasis> C
                        type and it is effectively used to represent an
                        <literal>Float32</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_DOUBLE</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>double</emphasis> C
                        type and it is effectively used to represent an
                        <literal>Float64</literal> type.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_TIME</emphasis></glossterm>

                <glossdef>
                  <para>This includes the next datatypes:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_UNIX_D32</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a POSIX
                        <emphasis>time_t</emphasis> C type and it is
                        effectively used to represent a
                        <literal>'Time32'</literal> aliasing type, which
                        corresponds to an <literal>Int32</literal>
                        type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_UNIX_D64</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a POSIX <emphasis>struct
                        timeval</emphasis> C type and it is effectively used
                        to represent a <literal>'Time64'</literal> aliasing
                        type, which corresponds to a
                        <literal>Float64</literal> type.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_STRING</emphasis></glossterm>

                <glossdef>
                  <para>The datatype used to describe strings in PyTables is
                  H5T_C_S1 (i.e. a <emphasis>string</emphasis> C type)
                  followed with a call to the HDF5
                  <literal>H5Tset_size()</literal> function to set their
                  length.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_ARRAY</emphasis></glossterm>

                <glossdef>
                  <para>This allows the construction of homogeneous,
                  multidimensional arrays, so that you can include such
                  objects in compound records. The types supported as elements
                  of H5T_ARRAY data types are the ones described above.
                  Currently, PyTables does not support nested H5T_ARRAY
                  types.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_COMPOUND</emphasis></glossterm>

                <glossdef>
                  <para>This allows the support of complex numbers. Its format
                  is described below:</para>

                  <para>The H5T_COMPOUND type class contains two members. Both
                  members must have the H5T_FLOAT atomic datatype class. The
                  name of the first member should be "r" and represents the
                  real part. The name of the second member should be "i" and
                  represents the imaginary part. The
                  <emphasis>precision</emphasis> property of both of the
                  H5T_FLOAT members must be either 32 significant bits (e.g.
                  H5T_NATIVE_FLOAT) or 64 significant bits (e.g.
                  H5T_NATIVE_DOUBLE). They represent Complex32 and Complex64
                  types respectively.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Currently, PyTables does not support nested H5T_COMPOUND
            types, the only exception being supporting complex numbers in
            <literal>Table</literal> objects as described above.</para>
          </section>
        </section>

        <section id="ArrayFormatDescr">
          <title><literal>Array</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>array</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'ARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'2.3'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>Array</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>N-dimensional
            contiguous</emphasis> layout (if you prefer a
            <emphasis>chunked</emphasis> layout see <literal>EArray</literal>
            below).</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The elements of <literal>Array</literal> must have either
            HDF5 <emphasis>atomic</emphasis> data types or a
            <emphasis>compound</emphasis> data type representing a complex
            number. The atomic data types can currently be one of the next
            HDF5 data type <emphasis>classes</emphasis>: H5T_BITFIELD,
            H5T_INTEGER, H5T_FLOAT and H5T_STRING. The H5T_TIME class is also
            supported for reading existing <literal>Array</literal> objects,
            but not for creating them. See the <literal>Table</literal> format
            description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" /> for more info about these
            types.</para>

            <para>In addition to the HDF5 atomic data types, the Array format
            supports complex numbers with the H5T_COMPOUND data type class.
            See the <literal>Table</literal> format description in <xref
            linkend="TableFormatDescr" xrefstyle="select: label" /> for more
            info about this special type.</para>

            <para>You should note that H5T_ARRAY class datatypes are not
            allowed in <literal>Array</literal> objects.</para>
          </section>
        </section>

        <section id="CArrayFormatDescr">
          <title><literal>CArray</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>CArray</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'CARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'1.0'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>CArray</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>N-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The elements of <literal>CArray</literal> must have either
            HDF5 <emphasis>atomic</emphasis> data types or a
            <emphasis>compound</emphasis> data type representing a complex
            number. The atomic data types can currently be one of the next
            HDF5 data type <emphasis>classes</emphasis>: H5T_BITFIELD,
            H5T_INTEGER, H5T_FLOAT and H5T_STRING. The H5T_TIME class is also
            supported for reading existing <literal>CArray</literal> objects,
            but not for creating them. See the <literal>Table</literal> format
            description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" /> for more info about these
            types.</para>

            <para>In addition to the HDF5 atomic data types, the CArray format
            supports complex numbers with the H5T_COMPOUND data type class.
            See the <literal>Table</literal> format description in <xref
            linkend="TableFormatDescr" xrefstyle="select: label" /> for more
            info about this special type.</para>

            <para>You should note that H5T_ARRAY class datatypes are not
            allowed yet in <literal>Array</literal> objects.</para>
          </section>
        </section>

        <section id="EArrayFormatDescr">
          <title><literal>EArray</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>earray</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'EARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">EXTDIM</emphasis></glossterm>

                <glossdef>
                  <para>(<emphasis>Integer</emphasis>) Must be set to the
                  extendable dimension. Only one extendable dimension is
                  supported right now.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'1.3'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>EArray</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>N-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The elements of <literal>EArray</literal> are allowed to
            have the same data types as for the elements in the Array format.
            They can be one of the HDF5 <emphasis>atomic</emphasis> data type
            <emphasis>classes</emphasis>: H5T_BITFIELD, H5T_INTEGER,
            H5T_FLOAT, H5T_TIME or H5T_STRING, see the
            <literal>Table</literal> format description in <xref
            linkend="TableFormatDescr" xrefstyle="select: label" /> for more
            info about these types. They can also be a H5T_COMPOUND datatype
            representing a complex number, see the <literal>Table</literal>
            format description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" />.</para>

            <para>You should note that H5T_ARRAY class data types are not
            allowed in <literal>EArray</literal> objects.</para>
          </section>
        </section>

        <section id="VLArrayFormatDescr">
          <title><literal>VLArray</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>vlarray</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'VLARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">PSEUDOATOM</emphasis></glossterm>

                <glossdef>
                  <para>This is used so as to specify the kind of pseudo-atom
                  (see <xref linkend="VLArrayFormatDescr"
                  xrefstyle="select:                   label" />) for the
                  <literal>VLArray</literal>. It can take the values
                  <literal>'vlstring'</literal> or
                  <literal>'object'</literal>. If your atom is not a
                  pseudo-atom then you should not specify it.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'1.3'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>VLArray</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>1-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Data types supported</title>

            <para>The data type of the elements (rows) of
            <literal>VLArray</literal> objects must be the H5T_VLEN
            <emphasis>variable-length</emphasis> (or VL for short) datatype,
            and the base datatype specified for the VL datatype can be of any
            <emphasis>atomic</emphasis> HDF5 datatype that is listed in the
            <literal>Table</literal> format description <xref
            linkend="TableFormatDescr" xrefstyle="select: label" />.  That
            includes the classes:</para>

            <itemizedlist>
              <listitem>
                <para>H5T_BITFIELD</para>
              </listitem>

              <listitem>
                <para>H5T_INTEGER</para>
              </listitem>

              <listitem>
                <para>H5T_FLOAT</para>
              </listitem>

              <listitem>
                <para>H5T_TIME</para>
              </listitem>

              <listitem>
                <para>H5T_STRING</para>
              </listitem>

              <listitem>
                <para>H5T_ARRAY</para>
              </listitem>
            </itemizedlist>

            <para>They can also be a H5T_COMPOUND data type representing a
            complex number, see the <literal>Table</literal> format
            description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" /> for a detailed description.</para>

            <para>You should note that this does not include another VL
            datatype, or a compound datatype that does not fit the description
            of a complex number. Note as well that, for
            <literal>object</literal> and <literal>vlstring</literal>
            pseudo-atoms, the base for the VL datatype is always a
            H5T_NATIVE_UCHAR. That means that the complete row entry in the
            dataset has to be used in order to fully serialize the object or
            the variable length string.</para>

            <para>In addition, if you plan to use a
            <literal>vlstring</literal> pseudo-atom for your text data and you
            are using ascii-7 (7 bits ASCII) codification for your strings,
            but you don't know (or just don't want) to convert it to the
            required UTF-8 codification, you should not worry too much about
            that because the ASCII characters with values in the range [0x00,
            0x7f] are directly mapped to Unicode characters in the range
            [U+0000, U+007F] and the UTF-8 encoding has the useful property
            that an UTF-8 encoded ascii-7 string is indistinguishable from a
            traditional ascii-7 string. So, you will not need any further
            conversion in order to save your ascii-7 strings and have an
            <literal>vlstring</literal> pseudo-atoms.</para>
          </section>
        </section>
      </section>

      <section>
        <title>Optional attributes for <literal>Leaves</literal></title>

        <para>The next attributes are optional for
        <emphasis>leaves</emphasis>:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">FLAVOR</emphasis></glossterm>

            <glossdef>
              <para>This is meant to provide the information about the kind of
              object kept in the <literal>Leaf</literal>, i.e. when the
              dataset is read, it will be converted to the indicated flavor.
              It can take one the next string values:</para>

              <glosslist>
                <glossentry>
                  <glossterm><emphasis
                  role="bold">"numpy"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data (record arrays, arrays, records, scalars)
                    will be returned as <literal>NumPy</literal>
                    objects.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold">"numarray"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data will be returned as
                    <literal>numarray</literal> objects.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold">"numeric"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data will be returned as
                    <literal>Numeric</literal> objects.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold">"python"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data will be returned as Python lists, tuples
                    or scalars.</para>
                  </glossdef>
                </glossentry>
              </glosslist>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>
    </appendix>
  </part>

  <bibliography>
    <bibliomixed id="HDFWhatIs"></bibliomixed>

    <bibliomixed id="HL-HDF"></bibliomixed>

    <bibliomixed id="HDFIntr"></bibliomixed>

    <bibliomixed id="TableExamples"></bibliomixed>

    <bibliomixed id="zlibRef"></bibliomixed>

    <bibliomixed id="Objectify"></bibliomixed>

    <bibliomixed id="Pyrex"></bibliomixed>

    <bibliomixed id="NetCDFRef"></bibliomixed>

    <bibliomixed id="NetCDF4Ref"></bibliomixed>

    <bibliomixed id="NumPy"></bibliomixed>

    <bibliomixed id="Numeric"></bibliomixed>

    <bibliomixed id="Numarray"></bibliomixed>

    <bibliomixed id="lzoRef"></bibliomixed>

    <bibliomixed id="bzip2Ref"></bibliomixed>

    <bibliomixed id="psycoRef"></bibliomixed>

    <bibliomixed id="scientificpythonRef"></bibliomixed>

    <bibliomixed id="scipyRef"></bibliomixed>

    <bibliomixed id="NewObjectTreeCacheRef"></bibliomixed>

    <bibliomixed id="ViTablesRef"></bibliomixed>

    <bibliomixed id="GnuWin32"></bibliomixed>
  </bibliography>
</book>
<!-- Local Variables: -->
<!-- fill-column: 78 -->
<!-- indent-tabs-mode: nil -->
<!-- End: -->